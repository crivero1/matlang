\section{MATLANG extended with FOR}

\subsection{Syntax and semantics of MATLANG with FOR}

A vocabulary $\cV$ is a set of matrix variables. An instance $\cI$ is a function that maps matrix names to concrete matrices of a determined dimension $(n,m)$
We also assume the presence of $F$, a set of functions $f:\mathbb{R}^{k}\rightarrow\mathbb{R}$ (with $1\leq k$).

Let $\cV$ be a vocabulary, a formula in the vocabulary $\cV$ is of the form:
$$
E:= V\in \cV \ |\    E^* \ |\ E_1+E_2 \ |\  E_1\cdot E_2 \ |\  f(E_1,\ldots, E_k), f\in F \ |\  \for v,X. E
$$

In the last expression $v,X\in \cV$.

Now, a MATLANG ++ schema $\cS$ is given a vocabulary $\cV$, a function $\texttt{size}$ and  an instance $\cI$ and a set of functions $F$, we define the well typedness of a formula of $\cS$ according to $\cI$ as follows. 

\begin{align*}
\text{type}(V) &= (n,m) \text{ if } \cI(V) \text{ is a } n\times m \text{ matrix } \\
\text{type}(E^*)&=(j,i) \text{ where } (i,j)=\text{type}(E) \\
\text{type}(E_1 + E_2) &= \text{type}(E_1) \text{ if } \text{type}(E_1)=\text{type}(E_2) \\
\text{type}(E_1\cdot E_2)&= (i,k) \text{ if } \text{type}(E_1)=(i,j)\text{ and type}(E_2)=(j,k) \\
\text{type}(f(E_1,\ldots, E_k))&=(1,1) \text{ if } \text{type}(E_1)=\cdots =\text{type}(E_k)=(1,1) \\
\text{type}\left( \for X,v. E\right) &= \text{type}(E) \text{ if } \text{type}(X)=\text{type}(E)\text{ and type}(v)=(n,1)
\end{align*}

We say that $E$ is well typed if it has a defined type. Now we will proceed with the semantics of these formulas.
First, we define $\text{type}(v)_1 = n_v$ as the first coordinate of the type of $v$ and $\cI[v:= e^n_i]$ as the instance $\cI$ except that $v$ is mapped to the $i-$th canonical vector of size $n$.
Now, given $\cS, \cI, F$ and a well typed expression $E$, we define the semantics for the values $\sem{E}(\cS,\cI)$ inductively as follows.

\begin{align*}
\sem{M}(\cS,\cI)&=\cI (M) \\
\sem{E^*}(\cS,\cI)&=\sem{E}(\cS,\cI)^* \\
\sem{E_1+E_2}(\cS,\cI)&= \sem{E_1}(\cS,\cI)+\sem{E_2}(\cS,\cI)\\
\sem{E_1\cdot E_2}(\cS,\cI)&= \sem{E_1}(\cS,\cI)\times \sem{E_2}(\cS,\cI)\\
\sem{f(E_1,\ldots ,E_k)}(\cS,\cI) &= f\left(\sem{E_1}(\cS,\cI),\ldots , \sem{E_k}(\cS,\cI)\right)\\
\sem{ \for X,v. E }(\cS,\cI) &= A_n \text{ where }
	\begin{cases}
	               A_0 = 0 \\
 	               A_i = \sem{E}(\cS,\cI[v:= e^{n_v}_i, X:=A_{i-1}]), i=1,\ldots, n_v
	\end{cases}
\end{align*}

Here, $+$ is matrix sum and $\times$ is matrix multiplication. Depending on context, we sometimes denote explicitly that an expression $E$ depends on $v$ or $X$ as $E(v)$, $E(X)$ or $E(v,X)$.

On the other hand, expressions that depend on a variable $v$ or $X$ can be well typed according to the typing rules, but we cannot infer a value (semantic) unless $v$ and $X$ are instantiated by $\for v,X. E$.

\subsection{Order}

Let's turn our attention to the order of the canonical vectors $\lbrace v_i\rbrace_i$. If we have the canonical vectors, using a simple matrix product we can compute a function that discriminates if two canonical vectors are the same or not, in particular
\[
  			f(v_i,v_j)=1-v_i^*v_j=\begin{cases}
               0 \text{ if } i=j \\
               1 \text{ if } i\neq j
            \end{cases}
		\]
does exactly that. So, without adding any object, we have equality for canonical vectors. Note that implicitly we are using the identity, the matrix equivalent to the equality relation. We can compute this as $$ I = \for X, v. X+vv^*$$

Can we get the matrix equivalent to a \textit{order} relation? The $\for$operator actually iterates over the canonical vector, so it makes sense that we can get this order matrices using this operator. First we are going to use a little trick with the $\for$operator. We compute$$v_{max} = \for X,v. v$$ This allows us to have the last canonical vector in every iteration and thus we have access to the last column of $X$ in every iteration, so we can sort of use the last column of $X$ as a placeholder for some vector. For example, to compute the matrix
\[
Z_{eq} = \begin{bmatrix}
    1 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 1 
\end{bmatrix}.
\]

We do $$Z_{\text{eq}}=\for v,X. X + \left[ (Xv_{max}) + v \right]v^* + vv^*_{max}.$$ This iteration does the following: add the current stored vector $Xv_{max}$ plus $v$ to the column of $X$ indicated by $v$ and then add $v$ to the last column of $X$ (the stored vector).

Also, we have

\[
  			f(v_i, v_j)=v_i^*Z_{eq}v_j=\begin{cases}
               1 \text{ if } i \leq j \\
               0 \text{ if } i > j
            \end{cases}
\]

This $Z_{eq}$ matrix discriminate between canonical vectors in terms of order. Note that this gives us an answer for any pair of canonical vector, so we have a total order in some sense.
Now, consider the following matrix
\[
Z:=Z_{eq} - I = \begin{bmatrix}
    0 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 0 
\end{bmatrix}.
\]
In some way, this matrix gives us an strict total ordering of the canonical vectors.
\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]
		
This kind of functions discriminate between canonical vectors, in terms of order.

Can we introduce a \textit{local} order of the canonical vectors? For instance, we can ask ourselves if we can define the \textit{successor} or \textit{predecessor} relation of the canonical vectors from a matrix. Let us introduce a new matrix:

\[
S := \begin{bmatrix}
    0 & 1         & 0         & \cdots &  0 \\
    0 & \ddots & 1         & \cdots & 0 \\
    \vdots & \vdots & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & \ddots & 1 \\
    0 & \cdots & \cdots & \cdots & 0 
\end{bmatrix}.
\]

We can compute this as $$S = \for X,v. X+\left[ (1 - v^*v_{max})vv_{max}^* - (Xv_{max}) v_{max}^* + (Xv_{max})v^*\right]$$ and note that

\[
  			Sv_i=\begin{cases}
               v_{i-1} \text{ if } i > 1 \\
              \mathbf{0} \text{ if } i = 1
            \end{cases}
		\]

So it's the matrix equivalent of the \textit{predecessor} relation. For example, now we can compute 

\[
  			g(v_i, v_j)=v_i^*Sv_j=\begin{cases}
               1 \text{ if } i = j - 1 \\
               0 \text{ if not}
            \end{cases}
		\]
		
With this matrix we can compute a function that tells us if the argument is the first canonical vector. Note that we have $$\mathbf{1}\cdot Su=0\leftrightarrow u=v_1, \hspace{1ex} u\in\lbrace v_1,\ldots, v_n\rbrace,$$ and we have that if $u\in\lbrace v_1,\ldots, v_n\rbrace$ then
\[
  			min(u)=(1-\mathbf{1}^*Su)=\begin{cases}
               1 \text{ if } u = v_1 \\
               0 \text{ if not}
            \end{cases}
		\]
		
This is enough since if it is used over vector variables it will allow us to identify when $v=e_1$. We actually do $min(u)=(1-\text{ones}(\text{diag}(u))^*Su)$, we show how to do $diag$ and $ones$ in the next section. This way, we can get $$v_{min} = \for X,v. v\cdot min(v)$$ 

\subsection{Simulating MATLANG}

When $E=E(v)$ there exists useful operations that arise frequently. The first one is to sum the same expression but iterating over canonical vectors. We define this as $$\ssum v. E(v)=\for v, X. X + E(v).$$

The second one is similar, but with multiplication: $$\sprod v. E(v)=\for X,v. X\cdot E(v) + min(v)\cdot E(v)$$
Here we need to identify the first iteration to not multiply by the zero matrix.

\subsubsection{Matrix pointwise function application (apply[f])}

We have that 

\begin{itemize}
	\item \textbf{Pointwise application:} if $A^{(1)}, \ldots, A^{(n)}$ are $m\times p$ matrices, then apply$\left[ f \right](A^{(1)}, \ldots, A^{(n)})$ is the $m\times p$ matrix $C$ where $C_{ij}=f(A^{(1)}_{ij}, \ldots, A^{(n)}_{ij})$.
\end{itemize}

So, given the \textbf{sum} operator and the function $f$, we can compute $C$ in the following way: $$C=\ssum x_i.\ssum x_j. f\left( x_i^*A^{(1)}x_j, \cdots, x_i^*A^{(n)}v_j\right)\cdot x_ix_j^*$$

Recall that $v_iv_j^*$ is the matrix that has a 1 in the position i, j and zero everywhere else.

Thus, we can simulate the MATLANG operator apply[$f$].

For example, matrix pointwise multiplication: $$\ssum x_i.\ssum x_j.\left( x_i^*A^{(1)}x_j\times \cdots\times x_i^*A^{(n)}x_j\right)\cdot x_ix_j^*$$

\subsubsection{Ones vector and diag()}

Note that $$\text{ones}(M)=\text{apply}\left[ f_{x\rightarrow 1}\right] \left(\ssum v. Mv\right).$$ And $$\text{diag}(u)=\ssum v. (v^*u)\cdot vv^*$$

\subsubsection{Identity}

We can compute the $n\times n$ identity as $$\ssum v. v \cdot v^*$$ if $\cI(v)=(n,1)$.

\subsubsection{Matrix multiplication via sum operator}

Note that using the sum operator we can compute matrix multiplication by definition $$A\cdot B = \ssum v_i\ssum v_j\ssum v_k \left( v_iAv_k\cdot v_kBv_j\right)v_iv_j^*$$

\subsection{Examples}

Let's see some examples of what we can do with MATLANG with FOR.

\subsubsection{Trace and diagonal product}

We can express $$tr(A)=\ssum v. v^*Av.$$

And the product of the diagonal of a matrix (this can be useful in computing the determinant of an upper/down triangular matrix): $$dp(A)=\sprod v. v^*Av.$$

\subsubsection{Transitive closure}

To compute transitive closure we need the quantity $(A+I)^n$ and use apply[$f>0$] on the result matrix. Then we have that $$(I+A)^n=\sprod v. (I+A),$$ and thus $$TC = \text{apply}\left[ f_{>0}\right]\left( \sprod v. (I+A) \right).$$
Using only the new operators, we have that $$TC=\ssum v_i. \ssum v_j. f_{>0}\left(v_i^*\left(\sprod v. (I+A)\right)v_j\right)$$

\subsubsection{$k$-cliques}

Let's try to see first if there is a four clique in the adjacency matrix $A$. To do this, we need to verify if there are paths between four \textbf{different} nodes. So we need the function 

\[
  			f(u,v)=1-u^*v=\begin{cases}
               0 \text{ if } u=v \\
               1 \text{ if } u\neq v
            \end{cases}
		\]

Define $$g(u,v,w,r)=f(u,v)\cdot f(u,w)\cdot f(u,r)\cdot f(v,w)\cdot f(v,r)\cdot f(w,r).$$ This is, $g$ is zero if any pair of vectors are the same.
		
So $$\ssum v_1.\ssum v_2. \ssum v_3. \ssum v_4. (v_1^*Av_2)(v_1^*Av_3)(v_1^*Av_4)(v_2^*Av_3)(v_2^*Av_4)(v_3^*Av_4)g(v_1,v_2,v_3,v_4).$$

\subsubsection{Power sum}

If we want to compute $I+ A + A^2 + \cdots + A^n$ we need to do a little trick with the identity. In this case, we need to add results of matrix powers. We do this like $$\ssum v.\sprod w. \left( (A-I)(wZv) + I\right).$$

To see why this works, note that
\[
  			wZv=\begin{cases}
               1 \text{ if } w<v \\
               0 \text{ if } w\geq v
            \end{cases}
		\]
This is, for a sum term $A^k$, if $w$ is previous to $v$ (in the canonical vectors ordering), then stop multiplying $A$ and use $I$ until the end of the current sum term, and then add this term to the final result.

\subsubsection{Pivoting a column}

Let $A$ be a $n\times n$ matrix and $\alpha$ a scalar. For computing Gaussian elimination we turn our attention in the elementary matrices of the form $$E=I + \alpha\cdot e_ie_j^*.$$ Note that $E\cdot A$ adds a $\alpha$-multiple of row $i$ to row $j$. It is worth noting that $E^{-1}= I - \alpha\cdot e_ie_j^*.$

The key property is that, if $A$ is $LU$ factorizable without any row interchange, then $A=LU$, for some upper diagonal matrix $U$ and $L^{-1}=T_{n-1}\cdots T_1$ where $T_{i}=E_{n}^{(i)}\cdots E_{i+1}^{(i)}$ for some elementary matrices $E^{(i)}_j=I + \alpha_{ij}\cdot e_ie_{j}^*.$ Here $T_iA$ results in $A$ with its $i-$th column reduced, and $E_j^{(i)}$ adds an $\alpha_{ij}$ multiple of row $i$ to row $j$. Note that if $E_j^{(i)}=I + \alpha_ij\cdot e_ie_j^*$ and $E_{j+1}^{i}=I + \alpha_i(j+1)\cdot e_ie_{j+1}^*$ (note $e_i^*e_{j+1}=0$) then

\begin{align*}
	E_{j+1}^{(i)}E_j^{(i)}&=(I + \alpha_{i(j+1)}\cdot e_ie_{j+1}^*)(I + \alpha_{ij}\cdot e_ie_j^*) \\
	&=I + \alpha_{ij}\cdot e_ie_j^* + \alpha_{i(j+1)}\cdot e_ie_{j+1}^* + \alpha_{ij}\alpha_{ij}\cdot e_i(e_{j+1}^*e_i)e_j^* \\
	&= I +\alpha_{ij}\cdot e_ie_j^* + \alpha_{i(j+1)}\cdot e_ie_{j+1}^*
\end{align*}

So, for a fixed column $i=1, \ldots, n-1$ we have 

\begin{align*}
	T_i = E_{n}^{(i)} \cdots E_{i+2}^{(i)}E_{i+1}^{(i)}&=(I + \alpha_{in}\cdot e_ie_n^*)\cdots (I + \alpha_{i(i+2)}\cdot e_ie_{i+2}^*)(I + \alpha_{i(i+1)}\cdot e_ie_{i+1}^*) \\
	&=I + \alpha_{i(i+1)}\cdot e_ie_{i+1}^* + \alpha_{i(i+2)}\cdot e_ie_{i+2}^* + \cdots + \alpha_{in}\cdot e_ie_{n}^* \\
	&= I + c_ie_i^*
\end{align*}

Where

\[
c_i=
\begin{bmatrix}
    0 \\
    \vdots \\
    0 \\
    \alpha_{i(i+1)} \\
    \vdots \\
    \alpha_{in}
\end{bmatrix}
\]

Here, $$\alpha_{ij}=\frac{( T_{i-1}\cdots T_1A )_{ij}}{( T_{i-1}\cdots T_1A ) _{ii}}$$

For instance, assume that $A$ has no zero pivots so no row interchanging is necessary and hence $(T_{i-1}\cdots T_1A )_{ii}=e_i^*T_{i-1}\cdots T_1Ae_i\neq 0$ for all $i$. We aim to reduce the first column, this is, compute $T_1$. For this, we need 

\[
c_i=
\begin{bmatrix}
    0 \\
    \alpha_{12} \\
    \vdots \\
    \alpha_{1n}
\end{bmatrix}
\]

Where $\alpha_{1j}=\frac{A_{1j}}{A_{11}}$

For this, let us assume we have $e_1$, this indicate the column we aim to reduce. We'll first get 

\[
a = 
\begin{bmatrix}
    0 \\
    A_{12} \\
    \vdots \\
    A_{1n}
\end{bmatrix}
\]

And then do $\text{apply}[\div]\left( a, (e_1^*Ae_1)\cdot\texttt{ones}(e_1) \right)$. Now, define $$\texttt{col}(A,e_j):=\for X,v. (e_jZv)(vAe_j)v + X$$

This gives us the lower diagonal entries of column $j$ of $A$. Then $$c_i=\text{apply}[\div]\left(\texttt{col}(A, e_1), (e_1^*Ae_1)\cdot\texttt{ones}(e_1) \right)$$ So

\begin{align*}
	T_1&=I+c_1e_1^* \\
	&=I + \left[ \text{apply}[\div]\left(\texttt{col}(A, e_1), (e_1^*Ae_1)\cdot\texttt{ones}(e_1) \right)\right]e_1^* 
\end{align*}

More generally, let $A_i=T_{i-1} \cdots T_1A$, this is, $A_i$ is $A$ whit its first $i-1$ columns reduced. Then, to reduce the $i$-th column of $A_i$ we compute $T_{i}$ as 

\begin{align*}
	T_{i}&=I+c_{i}e_{i}^* \\
	&=I + \left[ \text{apply}[\div]\left(\texttt{col}(A_i, e_{i}), (e_{i}^*A_ie_{i})\cdot\texttt{ones}(e_{i}) \right)\right]e_{i}^* 
\end{align*}

Not that this still is not useful, since it depends on $e_i$, but we will use the above expression that computes $T_i$ to compute $L^{-1}$. To this purpose, we define $$\texttt{reduce}(A,e_i)= I + \left[ \text{apply}[\div]\left(\texttt{col}(A, e_i), (e_i^*Ae_i)\cdot\texttt{ones}(e_i) \right)\right]e_i^* $$ This returns $T_i$ such that $T_iA$ is $A$ with its $i$-th column reduced. Now we do $$L^{-1}=\for X,v. (1-v^*v_{min})\texttt{reduce}(XA,v)X + (v^*v_{min})\texttt{reduce}(A,v)$$

\subsubsection{LU factorization in MATLANG + for}
Let's consider the following setting of the $A=LU$ factorization:

\begin{align*}
A_i&=T_{i-1}A_{i-1},\hspace{1ex}i=2, \cdots, n \\
A_1&=A \\
\end{align*}

Note that $A_n=U$ and 
\begin{align*}
A_i&=T_{i-1}A_{i-1} \\
&=\texttt{reduce}(A_{i-1},e_i)A_{i-1}.
\end{align*}

Now, if $A=LU$ and $$E(X, v)=(1-v^*v_{min})\texttt{reduce}(XA,v)X + (v^*v_{min})\texttt{reduce}(A,v)X$$
then $U=(\for X, v.E(X,v))\cdot A$, this is $L^{-1}=\for X, v.E(X,v)$.

\subsubsection{PA=LU factorization}

Permutation matrices are elementary matrices of the form $$P=I-uu^*$$ denotes row interchange (if multiplied by left) of rows $i+k$ and $j+k$ if $u=(e_{i+k}-e_{j+k})$. Now if $T_{k} = I-c_ke_k^*$ note that $P^2=I$ and $e_k^*P=e_k^*$, since $e_{k}$ has zeros in positions $i+k$ and $j+k$. Thus $$PT_kP=P^2-Pc_ke_k^*P=I-\widehat{c}_ke_k^*$$ Where $\widehat{c}_k=Pc_k$.

So far we assume that $A$ needs no row interchange during the $LU$ factorization process. Let's see if wee actually need row interchange immediately before step $k$ (reduce the $k$-th column of $A_{k}$) such that $1 \leq k < n$. Let $P$ be the elementary permutation matrix that is needed. This means that $A_{k+1}=\widehat{T}_kA_k=T_kPT_{k-1}\cdots T_1A$ where $\widehat{T}_k=T_{k}P$. The factorization ends up like $$U=T_{n-1}\cdots T_{k}PT_{k-1}\cdots T_1A = T_{n-1}\cdots \widehat{T}_{k}T_{k-1}\cdots T_1A$$. Note that $P$ is necessary if the $(k,k)$ entry of $A_{k}$ is zero. Let 

\[
a_k = 
\begin{bmatrix}
    0 \\
    0 \\
    \vdots \\
    A_{kk} \\
     \vdots \\
    A_{kn}
\end{bmatrix}
\]

We aim to compute a function that, given $a_k$ and $e_k$, outputs $P$ such that

\[
Pa_k = 
\begin{bmatrix}
    0 \\
    0 \\
    \vdots \\
    0 \\
    A_{ki} \\
    A_{k(k+1)}\\
    \vdots \\
    A_{k(i-1)} \\
    A_{kk} \\
    A_{k(i+1)} \\
    \vdots \\
    A_{kn}
\end{bmatrix}
\]

if $A_{kk}=0$ where $i$ is the first such that $A_ki\neq 0$ and $k<i\leq n$ (it must exists otherwise $A$ is not $PA=LU$ factorizable) and returns $I$ otherwise (no row interchanging is necessary). We define
\begin{align*}
\texttt{perm}(a,e_k)&=I - \\
 &\for X,v. \dfrac{(e_kZv)(vZ_{eq}Xv_{max})(a^*v)}{(Xv_{max})(Xv_{max})-v^*v_{min}}\left[ (e_k-v)(e_k-v^*)+vv_{max}^*-v_{max}v_{max}^*\right] \\
 & + (v^*v_{min})v_{max}v_{max}^* + X
\end{align*}

This way if $A$ is $PA=LU$ factorizable and 

\begin{align*}
	E(X, v)&=(1-v^*v_{min})\texttt{reduce}(\texttt{perm}(\texttt{col}(XA,v),v) \cdot X \cdot A,v) \texttt{perm}(\texttt{col}(XA,v),v)X \\
	&+ (v^*v_{min})\texttt{reduce}(A,v)\texttt{perm}(\texttt{col}(A,v),v)
\end{align*}
Then $U=(\for X, v.E(X,v))\cdot A$, this is $L^{-1}P=\for X, v.E(X,v)$.

\subsubsection{Independence of starting matrix}

Would it be different if in the definition of $\for$we start with some matrix $B$ and not the zero matrix? We show that we can simulate this using our setting. Let us have an operator $\lambda$ that does exactly what $\for$does, except that we have to give another parameter: a starting matrix. This is:

\begin{align*}
\lambda X, v.E(X,v)(B)&=A_n \\
A_i&=E(A_{i-1}, v_{i}),\hspace{1ex}i=1,\cdots, n \\
A_0&=B
\end{align*}
Now, we use the following expression with the operator $\for$to simulate $\lambda$: $$E'(X,v)=min(v)\cdot E(B,v)+(1-min(v))E(X,v).$$
Note that for $\lambda$ we have 
\begin{align*}
	A^{\lambda}_0&=B \\
	A^{\lambda}_1&=E(A^{\lambda}_0,v_1)=E(B,v_1) \\
	A^{\lambda}_2&=E(A^{\lambda}_1,v_2) \\
	&\vdots \\
	A^{\lambda}_n&=E(A^{\lambda}_{n-1}, v_n)
\end{align*}
and for $\for$
\begin{align*}
	A^{\for}_0&=\mathbf{0} \\
	A^{\for}_1&=E'(A^{\for}_0,v_1)=E(B,v_1) \\
	A^{\for}_2&=E'(A^{\for}_1,v_2)=E(A^{\for}_1,v_2) \\
	&\vdots \\
	A^{\for}_n&=E'(A^{\for}_{n-1}, v_n)=E(A^{\for}_{n-1}, v_n).
\end{align*}
Since $A^{\for}_1=A^{\lambda}_1$ we have that $A^{\for}_i=A^{\lambda}_i$ for $i=1,\ldots,n$. So $$\lambda X, v.E(X,v)(B)=\for X, v.E'(X,v)$$