\section{MATLANG extended with FOR}

\subsection{Syntax and semantics of MATLANG with FOR}

A vocabulary $\cS$ is a set of matrix names $\cM=M_1,\ldots, M_p$, a set of vector variable names $\cV=v_1,\ldots, v_l$ and a set of matrix variable names $\cX=X_1,\ldots, X_c$.
An instance $\cI$ is a function that maps matrix names to concrete matrices of a determined dimension $(n,m)$, vector variable names to a determined dimension $(n, 1)$ and matrix variable names to a determined dimension $(n,m)$.
We also assume the presence of $F$, a set of functions $f:\mathbb{R}^{k}\rightarrow\mathbb{R}$ (with $1\leq k$).

Let $\cS=(\cM, \cV, \cX)$ be a vocabulary, a formula in the vocabulary $\cS$ is of the form:
$$
E:= M\in \cM \ |\ v\in \cV \ |\ X\in \cX \ |\    E^* \ |\ E_1+E_2 \ |\  E_1\cdot E_2 \ |\  f(E_1,\ldots, E_k), f\in F \ |\  \for v,X. E
$$

In the last expression $v\in \cV, X\in \cX$.

Now, given a vocabulary $\cS$, an instance $\cI$ and a set of functions $F$, we define the well typedness of a formula of $\cS$ according to $\cI$ as follows. 

\begin{align*}
\text{type}(M) &= (n,m) \text{ if } \cI(M) \text{ is a } n\times m \text{ matrix } \\
\text{type}(v) &= (n,1) \text{ if } \cI(v) = (n, 1) \\
\text{type}(X) &= (n,m) \text{ if } \cI(X) = (n, m) \\
\text{type}(E^*)&=(j,i) \text{ where } (i,j)=\text{type}(E) \\
\text{type}(E_1 + E_2) &= \text{type}(E_1) \text{ if } \text{type}(E_1)=\text{type}(E_2) \\
\text{type}(E_1\cdot E_2)&= (i,k) \text{ if } \text{type}(E_1)=(i,j)\text{ and type}(E_2)=(j,k) \\
\text{type}(f(E_1,\ldots, E_k))&=(1,1) \text{ if } \text{type}(E_1)=\cdots =\text{type}(E_k)=(1,1) \\
\text{type}\left( \for X,v. E\right) &= \text{type}(E) \text{ if } \text{type}(X)=\text{type}(E)
\end{align*}

We say that $E$ is well typed if it has a defined type. Now we will proceed with the semantics of these formulas.
First, we define $\text{type}(v)_1 = n_v$ as the first coordinate of the type of $v$ and $\cI[v:= e^n_i]$ as the instance $\cI$ except that $v$ is mapped to the $i-$th canonical vector of size $n$.
Now, given $\cS, \cI, F$ and a well typed expression $E$, we define the semantics for the values $\sem{E}(\cS,\cI)$ inductively as follows.

\begin{align*}
\sem{M}(\cS,\cI)&=\cI (M) \\
\sem{E^*}(\cS,\cI)&=\sem{E}(\cS,\cI)^* \\
\sem{E_1+E_2}(\cS,\cI)&= \sem{E_1}(\cS,\cI)+\sem{E_2}(\cS,\cI)\\
\sem{E_1\cdot E_2}(\cS,\cI)&= \sem{E_1}(\cS,\cI)\times \sem{E_2}(\cS,\cI)\\
\sem{f(E_1,\ldots ,E_k)}(\cS,\cI) &= f\left(\sem{E_1}(\cS,\cI),\ldots , \sem{E_k}(\cS,\cI)\right)\\
\sem{ \for X,v. E }(\cS,\cI) &= A_n \text{ where }
	\begin{cases}
	               A_0 = 0 \\
 	               A_i = \sem{E}(\cS,\cI[v:= e^{n_v}_i, X:=A_{i-1}]), i=1,\ldots, n_v
	\end{cases}
\end{align*}

Here, $+$ is matrix sum and $\times$ is matrix multiplication. Depending on context, we sometimes denote explicitly that an expression $E$ depends on $v$ or $X$ as $E(v)$, $E(X)$ or $E(v,X)$.

On the other hand, expressions that depend on a variable $v$ or $X$ can be well typed according to the typing rules, but we cannot infer a value (semantic) unless $v$ and $X$ are instantiated by $\for v,X. E$.

\subsection{Order}

Let's turn our attention to the order of the canonical vectors $\lbrace v_i\rbrace_i$. If we have the canonical vectors, using a simple matrix product we can compute a function that discriminates if two canonical vectors are the same or not, in particular
\[
  			f(v_i,v_j)=1-v_i^*v_j=\begin{cases}
               0 \text{ if } i=j \\
               1 \text{ if } i\neq j
            \end{cases}
		\]
does exactly that. So, without adding any object, we have equality for canonical vectors. Note that implicitly we are using the identity, the matrix equivalent to the equality relation. We can compute this as $$ I = \for X, v. X+vv^*$$

Can we get the matrix equivalent to a \textit{order} relation? The $\for$operator actually iterates over the canonical vector, so it makes sense that we can get this order matrices using this operator. First we are going to use a little trick with the $\for$operator. We compute$$v_{max} = \for X,v. v$$ This allows us to have the last canonical vector in every iteration and thus we have access to the last column of $X$ in every iteration, so we can sort of use the last column of $X$ as a placeholder for some vector. For example, to compute the matrix
\[
Z_{eq} = \begin{bmatrix}
    1 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 1 
\end{bmatrix}.
\]

We do $$Z_{\text{eq}}=\for v,X. X + \left[ (Xv_{max}) + v \right]v^* + vv^*_{max}.$$ This iteration does the following: add the current stored vector $Xv_{max}$ plus $v$ to the column of $X$ indicated by $v$ and then add $v$ to the last column of $X$ (the stored vector).

Also, we have

\[
  			f(v_i, v_j)=v_i^*Z_{eq}v_j=\begin{cases}
               1 \text{ if } i \leq j \\
               0 \text{ if } i > j
            \end{cases}
\]

This $Z_{eq}$ matrix discriminate between canonical vectors in terms of order. Note that this gives us an answer for any pair of canonical vector, so we have a total order in some sense.
Now, consider the following matrix
\[
Z:=Z_{eq} - I = \begin{bmatrix}
    0 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 0 
\end{bmatrix}.
\]
In some way, this matrix gives us an strict total ordering of the canonical vectors.
\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]
		
This kind of functions discriminate between canonical vectors, in terms of order.

Can we introduce a \textit{local} order of the canonical vectors? For instance, we can ask ourselves if we can define the \textit{successor} or \textit{predecessor} relation of the canonical vectors from a matrix. Let us introduce a new matrix:

\[
S := \begin{bmatrix}
    0 & 1         & 0         & \cdots &  0 \\
    0 & \ddots & 1         & \cdots & 0 \\
    \vdots & \vdots & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & \ddots & 1 \\
    0 & \cdots & \cdots & \cdots & 0 
\end{bmatrix}.
\]

We can compute this as $$S = \for X,v. X+\left[ (1 - v^*v_{max})vv_{max}^* - (Xv_{max}) v_{max}^* + (Xv_{max})v^*\right]$$ and note that

\[
  			Sv_i=\begin{cases}
               v_{i-1} \text{ if } i > 1 \\
              \mathbf{0} \text{ if } i = 1
            \end{cases}
		\]

So it's the matrix equivalent of the \textit{predecessor} relation. For example, now we can compute 

\[
  			g(v_i, v_j)=v_i^*Sv_j=\begin{cases}
               1 \text{ if } i = j - 1 \\
               0 \text{ if not}
            \end{cases}
		\]
		
With this matrix we can compute a function that tells us if the argument is the first canonical vector. Note that we have $$\mathbf{1}\cdot Su=0\leftrightarrow u=v_1, \hspace{1ex} u\in\lbrace v_1,\ldots, v_n\rbrace,$$ and we have that if $u\in\lbrace v_1,\ldots, v_n\rbrace$ then
\[
  			min(u)=(1-\mathbf{1}^*Su)=\begin{cases}
               1 \text{ if } u = v_1 \\
               0 \text{ if not}
            \end{cases}
		\]
		
This is enough since if it is used over vector variables it will allow us to identify when $v=e_1$. We actually do $min(u)=(1-\text{ones}(\text{diag}(u))^*Su)$, we show how to do $diag$ and $ones$ in the next section. This way, we can get $$v_{min} = \for X,v. v\cdot min(v)$$ 

\subsection{Simulating MATLANG}

When $E=E(v)$ there exists useful operations that arise frequently. The first one is to sum the same expression but iterating over canonical vectors. We define this as $$\ssum v. E(v)=\for v, X. X + E(v).$$

The second one is similar, but with multiplication: $$\sprod v. E(v)=\for X,v. X\cdot E(v) + min(v)\cdot E(v)$$
Here we need to identify the first iteration to not multiply by the zero matrix.

\subsubsection{Matrix pointwise function application (apply[f])}

We have that 

\begin{itemize}
	\item \textbf{Pointwise application:} if $A^{(1)}, \ldots, A^{(n)}$ are $m\times p$ matrices, then apply$\left[ f \right](A^{(1)}, \ldots, A^{(n)})$ is the $m\times p$ matrix $C$ where $C_{ij}=f(A^{(1)}_{ij}, \ldots, A^{(n)}_{ij})$.
\end{itemize}

So, given the \textbf{sum} operator and the function $f$, we can compute $C$ in the following way: $$C=\ssum x_i.\ssum x_j. f\left( x_i^*A^{(1)}x_j, \cdots, x_i^*A^{(n)}v_j\right)\cdot x_ix_j^*$$

Recall that $v_iv_j^*$ is the matrix that has a 1 in the position i, j and zero everywhere else.

Thus, we can simulate the MATLANG operator apply[$f$].

For example, matrix pointwise multiplication: $$\ssum x_i.\ssum x_j.\left( x_i^*A^{(1)}x_j\times \cdots\times x_i^*A^{(n)}x_j\right)\cdot x_ix_j^*$$

\subsubsection{Ones vector and diag()}

Note that $$\text{ones}(M)=\text{apply}\left[ f_{x\rightarrow 1}\right] \left(\ssum v. Mv\right).$$ And $$\text{diag}(u)=\ssum v. (v^*u)\cdot vv^*$$

\subsubsection{Identity}

We can compute the $n\times n$ identity as $$\ssum v. v \cdot v^*$$ if $\cI(v)=(n,1)$.

\subsubsection{Matrix multiplication via sum operator}

Note that using the sum operator we can compute matrix multiplication by definition $$A\cdot B = \ssum v_i\ssum v_j\ssum v_k \left( v_iAv_k\cdot v_kBv_j\right)v_iv_j^*$$

\subsection{Examples}

Let's see some examples of what we can do with MATLANG with FOR.

\subsubsection{Trace and diagonal product}

We can express $$tr(A)=\ssum v. v^*Av.$$

And the product of the diagonal of a matrix (this can be useful in computing the determinant of an upper/down triangular matrix): $$dp(A)=\sprod v. v^*Av.$$

\subsubsection{Transitive closure}

To compute transitive closure we need the quantity $(A+I)^n$ and use apply[$f>0$] on the result matrix. Then we have that $$(I+A)^n=\sprod v. (I+A),$$ and thus $$TC = \text{apply}\left[ f_{>0}\right]\left( \sprod v. (I+A) \right).$$
Using only the new operators, we have that $$TC=\ssum v_i. \ssum v_j. f_{>0}\left(v_i^*\left(\sprod v. (I+A)\right)v_j\right)$$

\subsubsection{$k$-cliques}

Let's try to see first if there is a four clique in the adjacency matrix $A$. To do this, we need to verify if there are paths between four \textbf{different} nodes. So we need the function 

\[
  			f(u,v)=1-u^*v=\begin{cases}
               0 \text{ if } u=v \\
               1 \text{ if } u\neq v
            \end{cases}
		\]

Define $$g(u,v,w,r)=f(u,v)\cdot f(u,w)\cdot f(u,r)\cdot f(v,w)\cdot f(v,r)\cdot f(w,r).$$ This is, $g$ is zero if any pair of vectors are the same.
		
So $$\ssum v_1.\ssum v_2. \ssum v_3. \ssum v_4. (v_1^*Av_2)(v_1^*Av_3)(v_1^*Av_4)(v_2^*Av_3)(v_2^*Av_4)(v_3^*Av_4)g(v_1,v_2,v_3,v_4).$$

\subsubsection{Power sum}

If we want to compute $I+ A + A^2 + \cdots + A^n$ we need to do a little trick with the identity. In this case, we need to add results of matrix powers. We do this like $$\ssum v.\sprod w. \left( (A-I)(wZv) + I\right).$$

To see why this works, note that
\[
  			wZv=\begin{cases}
               1 \text{ if } w<v \\
               0 \text{ if } w\geq v
            \end{cases}
		\]
This is, for a sum term $A^k$, if $w$ is previous to $v$ (in the canonical vectors ordering), then stop multiplying $A$ and use $I$ until the end of the current sum term, and then add this term to the final result.

\subsubsection{Pivoting a column}

Let $A$ be a $n\times n$ matrix and $\alpha$ a scalar. For computing Gaussian elimination we turn our attention in the elementary matrices of the form $$E=I + \alpha\cdot e_ie_j^*.$$ Note that $E\cdot A$ adds a $\alpha$-multiple of row $i$ to row $j$. It is worth noting that $E^{-1}= I - \alpha\cdot e_ie_j^*.$

The key property is that, if $A$ is $LU$ factorizable without any row interchange, then $A=LU$, for some upper diagonal matrix $U$ and $L^{-1}=T_{n-1}\cdots T_1$ where $T_i=E_{n-1}^{(i)}\cdots E_{i}^{(i)}$ for some elementary matrices $E_j=I + \alpha_{ij}\cdot e_ie_j^*.$ Here $T_iA$ results in $A$ with its $i-$th column reduced, and $E_j^{(i)}$ adds an $\alpha_{ij}$ multiple of row $i$ to row $j+1$.

For instance, assume that $A$ has no zero pivots (no row interchanging is necessary), we aim to reduce the first column. Let us do the first reduction, this is, substract a multiple of the first row to the second row. Let $\lbrace v_i\rbrace_i$ be the canonical vectors. We first need to find the proper $\alpha$ to ponderate the first row. In gaussian elimination this is the first entry of the second row divided by the first entry of the first row, this is $$\alpha = \dfrac{v_2^*Av_1}{v_1^*Av_1}.$$
So $$E_1= I - \alpha_1\cdot v_1v_2^*$$
and $E_1A=A'$ where $A'$ has the second row reduced (first entry zero).

Do this for the rest of the rows and we will have $$E_{n-1}\cdots E_1A=A'$$ and
\[
  			A'_{i1}=\begin{cases}
               A_{11} \text{ if } i =1 \\
               0 \text{ if } i > 1
            \end{cases}
		\]

Note that

\begin{align*}
A'&=E_{n-1}\cdots E_1A \\
&=\left( E_1^*\cdots E_{n-1}^*\right)^*A \\
&=\left(\prod_{k}E_k^*\right)^*A \\
&=\left(\sprod_{v_k\neq v_1}\left(I-\alpha_k\cdot v_1v_k^*\right)^*\right)^*A \\
&=\left(\sprod_{v_k\neq v_1}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\right)^*\right)^*A
\end{align*}

And we get $A$ with the first column reduced. Note that we need $v_k\neq v_1$ because we don't want to reduce the first row since it will be set to all zeroes. Furthermore, we want to reduce only the downward rows. This can be done with the function 
\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Thus $$A'=\left(\sprod v_k.\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\cdot\left(v_1^*Zv_k\right)\right)^*\right)^*A.$$

So we can reduce the column $i$ of $A$ as $$\left(\sprod v_k.\left(I-\dfrac{v_k^*Av_i}{v_i^*Av_i}\cdot v_iv_k^*\cdot\left(v_i^*Zv_k\right)\right)^*\right)^*A.$$

Not that this still is not useful, since it depends on $v_i$, but we will use the above expression that computes $T_i$ to compute $L^{-1}$

\subsubsection{LU factorization in MATLANG + for}
Let's consider the following setting of the $A=LU$ factorization:

\begin{align*}
A_i&=T_{i-1}A_{i-1},\hspace{1ex}i=2, \cdots, n \\
A_1&=A \\
\end{align*}

Note that $A_n=U$ and 
\begin{align*}
A_i&=T_{i-1}A_{i-1} \\
&=\left(\sprod v_k.\left(I-\dfrac{v_k^*A_{i-1}v_i}{v_i^*A_{i-1}v_i}\cdot v_iv_k^*\cdot\left(v_i^*Zv_k\right)\right)^*\right)^*A_{i-1}.
\end{align*}

Now, if $A=LU$ and $$E(X, v)=\left(\sprod v_k.\left(I-\dfrac{v_k^*(XA)v}{v^*(XA)v}\cdot vv_k^*\cdot\left(v^*Zv_k\right)\right)^*\right)^*X$$
then $U=(\for X, v.E(X,v))\cdot A$, this is $L^{-1}=\for X, v.E(X,v)$.

\subsubsection{Independence of starting matrix}

Would it be different if in the definition of $\for$we start with some matrix $B$ and not the zero matrix? We show that we can simulate this using our setting. Let us have an operator $\lambda$ that does exactly what $\for$does, except that we have to give another parameter: a starting matrix. This is:

\begin{align*}
\lambda X, v.E(X,v)(B)&=A_n \\
A_i&=E(A_{i-1}, v_{i}),\hspace{1ex}i=1,\cdots, n \\
A_0&=B
\end{align*}
Now, we use the following expression with the operator $\for$to simulate $\lambda$: $$E'(X,v)=min(v)\cdot E(B,v)+(1-min(v))E(X,v).$$
Note that for $\lambda$ we have 
\begin{align*}
	A^{\lambda}_0&=B \\
	A^{\lambda}_1&=E(A^{\lambda}_0,v_1)=E(B,v_1) \\
	A^{\lambda}_2&=E(A^{\lambda}_1,v_2) \\
	&\vdots \\
	A^{\lambda}_n&=E(A^{\lambda}_{n-1}, v_n)
\end{align*}
and for $\for$
\begin{align*}
	A^{\for}_0&=\mathbf{0} \\
	A^{\for}_1&=E'(A^{\for}_0,v_1)=E(B,v_1) \\
	A^{\for}_2&=E'(A^{\for}_1,v_2)=E(A^{\for}_1,v_2) \\
	&\vdots \\
	A^{\for}_n&=E'(A^{\for}_{n-1}, v_n)=E(A^{\for}_{n-1}, v_n).
\end{align*}
Since $A^{\for}_1=A^{\lambda}_1$ we have that $A^{\for}_i=A^{\lambda}_i$ for $i=1,\ldots,n$. So $$\lambda X, v.E(X,v)(B)=\for X, v.E'(X,v)$$