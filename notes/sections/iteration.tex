\section{Iterations in extMATLANG}
It is natural for us to wish for some kind of iteration in MATLANG. There's a lot of matrix procedures and formulas that sums or multiplies results over an operation on the same matrix. Some examples:
\begin{itemize}
	\item The quantity $$\sum_{i=0}^nA^i=I+A+A^2+\ldots + A^n$$ it's used a lot in computing inverse or graph theory. This is a sum over the operation power to the $i$ of the matrix $A$.
	\item In the LU factorization process the result is $$A=G_1^{-1}G_2^{-2}\cdots G_n^{-1}U=LU$$ for some upper triangular matrix $U$ and some pivot matrices $G_1, \ldots, G_n$. Note that $$L=\prod_{i=1}^nG_{i}^{-1}$$
\end{itemize}

We have seen some examples of what we can compute using extMATLANG expressions. The extra property of these expressions relies on the operators $\ssum$ and $\sprod$. If we want to do more elaborate things, like iterations, we need some extra tool: \textbf{order}.

\subsection{Order}

Let's turn our attention to the order of the canonical vectors $\lbrace v_i\rbrace_i$. If we have the canonical vectors, using a simple matrix product we can compute a function that discriminates if two canonical vectors are the same or not, in particular
\[
  			f(v_i,v_j)=1-v_i^*v_j=\begin{cases}
               0 \text{ if } i=j \\
               1 \text{ if } i\neq j
            \end{cases}
		\]
does exactly that. So, without adding any object, we have equality for canonical vectors. Note that implicitly we are using de identity, the matrix equivalent to the equality relation.


Now, consider the following matrix
\[
Z = \begin{bmatrix}
    0 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 0 
\end{bmatrix}.
\]
In some way, this matrix gives us an ordering of the canonical vectors. We can see this in the functions of the form 

\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]
		
This kind of functions discriminate between canonical vectors, in terms of order. Note that this gives us an answer for any pair of canonical vector, so we have a total order in some sense.

Can we introduce a \textit{local} order of the canonical vectors? For instance, we can ask ourselves if we can define the \textit{successor} or \textit{predecessor} relation of the canonical vectors from a matrix. Let us introduce a new matrix:

\[
S := \begin{bmatrix}
    0 & 1         & 0         & \cdots &  0 \\
    0 & \ddots & 1         & \cdots & 0 \\
    \vdots & \vdots & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & \ddots & 1 \\
    0 & \cdots & \cdots & \cdots & 0 
\end{bmatrix}.
\]

Let's see what this matrix does. Note that

\[
  			Sv_i=\begin{cases}
               v_{i-1} \text{ if } i > 1 \\
              \mathbf{0} \text{ if } i = 1
            \end{cases}
		\]

So it's the matrix equivalent of the \textit{predecessor} relation. For example, now we can compute 

\[
  			g(v_i, v_j)=v_i^*Sv_j=\begin{cases}
               1 \text{ if } i = j - 1 \\
               0 \text{ if not}
            \end{cases}
		\]
		
With this matrix we can compute a function that tells us if the argument is the first canonical vector. Note that we don't have exactly $$\mathbf{1}\cdot Su=0\leftrightarrow u=cv_1, \hspace{1ex} c\in\mathbb{R},$$ it depends on the entries of $u$, but  $$\mathbf{1}\cdot f_{\neq 0}(Su)=0\leftrightarrow u=cv_1, \hspace{1ex} c\in\mathbb{R},$$ and we have that
\[
  			min(u)=(1-\mathbf{1}\cdot f_{\neq 0}(Su))=\begin{cases}
               1 \text{ if } u = cv_1 \\
               0 \text{ if not}
            \end{cases}
		\]

Since $S$ and $Z$ give us an order, it is natural to compute one from the other, so

\begin{align*}
Z&=f_{>0}\left(\sprod v. (S+I)\right) \\
S&=Z-f_{>0}(Z^2)
\end{align*}

\subsection{Examples in extMATLANG + Order}

Now, let's see what we can do with these new operators and an order. We assume that we have $\cM,\nu, V$. Also, the matrices $S,Z$.

\subsubsection{Power sum}

If we want to compute $I+ A + A^2 + \cdots + A^n$ we need to do a little trick with the identity. In this case, we need to add results of matrix powers. We do this like $$\ssum v.\sprod w. \left( (A-I)(wZv) + I\right).$$

To see why this works, note that
\[
  			wZv=\begin{cases}
               1 \text{ if } w<v \\
               0 \text{ if } w\geq v
            \end{cases}
		\]
This is, for a sum term $A^k$, if $w$ is previous to $v$ (in the canonical vectors ordering), then stop multiplying $A$ and use $I$ until the end of the current sum term, and then add this term to the final result.

\subsubsection{Pivoting a column}

Let $A$ be a matrix and $\alpha$ a scalar. For computing Gaussian elimination we turn our attention in the elementary matrices of the form $$E=I + \alpha\cdot e_ie_j^*.$$ Note that $E\cdot A$ adds a $\alpha$-multiple of row $i$ to row $j$. It is worth noting that $E^{-1}= I - \alpha\cdot e_ie_j^*.$

The key property is that, if $A$ is $LU$ factorizable without any row interchange, then $A=LU$, for some upper diagonal matrix $U$ and $L=(E_1\cdots E_k)^{-1}=E_k^{-1}\cdots E_1^{-1}$ for some elementary matrices $E_i=I + \alpha_i\cdot e_ie_j^*.$

For instance, assume that $A$ has no zero pivots (no row interchanging is necessary), we aim to reduce the first column. Let us do the first reduction, this is, substract a multiple of the first row to the second row. Let $\lbrace v_i\rbrace_i$ be the canonical vectors. We first need to find the proper $\alpha$ to ponderate the first row. In gaussian elimination this is the first entry of the second row divided by the first entry of the first row, this is $$\alpha = \dfrac{v_2^*Av_1}{v_1^*Av_1}.$$
So $$E_1= I - \alpha_1\cdot v_1v_2^*$$
and $E_1A=A'$ where $A'$ has the second row reduced (first entry zero).

Do this for the rest of the rows and we will have $$E_k\cdots E_1A=A'$$ and
\[
  			A'_{i1}=\begin{cases}
               A_{11} \text{ if } i =1 \\
               0 \text{ if } i > 1
            \end{cases}
		\]

Note that

\begin{align*}
A'&=E_k\cdots E_1A \\
&=\left( E_1^*\cdots E_k^*\right)^*A \\
&=\left(\prod_{k}E_k^*\right)^*A \\
&=\left(\sprod_{v_k\neq v_1}\left(I-\alpha_k\cdot v_1v_k^*\right)^*\right)^*A \\
&=\left(\sprod_{v_k\neq v_1}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\right)^*\right)^*A
\end{align*}

And we get $A$ with the first column reduced. Note that we need $v_k\neq v_1$ because we don't want to reduce the first row since it will be set to all zeroes. Furthermore, we want to reduce only the downward rows. This can be done with the function 
\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Thus $$A'=\left(\sprod v_k.\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\cdot\left(v_1^*Zv_k\right)\right)^*\right)^*A.$$

So we can reduce the column $i$ of $A$ as $$\left(\sprod v_k.\left(I-\dfrac{v_k^*Av_i}{v_i^*Av_i}\cdot v_iv_k^*\cdot\left(v_i^*Zv_k\right)\right)^*\right)^*A.$$

But this is all we can do: reduce a column. For example, we could wrongfully think that all that is left to complete the gaussian elimination on $A=LU$ is to do this for all columns, this is $$U=\left(\sprod v_i.\sprod v_j.\left(I-\dfrac{v_j^*Av_i}{v_i^*Av_i}\cdot v_iv_j^*\cdot\left(v_i^*Zv_j\right)\right)^*\right)^*A.$$
But note that here we obtain the multipliers $\alpha_{ij}$ from the original matrix $A$, a huge mistake. So all we can do with these new operators is to reduce one column of a given matrix. It's not a weak property, but not powerful enough.


\label{sec:iteration}