\section{Iterations in MATLANG}
It is natural for us to wish for some kind of iteration in MATLANG. There's a lot of matrix procedures and formulas that sums or multiplies results over an operation on the same matrix. Some examples:
\begin{itemize}
	\item The quantity $$\sum_{i=0}^nA^i=I+A+A^2+\ldots + A^n$$ it's used a lot in computing inverse or graph theory. This is a sum over the operation power to the $i$ of the matrix $A$.
	\item In the LU factorization process the result is $$A=G_1^{-1}G_2^{-2}\cdots G_n^{-1}U=LU$$ for some upper triangular matrix $U$ and some pivot matrices $G_1, \ldots, G_n$. Note that $$L=\prod_{i=1}^nG_{i}^{-1}$$
\end{itemize}

We can compute this and many other things using extMATLANG expressions. The extra property of these expressions relies on the operators $\sum$ and $\prod$. 

First of all, note that the definition of these operators is not ambiguous in taking column vectors instead of row vectors or doing the aggregate matrix multiplication by right, because row vectors are the canonical vectors as well, and transposing gives us one from the other. On the other hand, if you want the result of the aggregate multiplication to be computed in the inverse order, note that, since $(AB)^*=B^*A^*$ we have $$\prod^{\text{left}} x.E(x)=\left(\prod x.E(x)^*\right)^*.$$

Now, let's see what we can do with these new operators. Some of these examples are simulations of original MATLANG matrix operations. As a consecuence, the semantics of these operators are well defined and we can use them as well. We assume that we have $I,\nu, V$.

\subsection*{Identity}

We can express the identity of dimention $dim(I)$ as $$\sum x. x\cdot x^*.$$

\subsection*{Ones vector and diag()}

Note that $$\text{ones}(v)=\sum x. x$$ And $$\text{diag}(v)=\sum x. (x^*v)\cdot xx^*$$

\subsection*{Matrix pointwise function application (apply[f])}

We have that 

\begin{itemize}
	\item \textbf{Pointwise application:} if $A^{(1)}, \ldots, A^{(n)}$ are $m\times p$ matrices, then apply$\left[ f \right](A^{(1)}, \ldots, A^{(n)})$ is the $m\times p$ matrix $C$ where $C_{ij}=f(A^{(1)}_{ij}, \ldots, A^{(n)}_{ij})$.
\end{itemize}

So, given the \textbf{sum} operator and the function $f$, we can compute $C$ in the following way: $$C=\sum x_i.\sum x_j. f\left( x_i^*A^{(1)}x_j, \cdots, x_i^*A^{(n)}v_j\right)\cdot x_ix_j^*$$

Recall that $v_iv_j^*$ is the matrix that has a 1 in the position i, j and zero everywhere else.

Thus, we can simulate the MATLANG operator apply[$f$].

For example, matrix pointwise multiplication: $$\sum x_i.\sum x_j.\left( x_i^*A^{(1)}x_j\times \cdots\times x_i^*A^{(n)}x_j\right)\cdot x_ix_j^*$$

\subsection*{Trace and diagonal product}

We can express $$tr(A)=\sum x. x^*Ax.$$

And the product of the diagonal of a matrix (this can be useful in computing the determinant of an upper/down triangular matrix): $$dp(A)=\prod x. x^*Ax.$$


\subsection*{Determinant}

Recall that the determinant is define over permutations, this is $$det(A)=\sum_{p}\sigma(p)a_{1p_1}\cdots a_{np_n},$$ where the sum is taken over the $n!$ permutations of the natural order $(1, 2,\ldots, n)$ and 
       \[
  			\sigma(p)=\begin{cases}
               +1 \text{ if p can be restored to natural order in an even number of steps} \\
               -1 \text{ if p can be restored to natural order in an odd number of steps}
            \end{cases}
		\]
In the case of MATLANG, the permutations are represented as permutation matrices, this is, permutations of the identity. In this context, note that, for a permutation matrix $P$, we compute $a_{1p_1}\cdots a_{np_n}$ as $$\prod x. x^*\cdot A\cdot (Px).$$ See that $a_{ip_i}=v_i^*\cdot A\cdot (Pv_i)$, where $v_i$ is the $i$-th canonical vector. 


We also need to compute $\sigma(P)$, to this end, note that $$\sigma(P)=(-1)^{\sum_{i<j}\lbrace p_i>p_j\rbrace}.$$ Also, let $\lbrace v_i\rbrace_{i=1}^{n}$ be the canonical vectors.
Now, note that 

 		\[
  			2\cdot v_i^*Zv_j=\begin{cases}
               2 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]
		
Thus

		\[
  			1-2\cdot v_i^*Zv_j=\begin{cases}
               -1 \text{ if } i < j \\
               +1 \text{ if } i \geq j
            \end{cases}
		\]

So $$\sigma(P)=\prod x_i.\prod x_j:(i<j).\left(1-2\cdot (Px_i)^*Z(Px_j)\right),$$ and since 

		\[
  			v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Then we have that $$\sigma(P)=\prod x_i.\prod x_j.\left(1-2\cdot (Px_i)^*Z(Px_j)\cdot x_i^*Zx_j\right)$$

Thus, given we can iterate over permutation matrices, we have $$det(A)=\sum_{P}\left[\left(\prod x_i \prod x_j \left(1-2\cdot (Px_i)^*Z(Px_j)\cdot x_i^*Zx_j\right)\right)\left(\prod y. y^*\cdot A\cdot (Py)\right)\right].$$



\subsection*{Transitive closure}

To compute transitive closure we need the quantity $(A+I)^n$ and use apply[$f>0$] on the result matrix. Then we have that $$(I+A)^n=\prod v. (I+A),$$ and thus $$TC = \text{apply}\left[ f_{>0}\right]\left( \prod v. (I+A) \right).$$
Using only the new operators, we have that $$TC=\sum v_i. \sum v_j. f_{>0}\left(v_i^*\left(\prod v. (I+A)\right)v_j\right)$$

\subsection*{Power sum}

If we want to compute $I+ A + A^2 + \cdots + A^n$ we need to do a little trick with the identity. In this case, we need to add results of matrix powers. We do this like $$\sum v.\prod w. \left( (A-I)(wZv) + I\right).$$

To see why this works, note that
\[
  			wZv=\begin{cases}
               1 \text{ if } w<v \\
               0 \text{ if } w\geq v
            \end{cases}
		\]
This is, for a sum term $A^k$, if $w$ is previous to $v$ (in the canonical vectors ordering), then stop multiplying $A$ and use $I$ until the end of the current sum term, and then add this term to the final result.

\subsection*{$k$-cliques}

Let's try to see first if there is a four clique in the adjacency matrix $A$. To do this, we need to verify if there are paths between four \textbf{different} nodes. So we need the function 

\[
  			f(u,v)=1-u^*v=\begin{cases}
               0 \text{ if } u=v \\
               1 \text{ if } u\neq v
            \end{cases}
		\]

Define $$g(u,v,w,r)=f(u,v)\cdot f(u,w)\cdot f(u,r)\cdot f(v,w)\cdot f(v,r)\cdot f(w,r).$$ This is, $g$ is zero if any pair of vectors are the same.
		
So $$\sum v_1.\sum v_2. \sum v_3. \sum v_4. (v_1^*Av_2)(v_1^*Av_3)(v_1^*Av_4)(v_2^*Av_3)(v_2^*Av_4)(v_3^*Av_4)g(v_1,v_2,v_3,v_4).$$

\subsection*{Gaussian elimination}

Let $A$ be a matrix and $\alpha$ a scalar. For computing Gaussian elimination we turn our attention in the elementary matrices of the form $$E=I + \alpha\cdot e_ie_j^*.$$ Note that $E\cdot A$ adds a $\alpha$-multiple of row $i$ to row $j$. It is worth noting that $E^{-1}= I - \alpha\cdot e_ie_j^*.$

The key property is that, if $A$ is $LU$ factorizable without any row interchange, then $A=LU$, for some upper diagonal matrix $U$ and $L=(E_1\cdots E_k)^{-1}=E_k^{-1}\cdots E_1^{-1}$ for some elementary matrices $E_i=I + \alpha_i\cdot e_ie_j^*.$

For instance, assume that $A$ has no zero pivots (no row interchanging is necessary), we aim to reduce the first column. Let us do the first reduction, this is, substract a multiple of the first row to the second row. Let $\lbrace v_i\rbrace_i$ be the canonical vectors. We first need to find the proper $\alpha$ to ponderate the first row. In gaussian elimination this is the first entry of the second row divided by the first entry of the first row, this is $$\alpha = \dfrac{v_2^*Av_1}{v_1^*Av_1}.$$
So $$E_1= I - \alpha_1\cdot v_1v_2^*$$
and $E_1A=A'$ where $A'$ has the second row reduced (first entry zero).

Do this for the rest of the rows and we will have $$E_k\cdots E_1A=A'$$ and
\[
  			A'_{i1}=\begin{cases}
               A_{11} \text{ if } i =1 \\
               0 \text{ if } i > 1
            \end{cases}
		\]

Note that

\begin{align*}
A'&=E_k\cdots E_1A \\
&=\left( E_1^*\cdots E_k^*\right)^*A \\
&=\left(\prod_{k}E_k^*\right)^*A \\
&=\left(\prod_{v_k\neq v_1}\left(I-\alpha_k\cdot v_1v_k^*\right)^*\right)^*A \\
&=\left(\prod_{v_k\neq v_1}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\right)^*\right)^*A
\end{align*}

And we get $A$ with the first column reduced. Note that we need $v_k\neq v_1$ because we don't want to reduce the first row since it will be set to all zeroes. Furthermore, we want to reduce only the downward rows. This can be done with the function 
\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Thus $$A'=\left(\prod_{v_k}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\cdot\left(v_1^*Zv_k\right)\right)^*\right)^*A.$$

So we can reduce the column $i$ of $A$ as $$\left(\prod_{v_k}\left(I-\dfrac{v_k^*Av_i}{v_i^*Av_i}\cdot v_iv_k^*\cdot\left(v_i^*Zv_k\right)\right)^*\right)^*A.$$

But this is all we can do: reduce a column. For example, we could wrongfully think that all that is left to complete the gaussian elimination on $A=LU$ is to do this for all columns, this is $$U=\left(\prod_{v_i}\prod_{v_j}\left(I-\dfrac{v_j^*Av_i}{v_i^*Av_i}\cdot v_iv_j^*\cdot\left(v_i^*Zv_j\right)\right)^*\right)^*A.$$
But note that here we obtain the multipliers $\alpha_{ij}$ from the original matrix $A$, a huge mistake. So all we can do with these new operators is to reduce one column of a given matrix. It's not a weak property, but not powerful enough.

\subsection*{Recursive iteration}

Given the above problem, we sort of need an operator that iterates over the canonical vectors and compute the same operation over the result of the previous iteration.

Let $E(X,v)$ be a two variable expression. We define

$$\mu X, v.E(X,v)$$

as the operator that iterates over the canonical vectors $\lbrace v_i\rbrace_{i=1}^n$ and successively computes $E(A_{i-1}, v_i)$ where $A_i=E(A_{i-1}, v_{i})$ and $A_0=I$. The value of $\mu X, v.E(X,v)$ is $A_n$. This is

\begin{align*}
\mu X, v.E(X,v)&=A_n \\
A_i&=E(A_{i-1}, v_{i}),\hspace{1ex}i=1,\cdots, n \\
A_0&=I
\end{align*}

Formally, let $\lbrace v_i\rbrace_i$ be the canonical vectors of addecuate dimention. On instance $I$, we have that $$\left[\mu X, v. E(X,v)\right](I)=B$$ if and only if

\begin{align*}
I &= A_0 \\
\left[E(X, v)\right](I\left[X\rightarrow A_{i-1}, v\rightarrow v_{i}\right])&=A_i,\hspace{1ex}i=1,\cdots, n \\
A_n &= B
\end{align*}

\subsubsection*{LU factorization with recursive iteration}
Let's consider the following setting of the $A=LU$ factorization:

\begin{align*}
A_i&=T_{i-1}A_{i-1},\hspace{1ex}i=2, \cdots, n \\
A_1&=A \\
\end{align*}

Note that $A_n=U$ and 
\begin{align*}
A_i&=T_{i-1}A_{i-1} \\
&=\left(\prod_{v_k}\left(I-\dfrac{v_k^*A_{i-1}v_i}{v_i^*A_{i-1}v_i}\cdot v_iv_k^*\cdot\left(v_i^*Zv_k\right)\right)^*\right)^*A_{i-1}.
\end{align*}

Now, if $A=LU$ and $$E(X, v)=\left(\prod_{v_k}\left(I-\dfrac{v_k^*(XA)v}{v^*(XA)v}\cdot vv_k^*\cdot\left(v^*Zv_k\right)\right)^*\right)^*X$$
then $U=(\mu X, v.E(X,v))(A)$, this is $L^{-1}=\mu X, v.E(X,v)$.

Note that in the example above we \textit{inserted} the target matrix $A$ in the expression $E(X,v)$. Would it be different if in the definition of $\mu$ we start with some matrix $B$ and not the indentity? We show that we can simulate this using our setting. Let us have an operator $\lambda$ that does exactly what $\mu$ does, except that we have to give another parameter: a starting matrix. This is:

\begin{align*}
\lambda X, v.E(X,v)(B)&=A_n \\
A_i&=E(A_{i-1}, v_{i}),\hspace{1ex}i=1,\cdots, n \\
A_0&=B
\end{align*}
Now, we use the following expression with the operator $\mu$ to simulate $\lambda$: $$E'(X,v)=min(v)\cdot E(B,v_1)+(1-min(v))E(X,v).$$
Note that for $\lambda$ we have 
\begin{align*}
	A^{\lambda}_0&=B \\
	A^{\lambda}_1&=E(A^{\lambda}_0,v_1)=E(B,v_1) \\
	A^{\lambda}_2&=E(A^{\lambda}_1,v_2) \\
	&\vdots \\
	A^{\lambda}_n&=E(A^{\lambda}_{n-1}, v_n)
\end{align*}
and for $\mu$
\begin{align*}
	A^{\mu}_0&=I \\
	A^{\mu}_1&=E'(A^{\mu}_0,v_1)=E(B,v_1) \\
	A^{\mu}_2&=E'(A^{\mu}_1,v_2)=E(A^{\mu}_1,v_2) \\
	&\vdots \\
	A^{\mu}_n&=E'(A^{\mu}_{n-1}, v_n)=E(A^{\mu}_{n-1}, v_n).
\end{align*}
Since $A^{\mu}_1=A^{\lambda}_1$ we have that $A^{\mu}_i=A^{\lambda}_i$ for $i=1,\ldots,n$. So $$\lambda X, v.E(X,v)(B)=\mu X, v.E'(X,v)$$

\subsection*{Current main questions}
\begin{itemize}

\item $PA=LU$ factorization: if $A$ needs row interchange, is there some way to compute $P$ beforehand?
\begin{itemize}
\item I am not sure whether you can do this beforehand.
\item But, isn't it possible to simulate \emph{partial pivoting}, i.e., when dealing with canonical vector $v_i$, extract the $i$th column $X[*,i]$ from the current
matrix, find the maximal elements in $X[i-n,i]$ and corresponding indicator vector for this (i.e., ones on positions where the max value occurs), then somehow
reduce this to a vector in which, say only the smallest occurrence $k$ of ones is retained (smallest position), and then turn this into a permutation matrix that exchanges
all rows $X[j,*]$ for $j\geq i$ with the $k$th row, resulting in permuted version of $X$, on which you can then proceed as before for the $i$th step? This will require nested
recursion.
\end{itemize}
\item Study if we can express other factorizations ($LDU, LDL$,
Cholesky, etc.).
\item Starting from $A=LU$, can we compute $A^{-1}$ easily? It's possible that is the gaussian elimination upwards, this is, reduce $A$ to the identity.
\item The importance of $Z$ is that it gives us an order. Can we compute $Z$ with the new operators?
\item Besides function application, are there any other expressions where the sum operator is useful?
\item Can we compute $A^{dim(A)}$?
\item Explore the possibility to define the sum and product operators over invariant expressions, and then extend the definition through $Z$.

\end{itemize}

\subsubsection*{Observations}

\begin{itemize}
	\item Note that if we have
	\[
  			f(x)=\begin{cases}
               1 \text{ if } P(x) \\
               0 \text{ if } Q(x)
            \end{cases}
		\]
		If needed, we can compute a piecewise function with any values, like 
		\[
  			b - (b - a)f(x)=\begin{cases}
               a \text{ if } P(x) \\
               b \text{ if } Q(x)
            \end{cases}
		\]
		
\end{itemize}
\label{sec:iteration}