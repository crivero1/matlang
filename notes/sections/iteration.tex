\section{Iterations in MATLANG}
It is natural for us to wish for some kind of iteration in MATLANG. There's a lot of matrix procedures and formulas that sums or multiplies results over an operation on the same matrix. Some examples:
\begin{itemize}
	\item The quantity $$\sum_{i=0}^nA^i=I+A+A^2+\ldots + A^n$$ it's used a lot in computing inverse or graph theory. This is a sum over the operation power to the $i$ of the matrix $A$.
	\item In the LU factorization process the result is $$A=G_1^{-1}G_2^{-2}\cdots G_n^{-1}U=LU$$ for some upper triangular matrix $U$ and some pivot matrices $G_1, \ldots, G_n$. Note that $$L=\prod_{i=1}^nG_{i}^{-1}$$
\end{itemize}

With this in mind, we introduce the following operators:
\begin{itemize}
	\item Sum: $$\sum v.e(v)$$ means that, for every column vector $v$ of the identity of addecuate dimention (this is, $v$ are canonical vectors) do $e(v)$ and sum the result (pointwise). More formally, let $\lbrace v_i\rbrace_i$ be the canonical vectors of addecuate dimention. On instance $I$, we have that $$\left[\sum v. e(v)\right](I)=A$$ if and only if $$\left[ e(v)\right](I[v\rightarrow v_1]) + \left[ e(v)\right](I[v\rightarrow v_2]) + \cdots + \left[ e(v)\right](I[v\rightarrow v_n]) = A.$$ Here, $+$ is matrix addition.
	\item Multiplication: $$\prod_{v}e(v)$$ means that, for every column vector $v$ of the identity of addecuate dimention do $e(X,v_i)$ and matrix multiply the results $R_i$ as they come up, this is, $R_1R_2\cdots R_n$. More formally, let $\lbrace v_i\rbrace_i$ be the canonical vectors of addecuate dimention. On instance $I$, we have that $$\left[\prod v. e(v)\right](I)=A$$ if and only if $$\left[ e(v)\right](I[v\rightarrow v_1]) \times \left[ e(v)\right](I[v\rightarrow v_2]) \times \cdots \times \left[ e(v)\right](I[v\rightarrow v_n]) = A$$ Here, $\times$ is matrix multiplication.
\end{itemize}

Obviously, these operations are valid when the dimentions of the matrices match the wanted operation. \\

Also, note that the definition is not ambiguous in taking column vectors instead of row vectors or doing the aggregate matrix multiplication by right, because row vectors are the canonical vectors as well. On the other hand, if you want the result of the aggregate multiplication to be computed in the inverse order, note that, since $(AB)^*=B^*A^*$ we have $$\prod_{v}^{\text{left}}e(v)=\left(\prod_{v}e(v)^*\right)^*.$$

Now, let's see what we can do with these new operators.

\subsection*{Trace and diagonal product}

We can express $$tr(A)=\sum_v v^*Av.$$

And the product of the diagonal of a matrix (this can be useful in computing the determinant of an upper/down triangular matrix): $$dp(A)=\prod_v v^*Av.$$


\subsection*{Determinant}

Recall that the determinant is define over permutations, this is $$det(A)=\sum_{p}\sigma(p)a_{1p_1}\cdots a_{np_n},$$ where the sum is taken over the $n!$ permutations of the natural order $(1, 2,\ldots, n)$ and 
       \[
  			\sigma(p)=\begin{cases}
               +1 \text{ if p can be restored to natural order in an even number of steps} \\
               -1 \text{ if p can be restored to natural order in an odd number of steps}
            \end{cases}
		\]
In the case of MATLANG, the permutations are represented as permutation matrices, this is, permutations of the identity. In this context, note that, for a permutation matrix $P$, we compute $a_{1p_1}\cdots a_{np_n}$ as $$\prod_v v^*\cdot A\cdot (Pv).$$ See that $a_{ip_i}=v_i^*\cdot A\cdot (Pv_i)$, where $v_i$ is the $i$-th canonical vector. 


We also need to compute $\sigma(P)$, to this end, note that $$\sigma(P)=(-1)^{\sum_{i<j}\lbrace p_i>p_j\rbrace}.$$ Also, let $e_i$ be the canonical vector with a $1$ in its $i$-th entry and 

\[
Z = \begin{bmatrix}
    0 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 0 
\end{bmatrix}.
\]
Now, note that 

 		\[
  			2\cdot e_i^*Ze_j=\begin{cases}
               2 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]
		
Thus

		\[
  			1-2\cdot e_i^*Ze_j=\begin{cases}
               -1 \text{ if } i < j \\
               +1 \text{ if } i \geq j
            \end{cases}
		\]

So $$\sigma(P)=\prod_{e_i}\prod_{e_j:i<j}\left(1-2\cdot (Pe_i)^*Z(Pe_j)\right),$$ and since 

		\[
  			e_i^*Ze_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Then we have that $$\sigma(P)=\prod_{e_i}\prod_{e_j}\left(1-2\cdot (Pe_i)^*Z(Pe_j)\cdot e_i^*Ze_j\right)$$

Thus, given we can iterate over permutation matrices, we have $$det(A)=\sum_{P}\left(\prod_{e_i}\prod_{e_j}\left(1-2\cdot (Pe_i)^*Z(Pe_j)\cdot e_i^*Ze_j\right)\right)\left(\prod_v v^*\cdot A\cdot (Pv)\right).$$

\subsection*{Scalar functions are enough}

We have that 

\begin{itemize}
	\item \textbf{Pointwise application:} if $A^{(1)}, \ldots, A^{(n)}$ are $m\times p$ matrices, then apply$\left[ f \right](A^{(1)}, \ldots, A^{(n)})$ is the $m\times p$ matrix $C$ where $C_{ij}=f(A^{(1)}_{ij}, \ldots, A^{(n)}_{ij})$.
\end{itemize}

So, given the \textbf{sum} operator and the function $f$, we can compute $C$ in the following way: $$C=\sum_{v_i}\sum_{v_j}f\left( v_i^*A^{(1)}v_j, \cdots, v_i^*A^{(n)}v_j\right)\cdot v_iv_j^*$$

Recall that $v_iv_j^*$ is the matrix that has a 1 in the position i, j and zero everywhere else.

Thus, the operator apply$\left[ f \right]$ is no longer needed.

For example:

\begin{itemize}
	\item \textbf{Matrix sum:}$$\sum_{v_i}\sum_{v_j}\left( v_i^*A^{(1)}v_j+ \cdots + v_i^*A^{(n)}v_j\right)\cdot v_iv_j^*$$
	\item \textbf{Matrix pointwise multiplication:} $$\sum_{v_i}\sum_{v_j}\left( v_i^*A^{(1)}v_j\times \cdots\times v_i^*A^{(n)}v_j\right)\cdot v_iv_j^*$$
\end{itemize}

\subsection*{Gaussian elimination}

Let $A$ be a matrix and $\alpha$ a scalar. For computing Gaussian elimination we turn our attention in the elementary matrices of the form $$E=I + \alpha\cdot e_ie_j^*.$$ Note that $E\cdot A$ adds a $\alpha$-multiple of row $i$ to row $j$. It is worth noting that $E^{-1}= I - \alpha\cdot e_ie_j^*.$

The key property is that, if $A$ is $LU$ factorizable without any row interchange, then $A=LU$, for some upper diagonal matrix $U$ and $L=(E_1\cdots E_k)^{-1}=E_k^{-1}\cdots E_1^{-1}$ for some elementary matrices $E_i=I + \alpha_i\cdot e_ie_j^*.$

For instance, assume that $A$ has no zero pivots (no row interchanging is necessary), we aim to reduce the first column. Let us do the first reduction, this is, substract a multiple of the first row to the second row. Let $\lbrace v_i\rbrace_i$ be the canonical vectors. We first need to find the proper $\alpha$ to ponderate the first row. In gaussian elimination this is the first entry of the second row divided by the first entry of the first row, this is $$\alpha = \dfrac{v_2^*Av_1}{v_1^*Av_1}.$$
So $$E_1= I - \alpha_1\cdot v_1v_2^*$$
and $E_1A=A'$ where $A'$ has the second row reduced (first entry zero).

Do this for the rest of the rows and we will have $$E_k\cdots E_1A=A'$$ and
\[
  			A'_{i1}=\begin{cases}
               A_{11} \text{ if } i =1 \\
               0 \text{ if } i > 1
            \end{cases}
		\]

Note that

\begin{align*}
A'&=E_k\cdots E_1A \\
&=\left( E_1^*\cdots E_k^*\right)^*A \\
&=\left(\prod_{k}E_k^*\right)^*A \\
&=\left(\prod_{v_k\neq v_1}\left(I-\alpha_k\cdot v_1v_k^*\right)^*\right)^*A \\
&=\left(\prod_{v_k\neq v_1}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\right)^*\right)^*A
\end{align*}

And we get $A$ with the first column reduced. Note that we need $v_k\neq v_1$ because we don't want to reduce the first row since it will be set to all zeroes. Furthermore, we want to reduce only the downward rows. This can be done with the function 
\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Thus $$A'=\left(\prod_{v_k}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\cdot\left(v_1^*Zv_k\right)\right)^*\right)^*A.$$

So we can reduce the column $i$ of $A$ as $$\left(\prod_{v_k}\left(I-\dfrac{v_k^*Av_i}{v_i^*Av_i}\cdot v_iv_k^*\cdot\left(v_i^*Zv_k\right)\right)^*\right)^*A.$$

But this is all we can do: reduce a column. For example, we could wrongfully think that all that is left to complete the gaussian elimination on $A=LU$ is to do this for all columns, this is $$U=\left(\prod_{v_i}\prod_{v_j}\left(I-\dfrac{v_j^*Av_i}{v_i^*Av_i}\cdot v_iv_j^*\cdot\left(v_i^*Zv_j\right)\right)^*\right)^*A.$$
But note that here we obtain the multipliers $\alpha_{ij}$ from the original matrix $A$, a huge mistake.

\subsection*{Recursive iteration}

Given the above problem, we sort of need an operator that iterates over the canonical vectors and compute the same operation over the result of the previous iteration.

Let $\phi(X,v)$ be an expression. We define

$$\mu v.\phi(A,v)$$

as the operator that iterates over the canonical vectors $\lbrace v_i\rbrace_{i=1}^n$ and successively computes $\phi(X_i,v_i)$ where $X_i=\phi(X_{i-1}, v_{i-1})$ and $X_1=A$. The value of $\mu v.\phi(A,v)$ is $\phi(X_n, v_n)$. This is

\begin{align*}
\mu v.\phi(A,v)&=\phi(X_n, v_n) \\
X_i&=\phi(X_{i-1}, v_{i-1}),\hspace{1ex}i=2,\cdots, n \\
X_1&=A
\end{align*}

Formally, let $\lbrace v_i\rbrace_i$ be the canonical vectors of addecuate dimention. On instance $I$, we have that $$\left[\mu v. \phi(A,v)\right](I)=B$$ if and only if

\begin{align*}
X_1&=\left[\phi(X,v)\right](I\left[X\rightarrow A, v\rightarrow v_1\right]) \\
X_i&=\left[\phi(X, v)\right](I\left[X\rightarrow X_{i-1}, v\rightarrow v_{i-1}\right]),\hspace{1ex}i=2,\cdots, n
\end{align*}

Let's consider the following setting of the $A=LU$ factorization:

\begin{align*}
A_i&=T_{i-1}A_{i-1},\hspace{1ex}i=2, \cdots, n \\
A_1&=A \\
\end{align*}

Note that $A_n=U$ and 
\begin{align*}
A_i&=T_{i-1}A_{i-1} \\
&=\left(\prod_{v_k}\left(I-\dfrac{v_k^*A_{i-1}v_i}{v_i^*A_{i-1}v_i}\cdot v_iv_k^*\cdot\left(v_i^*Zv_k\right)\right)^*\right)^*A_{i-1}.
\end{align*}

Now, if $A=LU$ and $$\phi(X, v)=\left(\prod_{v_k}\left(I-\dfrac{v_k^*Xv}{v^*Xv}\cdot vv_k^*\cdot\left(v^*Zv_k\right)\right)^*\right)^*X,$$
then $U=\mu v.\phi(A,v)$.

\subsection*{Transitive closure}

To compute transitive closure we need the quantity $(A+I)^n$ and use apply[$f>0$] on the result matrix. Then we have that $$(I+A)^n=\prod v. (I+A),$$ and thus $$TC = \text{apply}\left[ f_{>0}\right]\left( \prod v. (I+A) \right).$$
Using only the new operators, we have that $$TC=\sum v_i. \sum v_j. f_{>0}\left(v_i^*\left(\prod v. (I+A)\right)v_j\right)$$

\subsection*{Power sum}

If we want to compute $I+ A + A^2 + \cdots + A^n$ we need to do a little trick with the identity. In this case, we need to add results of matrix powers. We do this like $$\sum v.\prod w. \left( (A-I)(wZv) + I\right).$$

To see why this works, note that
\[
  			wZv=\begin{cases}
               1 \text{ if } w<v \\
               0 \text{ if } w\geq v
            \end{cases}
		\]
This is, for a sum term $A^k$, if $w$ is previous to $v$ (in the canonical vectors ordering), then stop multiplying $A$ and use $I$ until the end of the current sum term, and then add this term to the final result.

\subsection*{$k$-cliques}

Let's try to see first if there is a four clique in the adjacency matrix $A$. To do this, we need to verify if there are paths between four \textbf{different} nodes. So we need the function 

\[
  			f(u,v)=1-u^*v=\begin{cases}
               0 \text{ if } u=v \\
               1 \text{ if } u\neq v
            \end{cases}
		\]

Define $$g(u,v,w,r)=f(u,v)\cdot f(u,w)\cdot f(u,r)\cdot f(v,w)\cdot f(v,r)\cdot f(w,r).$$ This is, $g$ is zero if any pair of vectors are the same.
		
So $$\sum v_1.\sum v_2. \sum v_3. \sum v_4. (v_1^*Av_2)(v_1^*Av_3)(v_1^*Av_4)(v_2^*Av_3)(v_2^*Av_4)(v_3^*Av_4)g(v_1,v_2,v_3,v_4).$$

\subsection*{Current main questions}
\begin{itemize}

\item $PA=LU$ factorization: if $A$ needs row interchange, is there some way to compute $P$ beforehand?
\begin{itemize}
\item I am not sure whether you can do this beforehand.
\item But, isn't it possible to simulate \emph{partial pivoting}, i.e., when dealing with canonical vector $v_i$, extract the $i$th column $X[*,i]$ from the current
matrix, find the maximal elements in $X[i-n,i]$ and corresponding indicator vector for this (i.e., ones on positions where the max value occurs), then somehow
reduce this to a vector in which, say only the smallest occurrence $k$ of ones is retained (smallest position), and then turn this into a permutation matrix that exchanges
all rows $X[j,*]$ for $j\geq i$ with the $k$th row, resulting in permuted version of $X$, on which you can then proceed as before for the $i$th step? This will require nested
recursion.
\end{itemize}
\item Study if we can express other factorizations ($LDU, LDL$,
Cholesky, etc.).
\item Starting from $A=LU$, can we compute $A^{-1}$ easily? It's possible that is the gaussian elimination upwards, this is, reduce $A$ to the identity.
\item The importance of $Z$ is that it gives us an order. Can we compute $Z$ with the new operators?
\item Besides function application, are there any other expressions where the sum operator is useful?
\item Can we compute $A^{dim(A)}$?
\item Explore the possibility to define the sum and product operators over invariant expressions, and then extend the definition through $Z$.

\end{itemize}

\subsubsection*{Observations}

\begin{itemize}
	\item Note that if we have
	\[
  			f(x)=\begin{cases}
               1 \text{ if } P(x) \\
               0 \text{ if } Q(x)
            \end{cases}
		\]
		If needed, we can compute a piecewise function with any values, like 
		\[
  			b - (b - a)f(x)=\begin{cases}
               a \text{ if } P(x) \\
               b \text{ if } Q(x)
            \end{cases}
		\]
		
\end{itemize}
\label{sec:iteration}