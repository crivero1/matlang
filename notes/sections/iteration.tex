\section{Iterations in MATLANG}
It is natural for us to wish for some kind of iteration in MATLANG. There's a lot of matrix procedures and formulas that sums or multiplies results over an operation on the same matrix. Some examples:
\begin{itemize}
	\item The quantity $$\sum_{i=0}^nA^i=I+A+A^2+\ldots + A^n$$ it's used a lot in computing inverse or graph theory. This is a sum over the operation power to the $i$ of the matrix $A$.
	\item In the LU factorization process the result is $$A=G_1^{-1}G_2^{-2}\cdots G_n^{-1}U=LU$$ for some upper triangular matrix $U$ and some pivot matrices $G_1, \ldots, G_n$. Note that $$L=\prod_{i=1}^nG_{i}^{-1}$$
\end{itemize}

With this in mind, we introduce the following operators:
\begin{itemize}
	\item Sum: $$\sum_{v}e(X,v)$$ means that, for every column vector $v$ of the identity of addecuate dimention (this is, $v$ are canonical vectors) do $e(X, v)$ and sum the result (pointwise).
	\item Multiplication: $$\prod_{v}e(X,v)$$ means that, for every column vector $v$ of the identity of addecuate dimention do $e(X,v_i)$ and matrix multiply the results $R_i$ as they come up, this is, $R_1R_2\cdots R_n$.
\end{itemize}

Obviously, these operations are valid when the dimentions of the matrices match the wanted operation. \\

Also, note that the definition is not ambiguous in taking column vectors instead of row vectors or doing the aggregate matrix multiplication by right, because row vectors are the canonical vectors as well. On the other hand, if you want the result of the aggregate multiplication to be computed in the inverse order, note that, since $(AB)^*=B^*A^*$ we have $$\prod_{v}^{\text{left}}e(X,v)=\left(\prod_{v}e(X,v)^*\right)^*.$$

Now, let's see what we can do with these new operators.

\subsection*{Trace and diagonal product}

We can express $$tr(A)=\sum_v v^*Av.$$

And the product of the diagonal of a matrix (this can be useful in computing the determinant of an upper/down triangular matrix): $$dp(A)=\prod_v v^*Av.$$


\subsection*{Determinant}

Recall that the determinant is define over permutations, this is $$det(A)=\sum_{p}\sigma(p)a_{1p_1}\cdots a_{np_n},$$ where the sum is taken over the $n!$ permutations of the natural order $(1, 2,\ldots, n)$ and 
       \[
  			\sigma(p)=\begin{cases}
               +1 \text{ if p can be restored to natural order in an even number of steps} \\
               -1 \text{ if p can be restored to natural order in an odd number of steps}
            \end{cases}
		\]
In the case of MATLANG, the permutations are represented as permutation matrices, this is, permutations of the identity. In this context, note that, for a permutation matrix $P$, we compute $a_{1p_1}\cdots a_{np_n}$ as $$\prod_v v^*\cdot A\cdot (Pv).$$ See that $a_{ip_i}=v_i^*\cdot A\cdot (Pv_i)$, where $v_i$ is the $i$-th canonical vector. 


We also need to compute $\sigma(P)$, to this end, note that $$\sigma(P)=(-1)^{\sum_{i<j}\lbrace p_i>p_j\rbrace}.$$ Also, let $e_i$ be the canonical vector with a $1$ in its $i$-th entry and 

\[
Z = \begin{bmatrix}
    0 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 0 
\end{bmatrix}.
\]
Now, note that 

 		\[
  			2\cdot e_i^*Ze_j=\begin{cases}
               2 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]
		
Thus

		\[
  			1-2\cdot e_i^*Ze_j=\begin{cases}
               -1 \text{ if } i < j \\
               +1 \text{ if } i \geq j
            \end{cases}
		\]

So $$\sigma(P)=\prod_{e_i}\prod_{e_j:i<j}\left(1-2\cdot (Pe_i)^*Z(Pe_j)\right),$$ and since 

		\[
  			e_i^*Ze_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Then we have that $$\sigma(P)=\prod_{e_i}\prod_{e_j}\left(1-2\cdot (Pe_i)^*Z(Pe_j)\cdot e_i^*Ze_j\right)$$

Thus, given we can iterate over permutation matrices, we have $$det(A)=\sum_{P}\left(\prod_{e_i}\prod_{e_j}\left(1-2\cdot (Pe_i)^*Z(Pe_j)\cdot e_i^*Ze_j\right)\right)\left(\prod_v v^*\cdot A\cdot (Pv)\right).$$

\subsection*{Escalar functions are enough}

We have that 

\begin{itemize}
	\item \textbf{Pointwise application:} if $A^{(1)}, \ldots, A^{(n)}$ are $m\times p$ matrices, then apply$\left[ f \right](A^{(1)}, \ldots, A^{(n)})$ is the $m\times p$ matrix $C$ where $C_{ij}=f(A^{(1)}_{ij}, \ldots, A^{(n)}_{ij})$.
\end{itemize}

So, given the \textbf{sum} operator and the function $f$, we can compute $C$ in the following way: $$C=\sum_{v_i}\sum_{v_j}f\left( v_i^*A^{(1)}v_j, \cdots, v_i^*A^{(n)}v_j\right)\cdot v_iv_j^*$$

Recall that $v_iv_j^*$ is the matrix that has a 1 in the position i, j and zero everywhere else.

Thus, the operator apply$\left[ f \right]$ is no longer needed.

For example:

\begin{itemize}
	\item \textbf{Matrix sum:}$$\sum_{v_i}\sum_{v_j}\left( v_i^*A^{(1)}v_j+ \cdots + v_i^*A^{(n)}v_j\right)\cdot v_iv_j^*$$
	\item \textbf{Matrix pointwise multiplication:} $$\sum_{v_i}\sum_{v_j}\left( v_i^*A^{(1)}v_j\times \cdots\times v_i^*A^{(n)}v_j\right)\cdot v_iv_j^*$$
\end{itemize}

\subsection*{Gaussian elimination}

Let $A$ be a matrix and $\alpha$ a scalar. For computing Gaussian elimination we turn our attention in the elementary matrices of the form $$E=I + \alpha\cdot e_ie_j^*.$$ Note that $E\cdot A$ adds a $\alpha$-multiple of row $i$ to row $j$. It is worht noting that $E^{-1}= I - \alpha\cdot e_ie_j^*.$

The key property is that, if $A$ is $LU$ factorizable without any row interchange, then $A=LU$, for some upper diagonal matrix $U$ and $L=(E_1\cdots E_k)^{-1}=E_k^{-1}\cdots E_1^{-1}$ for some elementary matrices $E_i=I + \alpha_i\cdot e_ie_j^*.$

For instance, assume that $A$ has no zero pivots (no row interchanging is necessary), we aim to reduce the first column. Let us do the first reduction, this is, substract a multiple of the first row to the second row. Let $\lbrace v_i\rbrace_i$ be the canonical vectors. We first need to find the proper $\alpha$ to ponderate the first row. In gaussian elimination this is the first entry of the second row divided by the first entry of the first row, this is $$\alpha = \dfrac{v_2^*Av_1}{v_1^*Av_1}.$$
So $$E_1= I - \alpha_1\cdot v_1v_2^*$$
and $E_1A=A'$ where $A'$ has the second row reduced (first entry zero).

Do this for the rest of the rows and we will have $$E_k\cdots E_1A=A'$$ and
\[
  			A'_{i1}=\begin{cases}
               A_{11} \text{ if } i =1 \\
               0 \text{ if } i > 1
            \end{cases}
		\]

Note that

\begin{align*}
A'&=E_k\cdots E_1A \\
&=\left( E_1^*\cdots E_k^*\right)^*A \\
&=\left(\prod_{k}E_k^*\right)^*A \\
&=\left(\prod_{v_k\neq v_1}\left(I-\alpha_k\cdot v_1v_k^*\right)^*\right)^*A \\
&=\left(\prod_{v_k\neq v_1}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\right)^*\right)^*A
\end{align*}

And we get $A$ with the first column reduced. Note that we need $v_k\neq v_1$ because we don't want to reduce the first row since it will be set to all zeroes. Furthermore, we want to reduce only the downward rows. This can be done with the function 
\[
  			f(v_i, v_j)=v_i^*Zv_j=\begin{cases}
               1 \text{ if } i < j \\
               0 \text{ if } i \geq j
            \end{cases}
		\]

Thus $$A'=\left(\prod_{v_k}\left(I-\dfrac{v_k^*Av_1}{v_1^*Av_1}\cdot v_1v_k^*\cdot\left(v_1^*Zv_k\right)\right)^*\right)^*A.$$

All that is left to complete the gaussian elimination on $A=LU$ is to do this for all columns, this is $$U=\left(\prod_{v_i}\prod_{v_j}\left(I-\dfrac{v_j^*Av_i}{v_i^*Av_i}\cdot v_iv_j^*\cdot\left(v_i^*Zv_j\right)\right)^*\right)^*A.$$
\subsection*{Current main questions}
\begin{itemize}

\item $PA=LU$ factorization: if $A$ needs row interchange, is there some way to compute $P$ beforehand?
\item Study if we can express other factorizations ($LDU, LDL$,
Cholesky, etc.).
\item Starting from $A=LU$, can we compute $A^{-1}$ easily? It's possible that is the gaussian elimination upwards, this is, reduce $A$ to the identity.
\item The importance of $Z$ is that it gives us an order. Can we compute $Z$ with the new operators?
\item Besides function application, is there anything else that the sum operator is useful?
\item Can we compute $A^{dim(A)}$?
\item Explore the possibility to define the sum and product operators over invariant expressions, and then extend the definition through $Z$.

\end{itemize}

\subsubsection*{Observations}

\begin{itemize}
	\item Note that if we have
	\[
  			f(x)=\begin{cases}
               1 \text{ if } P(x) \\
               0 \text{ if } Q(x)
            \end{cases}
		\]
		If needed, we can compute a piecewise function with any values, like 
		\[
  			b - (b - a)f(x)=\begin{cases}
               a \text{ if } P(x) \\
               b \text{ if } Q(x)
            \end{cases}
		\]
		
\end{itemize}
\label{sec:iteration}