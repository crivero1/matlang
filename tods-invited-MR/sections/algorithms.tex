%!TEX root = ../main.tex

One of our main motivations to introduce for-loops is to be able to express classical linear algebra algorithms by a small set of core operators. We have seen that \langfor is
quite expressive as it can check for cliques, compute the transitive closure, and can even
leverage a successor relation on canonical vectors. The big question is how expressive \langfor
actually is. We will answer this question more formally in Section \ref{sec:circuits}, where we compare \langfor with 
arithmetic circuits of polynomial degree.
Before doing so, in this section we will illustrate that \langfor is capable of expressing many classical linear algebra algorithms. More precisely, we start by showing how  \langfor is able to
compute LU decompositions of matrices. These decompositions form the basis of many other algorithms, such as solving linear systems of equations. We further show that \langfor is expressive enough to compute matrix inversion and the determinant. We recall that matrix inversion and determinant need to be explicitly added as ad-hoc operators in \lang~\cite{matlang-journal} and that the LARA language is unable to invert matrices under usual complexity-theoretic assumptions~\cite{BarceloH0S20}.


\subsection{LU decomposition}

\begin{algorithm}[t]
    \caption{\EDIT{LU pseudocode}}
    \label{alg:lu}
    \begin{algorithmic}[1]
        \Function{LU}{$A$}
            \State{$A_0\coloneqq A$}
            \State{$T_0\coloneqq I$}
            \For{$i=1,2,\ldots,n$}
                \State{$A_i\coloneqq T_{i-1}\cdot A_{i-1}$}
                \State{$L^{-1}_i\coloneqq$ GETREDUCEMATRIX$(A_i,i)\cdot T_{i-1}$}\Comment{Note that $L_i^{-1}=T_i\cdot\cdots\cdot T_1$}
            \EndFor
            \State{}
            \State{$L\coloneqq (-1)\times L^{-1}_n + 2\times I$ }\Comment{This is equal to $T_1^{-1}\cdot\cdots\cdot T_n^{-1}$}
            \State{$U\coloneqq A_n$ }
            \State{}
            \Return{$L,U$}
        \EndFunction
        \State{ }
        \Function{GETREDUCEMATRIX}{$A,i$}\Comment{This computes $T_i$}
            \State{$c\coloneqq [0,\ldots,0]^T$}
            \For{$j=i+1,\ldots,n$}
                \State{ $c_{j,1} \coloneqq -\dfrac{A_{ij}}{A_{ii}}$}\Comment{Assuming that $A_{ii}\neq 0$}
                % \State{$T^{(i)}_{ij}\coloneqq A_{ij}+c_{j}A_{ii}$}
            \EndFor
            \State{}
            \State{$b_i\coloneqq [0,\ldots,1,\ldots,0]^T$}\Comment{$i$-th canonical vector}
            \State{$T\coloneqq I + c\cdot b^T$}
            \State{}
            \Return{$T$}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

A lower-upper (LU) decomposition factors a matrix $A$ as the product of a lower triangular matrix $L$ and upper triangular matrix $U$.  
This decomposition, and more generally LU decomposition with row pivoting (PLU),  underlies many linear algebra algorithms and 
we next show that \langfor can compute these decompositions.

LU decomposition \EDIT{(described in Algorithm~\ref{alg:lu})} can be seen as a matrix form of Gaussian elimination in which the columns of $A$
are reduced, one by one, to obtain the matrix $U$. The reduction of columns of $A$ is achieved
as follows. Consider the first column $[A_{11},\ldots,A_{n1}]^T$ of $A$ and  define 
$c_1 \coloneqq   [0, \alpha_{21},\ldots, \alpha_{n1}]^T$ 
with $\alpha_{j1} \coloneqq   -\frac{A_{j1}}{A_{11}}$. Let $T_1\coloneqq  I+ c_1\cdot b_1^T$ and consider
$T_1\cdot A$. That is, the $j$th row of $T_1\cdot A$ is obtained by multiplying the first row of $A$ by $\alpha_{j1}$ and adding it to the $j$th row of $A$. As a result, the first column of $T_1\cdot A$ is equal to $[A_{11},0,\ldots,0]^T$, i.e., 
all of its entries below the diagonal are zero.  One then iteratively performs a similar computation, using the matrix $T_i\coloneqq  I+c_i\cdot b_i^T$, where $c_i$ now depends on the $i$th column in $T_{i-1}\cdots T_1\cdot A$. As a consequence, $T_i\cdot T_{i-1}\cdots T_1\cdot A$ is upper triangular
in its first $i$ columns. At the end of this process, $T_n\cdots T_1\cdot A=U$ where $U$ is the desired upper triangular matrix.
Furthermore, it is easily verified that each $T_i$ is invertible and by defining $L\coloneqq  T_1^{-1}\cdot\cdots\cdot T_n^{-1}$ one obtains a lower triangular matrix satisfying $A=L\cdot U$. The above procedure is only successful when the denominators used in the definition of the vectors $c_i$ are non-zero. When this is the case we call a matrix $A$ \textit{LU-factorizable} \cite{num}. 

In case when such a denominator is zero in one of the reduction steps, one can remedy this situation by \textit{row pivoting}. That is, when the $i$th entry of the
$i$th row in $T_{i-1}\cdots T_1\cdot A$ is zero, one replaces the $i$th row by  $j$th row in this matrix, with $j>i$, provided that the $i$th entry of the $j$th row is non-zero. If no such row exists, this implies that all elements below the diagonal are zero already in column $i$ and one can proceed with the next column. One can formulate this in matrix terms by stating that there exists a permutation matrix $P$, which pivots rows, such that $P\cdot A=L\cdot U$. Any matrix $A$ is LU-factorizable \textit{with pivoting} \cite{num}.

To implement the described procedure in \langfor, we first assume that the input matrices are LU-factorizable, and  deal with general matrices later on. The following result tells us how to do this in $\langforf{f_/}$; that is, in \langfor extended with the \textit{division} function  $f_/:\RR^2\to\RR$, which is defined by $f_/(x,y)\coloneqq x/y$, for $y\neq 0$, and $f_/(x,y)\coloneqq   0$, otherwise. %, given by $f_/(x,y) \coloneqq   x/y$, where we define division by zero to be equal zero.

\begin{proposition}\label{prop:gauss}
There exist $\langforf{f_/}$ expressions $e_L(V)$ and $e_U(V)$ such that
$\sem{e_L}{\I}=L$ and $\sem{e_U}{\I}=U$ form an LU-decomposition of $A$,
\EDIT{where $\I$ assigns $V$ to a matrix $A$ that is} LU-factorizable.
\end{proposition}
\begin{proof}
Assume $A$ to be an LU-factorizable matrix. To compute the LU decomposition of a matrix, as described above, we need to compute the vector $c_i$ for each column $i$. We do this in two steps. First, we extract from our input matrix its $i$th column and set all its upper diagonal entries to zero
by means of 
 the 
 expression:
$$\ccol{V}{y} \coloneqq   \Sigma v. \  \EDIT{\isless}(y,v)\cdot(v^T\cdot V \cdot y)\times v.$$
Indeed, when $V$ is assigned to a matrix $A$ and $y$ to $b_i$, we initially assign the zero (column) vector to $A_0$, i.e., 
$A_0\coloneqq  \mathbf{0}$, and then in consecutive iterations,  $A_j\coloneqq  A_{j-1}+ (b_j^T\cdot A\cdot b_i)\times b_i$ if $j>i$ (because $\EDIT{\isless}(b_i,b_j)=1$ if $j>i$) and $A_j\coloneqq A_{j-1}$ otherwise (because $\EDIT{\isless}(b_i,b_j)=0$ for $j\leq i$). The result of this evaluation is the desired column vector.
Using $\ccol{V}{y}$, we can now compute $T_i$ by the following expression (which corresponds to function GETREDUCEMATRIX in Algorithm~\ref{alg:lu}):
$$\red{V}{y} \coloneqq   e_{\mathsf{Id}}+ f_/\bigl(\ccol{V}{y},-(y^T\cdot V\cdot y)\times e_{\ones}(y)\bigr)\cdot y^T.$$
When $V$ is assigned to $A$ and $y$ to $b_i$, $f_/(\ccol{A}{b_i},-(b_i^T\cdot A\cdot b_i)\times \ones)$ is equal to the vector $c_i$ used in the definition of $T_i$. To perform the reduction steps for all columns, we consider
the expression:
$$
e_{U}(V) \coloneqq    \left( \initf{e_{\mathsf{Id}}}{y}{X}{\red{X\cdot V}{y}\cdot X} \right) \cdot V.
$$
That is, when $V$ is assigned $A$, $X$ will be initially $A_0\coloneqq  e_{\mathsf{Id}}$, and then
$A_i\coloneqq  \red{A_{i-1}\cdot A}{b_i}\cdot A=T_i\cdot T_{i-1}\cdots T_1\cdot A$, as desired.

In order to construct $e_L(V)$, we will use the fact that $e_U(A)=T_n\cdot\cdots\cdot T_1\cdot A$ with $L^{-1}=T_n\cdot\cdots\cdot T_1$, and that each $T_i$ can easily be inverted. For this, let
    $$
    e_{L^{-1}}(V) \coloneqq    \initf{e_{\mathsf{Id}}}{y}{X}{\red{X\cdot V}{y}\cdot X}.
    $$
    such that	$e_{\mathsf{U}}(V)=  e_{L^{-1}}(V) \cdot V$. It now suffices to observe that, since $T_n=I$,
    \begin{align*}
    L^{-1}&=(I-c_1\cdot b_1^T)\cdots (I-c_{n-1}\cdot  b_{n-1}^T)
    =I-c_1\cdot b_1^T-\cdots - c_{n-1}\cdot b_{n-1}^T,
    \end{align*}
and therefore,    
    \begin{align*}
    L&=(I+c_1\cdot b_1^T)\cdots (I+c_{n-1}\cdot b_{n-1}^T) =I+c_1\cdot b_1^T+\cdots + c_{n-1}\cdot b_{n-1}^T.
    \end{align*}
    As a consequence, to obtain $L$ from $L^{-1}$ we just need to multiply every entry below the diagonal by $-1$. Since both  $L$ and $L^{-1}$ are lower triangular, this can done 
    by computing $L=-1\times L^{-1} + 2\times I$. Translated into \langfor, this means that we can define
    $$
    e_{L}(V) \coloneqq    -1\times e_{L^{-1}}(V) + 2\times e_{\mathsf{Id}},
    $$
    which concludes the proof of the proposition.    
\end{proof}

When row pivoting is needed, we can also obtain a permutation matrix
$P$ such that $P\cdot A=L\cdot U$ holds by means of an expression in \langfor, provided
that we additionally allow the function $f_{>0}$, 
where $f_{>0}:\RR\to\RR$ is such that $f_{>0}(x)\coloneqq  1$ if $x>0$ and $f_{>0}(x)\coloneqq  0$ otherwise.
Intuitively, $f_{>0}$ introduces a limited form of \texttt{if-then-else} to \langfor, allowing to continue reducing columns only when the right pivot has been found.

\begin{proposition}\label{prop:palu}
There exists expressions $e_{L^{-1}P}(M)$ and $e_U(M)$ in $\langforf{f_/,f_{>0}}$ such that
$L^{-1}\cdot P=\sem{e_{L^{-1}P}}{\I}$ and $U=\sem{e_U}{\I}$ satisfy $L^{-1}\cdot P\cdot A=U$,
\EDIT{when $\I$ assigns $M$ to a matrix $A$ that is PALU factorizable.}
\end{proposition}

\input{sections/proofs/palu.tex}


\subsection{Determinant and inverse}\label{sec:queries:inverse}
Other key linear algebra operations include the computation of the determinant and
the inverse of a matrix (if the matrix is invertible). As a consequence of the expressibility
in $\langforf{f_/,f_{>0}}$ of LU-decompositions with pivoting, it can be shown that the determinant
and inverse can be expressed as well. 

However, the results
in the next section (connecting \langfor with arithmetic circuits) imply that the determinant
and inverse of a matrix can already be defined in $\langforf{f_/}$. So instead of using LU decomposition with pivoting for matrix inversion and computing the determinant, we provide an alternative solution that will be based on Csanky's algorithm~\cite{Csanky76}. The main objective of this subsection is to show the following result:

\begin{proposition}\label{prop:inverse}
    There are $\langforf{f_/}$ expressions $e_{\mathsf{det}}(V)$ and $e_{\mathsf{inv}}(V)$ such that
    $\sem{e_{\mathsf{det}}}{\I}=\mathsf{det}(A)$ and  
    $\sem{e_{\mathsf{inv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
    to $A$ and $A$ is invertible.
\end{proposition}
\input{sections/proofs/inverse.tex}
