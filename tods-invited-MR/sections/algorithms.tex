%!TEX root = ../main.tex

One of our main motivations to introduce for-loops is to be able to express classical linear algebra algorithms by a small set of core operators. We have seen that \langfor is
quite expressive as it can check for cliques, compute the transitive closure, and can even
leverage a successor relation on canonical vectors. The big question is how expressive \langfor
actually is. We will answer this question more formally in Section \ref{sec:circuits}, where we compare \langfor with 
arithmetic circuits of polynomial degree.
Before doing so, in this section we will illustrate that \langfor is capable of expressing many classical linear algebra algorithms. More precisely, we start by showing how  \langfor is able to
compute LU decompositions of matrices. These decompositions form the basis of many other algorithms, such as solving linear systems of equations. We further show that \langfor is expressive enough to compute matrix inversion and the determinant. We recall that matrix inversion and determinant need to be explicitly added as ad-hoc operators in \lang~\cite{matlang-journal} and that the LARA language is unable to invert matrices under usual complexity-theoretic assumptions~\cite{BarceloH0S20}.


\subsection{LU decomposition}

A lower-upper (LU) decomposition factors a matrix $A$ as the product of a lower triangular matrix $L$ and upper triangular matrix $U$.  
This decomposition, and more generally LU decomposition with row pivoting (PLU),  underlies many linear algebra algorithms and 
we next show that \langfor can compute these decompositions.

The LU decomposition \EDIT{(cfr.~Algorithm~\ref{alg:lu})} can be seen as a matrix form of Gaussian elimination in which the columns of $A$
are reduced, one by one, to obtain the matrix $U$. The reduction of columns of $A$ is achieved
as follows. Consider the first column $[A_{11},\ldots,A_{n1}]^T$ of $A$ and  define 
$c_1 \coloneqq   [0, \alpha_{21},\ldots, \alpha_{n1}]^T$ 
with $\alpha_{j1} \coloneqq   -\frac{A_{j1}}{A_{11}}$. Let $T_1\coloneqq  I+ c_1\cdot b_1^T$ and consider
$T_1\cdot A$. That is, the $j$th row of $T_1\cdot A$ is obtained by multiplying the first row of $A$ by $\alpha_{j1}$ and adding it to the $j$th row of $A$. As a result, the first column of $T_1\cdot A$ is equal to $[A_{11},0,\ldots,0]^T$, i.e., 
all of its entries below the diagonal are zero.  One then iteratively performs a similar computation, using the matrix $T_i\coloneqq  I+c_i\cdot b_i^T$, where $c_i$ now depends on the $i$th column in $T_{i-1}\cdots T_1\cdot A$. 
The vector $c_i$ has zero entries up to position $i$. In 
Algorithm~\ref{alg:lu}, the function \textsc{GetReduceMatrix}$(A_i,i)$ describes in detail how $c_i$  (lines 4--6) and $T_i$ (lines 7--8) are computed.  This function is only successful when the denominators used in the definition of the vectors $c_i$ are non-zero. When this is the case we call a matrix $A$ \textit{LU-factorizable} \cite{num}. We assume that $A$ satisfies this property and deal with the general case later on.


The function $\textsc{LU}(A)$ is in charge of computing the matrix $A_i\coloneqq T_i\cdot T_{i-1}\cdots T_1\cdot A$ (line 5) which is upper triangular in its first $i$ columns. At the end of this process, $U\coloneqq T_n\cdots T_1\cdot A$ (line 11) and $U$ is the desired upper triangular matrix.
Also, $\textsc{LU}(A)$ computes the matrices $L_i^{-1}\coloneqq T_i\cdots T_1$ (lines 7) which is lower triangule in its first columns. We observe that $T_n=I$ and 
it is easily verified that each $T_i$ is invertible. We have, more explicitly,
    \begin{align*}
    L^{-1}_n&=(I+c_1\cdot b_1^T)\cdots (I+c_{n-1}\cdot  b_{n-1}^T)
    =I+c_1\cdot b_1^T+\cdots +c_{n-1}\cdot b_{n-1}^T,
    \end{align*}
and therefore,    
    \begin{align*}
    L_n&=(I-c_1\cdot b_1^T)\cdots (I-c_{n-1}\cdot b_{n-1}^T) =I-c_1\cdot b_1^T-\cdots - c_{n-1}\cdot b_{n-1}^T.
    \end{align*}
So, once we have $L_n^{-1}$ we can get	also obtain $L_n$ by computing
$L_n\coloneqq (-1)\times L_n^{-1} + 2\times I$ (line 9). The matrix $L\coloneqq L_n$ is the desired
lower triangular matrix satisfying $A=L\cdot U$.

 % \EDIT{The above process is precisely what is computed by the functions \textsc{LU}$(A)$ in Algorithm~\ref{alg:lu} and calls to the function \textsc{GetReduceMatrix}$(A_i,i)$ computing $T_i$.}
  
We next show how to implement the described functions in \langfor. The following result tells us how to do this in $\langforf{f_/}$; that is, in \langfor extended with the \textit{division} function  $f_/:\RR^2\to\RR$, which is defined by $f_/(x,y)\coloneqq x/y$, for $y\neq 0$, and $f_/(x,y)\coloneqq   0$, otherwise. %, given by $f_/(x,y) \coloneqq   x/y$, where we define division by zero to be equal zero.
	\begin{algorithm}[t]
    \caption{\EDIT{Pseudocode LU decomposition (for LU-factorizable input matrices)}}
    \label{alg:lu}
\begin{minipage}{\textwidth}
\begin{minipage}{0.49\textwidth}
    \begin{algorithmic}[1]
        \Function{\textsc{LU}}{$A$}\Comment{Input: $n\times n$ matrix $A$}
            \State{$A_0\coloneqq A$}
            \State{$T_0\coloneqq I$}
            \State{$L_0^{-1}\coloneqq I$}
            \For{$i=1,2,\ldots,n$}
                \State{\hspace{-2ex}$T_i\coloneqq$\textsc{GetReduceMatrix}$(A_{i-1},i)$}
                \State{\hspace{-2ex}$A_i\coloneqq T_{i}\cdot A_{i-1}$}
                \State{\hspace{-2ex}$L^{-1}_i\coloneqq T_i\cdot L_{i-1}^{-1}$}
				\floris{something seems off shouldn't this be $T_i\cdot L_{i-1}^{-1}$ but then $L_0^{-1}$ needs to be defined as well?}
                \thomas{You are right. I did that now.}
			% \State{\Comment{Note that $L_i^{-1}=T_i\cdot\cdots\cdot T_1$}}
            \EndFor
            \State{$L\coloneqq (-1)\times L^{-1}_n + 2\times I$ }
			% \State{\Comment{Note that $L=T_1^{-1}\cdot\cdots\cdot T_n^{-1}$}}
            \State{$U\coloneqq A_n$}
			 \State{}
            \Return{$L,U$}
        \EndFunction 
		 \end{algorithmic}
\end{minipage}
\begin{minipage}{0.49\textwidth}
 \begin{algorithmic}[1]
        \Function{\textsc{GetReduceMatrix}}{$A,i$}
	\State{\Comment{Input: $n\times n$ matrix $A$, $i=1,\ldots,n$}}	%
	\State{}
	\State{$c_i\coloneqq [\alpha_{1i}:=0,\alpha_{2i}:=0,\ldots,\alpha_{ni}:=0]^T$}
            \For{$j=i+1,\ldots,n$}
                \State{ $\alpha_{j,i} \coloneqq -\dfrac{A_{ji}}{A_{ii}}$}
				\Comment{(Assuming  $A_{ii}\neq 0$)}
                % \State{$T^{(i)}_{ij}\coloneqq A_{ij}+c_{j}A_{ii}$}
            \EndFor
            \State{$b_i\coloneqq [0,\ldots,1,\ldots,0]^T$}
			% \State{\Comment{$b_i$ is the $i$-th canonical vector}}
	        \State{$T_i\coloneqq I + c_i\cdot b_i^T$}
			\State{}
            \Return{$T_i$}
        \EndFunction
    \end{algorithmic}
	\vspace*{1.5ex}
\end{minipage}
\end{minipage}
\end{algorithm}

\begin{proposition}\label{prop:gauss}
There exist $\langforf{f_/}$ expressions $e_L(V)$ and $e_U(V)$ such that
$\sem{e_L}{\I}=L$ and $\sem{e_U}{\I}=U$ form an LU-decomposition of $A$,
\EDIT{where $\I$ assigns $V$ to a matrix $A$ that is} LU-factorizable.
\end{proposition}
\begin{proof}
Assume $A$ to be an LU-factorizable matrix. To compute the LU decomposition of a matrix, as described above, we need to compute the vector $c_i$ for each column $i$. We do this in two steps. First, we extract from our input matrix its $i$th column and set all its upper diagonal entries to zero
by means of 
 the 
 expression:
$$\ccol{V}{y} \coloneqq   \Sigma v. \  \EDIT{\isless}(y,v)\cdot(v^T\cdot V \cdot y)\times v.$$
Indeed, when $V$ is assigned to a matrix $A$ and $y$ to $b_i$, we initially assign the zero (column) vector to $A_0$, i.e., 
$A_0\coloneqq  \mathbf{0}$, and then in consecutive iterations,  $A_j\coloneqq  A_{j-1}+ (b_j^T\cdot A\cdot b_i)\times b_i$ if $j>i$ (because $\EDIT{\isless}(b_i,b_j)=1$ if $j>i$) and $A_j\coloneqq A_{j-1}$ otherwise (because $\EDIT{\isless}(b_i,b_j)=0$ for $j\leq i$). The result of this evaluation is the desired column vector.
Using $\ccol{V}{y}$, we can now compute $T_i$ by the following expression (which corresponds to function \textsc{GetReduceMatrix} in Algorithm~\ref{alg:lu}):
$$\red{V}{y} \coloneqq   e_{\mathsf{Id}}+ f_/\bigl(\ccol{V}{y},-(y^T\cdot V\cdot y)\times e_{\ones}(y)\bigr)\cdot y^T.$$
When $V$ is assigned to $A$ and $y$ to $b_i$, $f_/(\ccol{A}{b_i},-(b_i^T\cdot A\cdot b_i)\times \ones)$ is equal to the vector $c_i$ used in the definition of $T_i$. To perform the reduction steps for all columns, we consider
the expression:
$$
e_{U}(V) \coloneqq    \left( \initf{e_{\mathsf{Id}}}{y}{X}{\red{X\cdot V}{y}\cdot X} \right) \cdot V.
$$
That is, when $V$ is assigned $A$, $X$ will be initially $A_0\coloneqq  e_{\mathsf{Id}}$, and then
$A_i\coloneqq  \red{A_{i-1}\cdot A}{b_i}\cdot A=T_i\cdot T_{i-1}\cdots T_1\cdot A$, as desired.
\floris{I am confused how this relates to $A_i$ in the pseudo-code?}
\thomas{Yes there was an index problem, I think it is fixed now.

Maybe using the same routine to output $U$ and $L$ is a bit confusing when compared to the correctedness proof,
were we calculate them separately.
Should we break the function $\textsc{LU}(A)$ and have two separate pseudocodes for computing $U$ and $L$?
}

In order to construct $e_L(V)$, we will use the fact that $e_U(A)=T_n\cdot\cdots\cdot T_1\cdot A$ with $L^{-1}=T_n\cdot\cdots\cdot T_1$, and that each $T_i$ can easily be inverted. For this, let
    $$
    e_{L^{-1}}(V) \coloneqq    \initf{e_{\mathsf{Id}}}{y}{X}{\red{X\cdot V}{y}\cdot X}.
    $$
    such that	$e_{\mathsf{U}}(V)=  e_{L^{-1}}(V) \cdot V$.%  It now suffices to observe that, since $T_n=I$,
%     \begin{align*}
%     L^{-1}&=(I-c_1\cdot b_1^T)\cdots (I-c_{n-1}\cdot  b_{n-1}^T)
%     =I-c_1\cdot b_1^T-\cdots - c_{n-1}\cdot b_{n-1}^T,
%     \end{align*}
% and therefore,
%     \begin{align*}
%     L&=(I+c_1\cdot b_1^T)\cdots (I+c_{n-1}\cdot b_{n-1}^T) =I+c_1\cdot b_1^T+\cdots + c_{n-1}\cdot b_{n-1}^T.
%     \end{align*}
%     As a consequence, to obtain $L$ from $L^{-1}$ we just need to multiply every entry below the diagonal by $-1$. Since both  $L$ and $L^{-1}$ are lower triangular, this can done
%     by computing $L=-1\times L^{-1} + 2\times I$.
%Translated into \langfor, this means that we can define
It then remains to consider
    $$
    e_{L}(V) \coloneqq    -1\times e_{L^{-1}}(V) + 2\times e_{\mathsf{Id}},
    $$
which, as we have described above, suffices to inver $L^{-1}$. This concludes the proof of the proposition.    
\end{proof}


In case when  a denominator is zero in one of the reduction steps in the function
\textsc{GetReduceMatrix}, one can remedy this situation by \textit{row pivoting}.  More precisely, row pivoting is needed when the $i$th entry of the
$i$th row in $T_{i-1}\cdots T_1\cdot A$ is zero. In this cases, one replaces the $i$th row by  $j$th row in this matrix, with $j>i$, provided that the $i$th entry of the $j$th row is non-zero. If no such row exists, this implies that all elements below the diagonal are zero already in column $i$ and one can proceed with the next column. One can formulate this in matrix terms by stating that there exists a permutation matrix $P$, which pivots rows, such that $P\cdot A=L\cdot U$. Any matrix $A$ is LU-factorizable \textit{with pivoting} \cite{num}.

When row pivoting is needed, we can also obtain a permutation matrix
$P$ such that $P\cdot A=L\cdot U$ holds by means of an expression in \langfor, provided
that we additionally allow the function $f_{>0}$, 
where $f_{>0}:\RR\to\RR$ is such that $f_{>0}(x)\coloneqq  1$ if $x>0$ and $f_{>0}(x)\coloneqq  0$ otherwise.
Intuitively, $f_{>0}$ introduces a limited form of \texttt{if-then-else} to \langfor, allowing to continue reducing columns only when the right pivot has been found.

\begin{proposition}\label{prop:palu}
There exists expressions $e_{L^{-1}P}(M)$ and $e_U(M)$ in $\langforf{f_/,f_{>0}}$ such that
$L^{-1}\cdot P=\sem{e_{L^{-1}P}}{\I}$ and $U=\sem{e_U}{\I}$ satisfy $L^{-1}\cdot P\cdot A=U$,
\EDIT{when $\I$ assigns $M$ to a matrix $A$ that is PALU factorizable.}
\end{proposition}

\input{sections/proofs/palu.tex}


\subsection{Determinant and inverse}\label{sec:queries:inverse}
Other key linear algebra operations include the computation of the determinant and
the inverse of a matrix (if the matrix is invertible). As a consequence of the expressibility
in $\langforf{f_/,f_{>0}}$ of LU-decompositions with pivoting, it can be shown that the determinant
and inverse can be expressed as well. 

However, the results
in the next section (connecting \langfor with arithmetic circuits) imply that the determinant
and inverse of a matrix can already be defined in $\langforf{f_/}$. So instead of using LU decomposition with pivoting for matrix inversion and computing the determinant, we provide an alternative solution that will be based on Csanky's algorithm~\cite{Csanky76}. The main objective of this subsection is to show the following result:

\begin{proposition}\label{prop:inverse}
    There are $\langforf{f_/}$ expressions $e_{\mathsf{det}}(V)$ and $e_{\mathsf{inv}}(V)$ such that
    $\sem{e_{\mathsf{det}}}{\I}=\mathsf{det}(A)$ and  
    $\sem{e_{\mathsf{inv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
    to $A$ and $A$ is invertible.
\end{proposition}
\input{sections/proofs/inverse.tex}
