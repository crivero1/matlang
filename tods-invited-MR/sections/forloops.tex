%!TEX root = ../main.tex
% !TeX spellcheck = en_US

To extend \lang\ with recursion, we take inspiration from classical linear algebra algorithms, such as those described in \cite{num}. Many of these algorithms are based on \textit{for-loops} in which the termination condition for each loop is determined by the matrix dimensions. We have seen how the transitive closure of a matrix can be computed using for-loops in the Introduction. Here we add this ability to \lang, and show that the resulting language, called $\langfor,$ can compute properties outside of the scope of \lang. We show more advanced examples, such as Gaussian elimination and matrix inversion, later in the paper. 

\EDIT{For the variables in $\Mvar$ we use uppercase letters
$A,B,C,\allowbreak X,\allowbreak V_1,V_2,\ldots$ for variables that can be instantiated by any matrix, vector or scalar, and we use 
lowercase letters $x,y,u,w,v_1,v_2,\ldots$
for \emph{iterator variables} that can only be instantiated with canonical vectors, which will be introduced shortly.}

\subsection{Syntax and semantics of \langfor}\label{ss:fML} The syntax of \langfor is defined just as for \lang but with an extra rule in the grammar:
\medskip

\begin{tabular}{lcll}
 $\ffor{v}{X}{e}$ & (canonical for-loop, with $v, X \in \Mvar$). 
\end{tabular}

\medskip
\noindent Intuitively, $X$ is a matrix variable which is iteratively updated according to the expression $e$. We simulate iterations of the form ``\texttt{for} $i\in [1..n]$'' by letting $v$ loop over the \textit{canonical vectors} $b_1^n,\ldots,b_n^n$ of dimension $n$. Here,
$b_1^n = [1\ 0 \cdots 0]^T$, $b_2^n = [0\ 1\ 0 \cdots 0]^T$, etc. When $n$ is clear from the context we simply write $b_1,b_2,\ldots$. We remark that the expression $e$ in the rule above may depend on $v$ and $X$. 
\EDIT{We call $v$ an \emph{iterator variable}.}

We next make the semantics of this new construct precise and start by
declaring the \textit{type of for-loop expressions}.
Given a schema $\Sch=(\Mnam,\size)$, the type of a \langfor expression $e$, denoted $\ttype(e)$, is defined inductively as in \lang but with following extra rule:
\begin{itemize}
\item $\ttype(\ffor{v}{X}{e}) \coloneqq  (\alpha,\beta)$ if 
$\ttype(e)=\ttype(X) =(\alpha,\beta)$ and $\ttype(v) = (\gamma,1)$.
\end{itemize}
We note that $\Sch$ now necessarily includes $v$ and $X$ as variables and assigns size symbols to them.
We also remark that in the definition of the type of $\ffor{v}{X}{e}$, we require that $\ttype(X) = \ttype(e)$ as this expression updates the content of the variable $X$ in each iteration using the result of $e$. We further restrict the type of 
$v$ to be a \textit{column} vector, i.e., $\ttype(v)=(\gamma,1)$, since $v$ will be instantiated with canonical vectors. As before,
a \langfor\ expression $e$ is \textit{well-typed} over a schema $\Sch=(\Mnam,\size)$ if its type is defined. 

For well-typed \langfor\ expressions we next define their semantics. This is done in an inductive way, just as for \lang. To define the semantics of $\ffor{v}{X}{e}$ over an instance $\I$, we need the following notation. Let $\I$ be an instance and $V_1,\ldots,V_\ell\in \Mnam$. Then $\I[V_1 \coloneqq  A_1, \ldots, V_\ell\coloneqq A_\ell]$ denotes an instance that coincides with $\I$, except that the value of the matrix variable $V_i$ is given by the matrix $A_i$, for $i\in\lbrace 1, \ldots, \ell\rbrace$.

We now assume that
$\ttype(v)= (\gamma,1)$, $\ttype(e) = (\alpha,\beta)$ and $n \coloneqq  \dom(\gamma)$. Then, $\sem{\ffor{v}{X}{e}}{\I}$ is defined iteratively, as follows:
\begin{itemize}
\item Let $A_0 \coloneqq  \mathbf{0}$ be the zero matrix of size $\dom(\alpha)\times \dom(\beta)$.
\item For $i=1,\ldots n$, compute $A_i\coloneqq  \sem{e}{\I[v \coloneqq  b^{n}_i, X\coloneqq  A_{i-1}]}$.
\item Finally, set $\sem{\ffor{v}{X}{e}}{\I}\coloneqq  A_{n}$.
\end{itemize}

For better understanding how \langfor  works, we next provide some  examples.
We start by showing that the one-vector and $\diag$ operators are redundant
in \langfor.

\begin{example}\label{ex:onevec}
We first show how the one-vector operator $\ones(e)$ can be expressed using for-loops.
It suffices to consider the expression
$$e_{\ones}(e)\coloneqq \ffor{v}{X}{X+v},$$
with $\ttype(v)=(\alpha,1)=\ttype(X)$ if $\ttype(e)=(\alpha,\beta)$. This expression is well-typed
and is of type $(\alpha,1)$. When evaluated over some instance $\I$ with $n=\dom(\alpha)$, $\sem{e_{\ones}(e)}{\I}$ is defined as follows.
Initially, $A_0\coloneqq \mathbf{0}$. Then $A_i\coloneqq A_{i-1}+b_i^n$, i.e., the $i$th canonical vector is added to $A_{i-1}$.
Finally, $\sem{e_{\ones}(e)}{\I}\coloneqq A_n$ and this now clearly coincides with $\sem{\ones(e)}{\I}$. Notice that the expression $e_1(e)$ does not depend on $e$ in any way, except for restricting $\ttype(X)$. \qed
\end{example}

\begin{example}\label{ex:diag}
We next show that the $\diag(e)$ operator is also redundant in \langfor.
Indeed, it suffices to consider the expression
$$e_{\mathsf{diag}}\coloneqq 
\ffor{v}{X}{X + (v^T\cdot e) \times v\cdot v^T},$$ where $e$ is a \langfor\  expression of type $(\alpha,1)$. For this expression to be well-typed, $v$ has to be a vector variable of type $(\alpha,1)$ and $X$ a matrix variable of type $(\alpha,\alpha)$. Then, $\sem{e_{\mathsf{diag}}}{\I}$ is defined as follows.
Initially, $A_0$ is the zero matrix of dimension $n\times n$, where $n=\dom(\alpha)$. Then, in each iteration
$i\in[1..n]$, $A_{i}\coloneqq A_{i-1}+  ((b_i^n)^T\cdot\sem{e}{\I})\times (b_i^n\cdot (b_i^n)^T)$. In other words, $A_i$ is obtained by adding the matrix with value $(\sem{e}{\I})_i$ on position $(i,i)$ to $A_{i-1}$. Hence, $\sem{e_{\mathsf{diag}}}{\I}\coloneqq A_n=\sem{\diag(e)}{\I}$. When we want to make it explicit that $e_\mathsf{diag}$ is defined based on $e$, we will often write $e_\mathsf{diag}(e)$.\qed
 \end{example}

These examples illustrate that we can limit \langfor to consist of the following ``core'' operators: transposition, matrix multiplication and addition, scalar multiplication, pointwise function application, and for-loops. More specific, \langfor is defined by the following simplified syntax:
$$
e :\coloneqq  V \ \mid \ e^T \!\ \mid \ e_1 \cdot e_2 \ \mid \ e_1 + e_2 \ \mid \ e_1\times e_2  \ \mid \  f(e_1,\ldots ,e_k) \ \mid \ \ffor{v}{X}{e}
$$
Similarly as for \lang, we write $\langforf{\Fun}$ for some set $\Fun$ of functions when these are required for the task at hand. Throughout the paper we will write $a$ instead of $[a]$ to denote a scalar $a\in\RR$ in our expressions. For example,  $(1-v^T\cdot v)$ stands for  $([1]-v^T\cdot v)$. 

As a final example, we show that we can check whether a graph contains a $4$-clique using \langfor.
\begin{example}\label{ex:fourcliques}
To test for $4$-cliques it suffices to consider the following expression with for-loops nested four times:
\begin{center}
\parbox{0cm}{
\begin{tabbing}
\texttt{for\,}\=$u,\,X_1.\ \ X_1 \ + $\\
\> \texttt{for\,}\=$v,\,X_2.\ \ X_2 \ +$ \\
\>\>\texttt{for\,}\=$w,\,X_3.\ \ X_3 \ +$ \\
\>\>\>\texttt{for\,}\=$x,\,X_4.\ \ X_4 \ +$ \\
\>\>\>\>$(u^T\!\cdot\! V\!\cdot\! v) \!\cdot\! (u^T\!\cdot\! V\!\cdot\! w)\!\cdot\! (u^T\!\cdot\! V\!\cdot\! x) \!\cdot\! (v^T\!\cdot\! V\!\cdot\! w) \!\cdot\! (v^T\!\cdot\! V\!\cdot\! x)\!\cdot\! (w^T\!\cdot\! V\!\cdot\! x) \!\cdot\! g(u,v,w,x)$
\end{tabbing}
}
\end{center}
with $g(u,v,w,x)\coloneqq f(u,v)\cdot f(u,w)\cdot f(u,x)\cdot f(v,w)\cdot f(v,x)\cdot f(w,x)$ and
$f(u,v)=1-u^T\cdot v$. Note that $f(b_i^n,b_j^n)=1$ if $i\neq j$ and $f(b_i^n,b_j^n)=0$ otherwise.
Hence, $g(b_i^n,b_j^n,b_k^n,b_\ell^n)=1$ if and only if all $i,j,k,l$ are pairwise different.
When evaluating the expression on an instance $\I$ such that $V$ is assigned to the adjacency 
matrix of a graph, the expression above evaluates to a non-zero value if and only if the graph
contains a four-clique.\qed
\end{example}

Given that \lang can not check for 4-cliques \cite{matlang-journal}, we easily obtain the following.

\begin{proposition}
\label{cor-ml-fml}
$\langf{\Fun}$ is properly subsumed by $\langforf{\Fun}$, for any collection of functions $\Fun$.
\end{proposition} 

\paragraph{Loop Initialization.} As the reader may have observed, in the semantics of for-loops we 
always initialize $A_0$ to the zero matrix~$\mathbf{0}$ (of appropriate dimensions). It is often convenient
to start the iteration given some concrete matrix  originating from the result of evaluating a \langfor\ expression $e_0$. To make this explicit, we write $\initf{e_0}{v}{X}{e}$ and its semantics is defined as above
with the difference that $A_0\coloneqq \sem{e_0}{\I}$. We observe, however, that $\initf{e_0}{v}{X}{e}$ can already
be expressed in \langfor. In other words, we do not loose generality by assuming an initialization of $A_0$ by $\mathbf{0}$.
The key insight is that in \langfor\ we can check during evaluation whether or not
the current canonical vector $b_i^n$ is equal to $b_1^n$ using the
\EDIT{order predicate\footnote{We explain in detail how $\mmin$, as well as many other operators which access the order of canonical vectors, can be defined in \langfor in the following subsection. We also remark that natural numbers, such as 1, or more precisely, the matrix $[1]$, can be defined based on $\mmin$, e.g. by the expression $\ffor{v}{X} X + \mmin{v}\times v^T\cdot v$.}}
$\mmin{v}$, which, when evaluated on an instance, returns $1$ if its input vector is $b_1^n$, and returns $0$ for any other canonical vector. Given $\mmin$, consider now the
\langfor\ expression
 $$\ffor{v}{X}{\mmin{v}\times e(v,X/e_0) + (1-\mmin{v})\times e(v,X)},$$
 where we explicitly list $v$ and $X$ as matrix variables on which $e$ potentially depends on, and where
 $e(v,X/e_0)$ denotes the expression obtained by replacing every occurrence of $X$ in $e$ with $e_0$.
When evaluating this expression on an instance $\I$, $A_0$ is initial set to the zero matrix, in the first iteration (when  $v=b_1^n$ and thus $\mmin{v}=1$)
we have $A_1=\sem{e}{\I[v\coloneqq b_1^n,X\coloneqq \sem{e_0}{\I}]}$, and for consecutive iterations (when only the part related to $1-\mmin{v}$ applies) $A_i$ is updated as before. Clearly, the result of this evaluation is equal to
$\sem{\initf{e_0}{v}{X}{e}}{\I}$.


As an illustration of why arbitrary initial matrices migh be useful, we consider the Floyd-Warshall algorithm given in the Introduction. 

\begin{example}\label{ex:floyd}
Consider the following expression:
\begin{center}
\parbox{0cm}{
\begin{tabbing}
$e_{FW} \coloneqq  $ \texttt{for\,}\=$v_k,\, X_1\!=\!A.\ \ X_1 \ + $\\
\> \texttt{for\,}\=$v_i, \, X_2.\ \ X_2 \ +$ \\
\>\>\texttt{for\,}\=$v_j,\, X_3.\ \ X_3 \ +$ \\
\>\>\>$(v_i^T\cdot X_1\cdot v_k \cdot v_k^T\cdot X_1\cdot v_j)\times v_i\cdot v_j^T$
\end{tabbing}
}
\end{center}
The expression $e_{FW}$ simulates the Floyd-Warshall algorithm by updating the matrix $A$, which is stored in the variable $X_1$. The inner sub-expression here constructs an $n\times n$ matrix that contains one in the position $(i,j)$ if and only if one can reach the vertex $j$ from $i$ by going through $k$, and zero elsewhere. If an instance $\I$ assigns to $A$ the adjacency matrix of a graph, then $\sem{e_{FW}}{\I}$ will be equal to the matrix produced by the algorithm given in the Introduction.
\qed
\end{example}

\subsection{Defining order}
\label{sec:formatlang:design}

By introducing for-loops we not only extend \lang\ with bounded recursion, we also introduce order information. Indeed,
the semantics of the \texttt{for} operator assumes that the canonical vectors $b_1,b_2,\ldots$
are accessed in this order. This implies, among other things, that \langfor\ expressions are not permutation-invariant.
We can, for example, return the bottom right-most entry in a matrix. Indeed, we first consider the expression 
$$
e_{\mathsf{max}} \coloneqq  \ffor{v}{X}{v}
$$
which, for it to be well-typed, requires both $v$ and $X$ to be of type $(\alpha,1)$. Then, $\sem{e_{\mathsf{max}}}{\I}=b_n^n$, for $n=\dom(\alpha)$, simply because initially, $X=\mathbf{0}$, but $X$ will be overwritten by $b_1^n,b_2^n,\ldots,b_n^n$, in this order. Hence, at the end of the evaluation $b_n^n$ is returned.
To extract the bottom right-most entry from a matrix, we now simply use $e_{\mathsf{max}}^T\cdot V\cdot e_{\mathsf{max}}$.



Although the order is implicit in \langfor, we can in fact \textit{explicitly} use this order in \langfor expressions. More precisely, the order on canonical vectors is made accessible by
using the matrix $S_\leq$ shown in Fig.~\ref{fig:ordermat}.
We observe that $S_{\leq}$ has the property that $b_i^T\cdot S_{\leq} \cdot b_j=1$, for two canonical vectors $b_i$ and $b_j$ of the same dimension, if and only if $i\leq j$. Otherwise, $b_i^T\cdot S_{\leq} \cdot b_j=0$. 
Interestingly, we can build the matrix $S_{\leq}$ with the following \langfor expression:
$$
e_{\leq}=\ffor{v}{X}{X + \left((X\cdot e_{\mathsf{max}}) + v \right)\cdot v^T + v\cdot e^T_{\mathsf{max}}},
$$
where $e_{\mathsf{max}}$ is as defined above. The intuition behind this expression is that by using the last canonical vector $b_n$, as returned by $e_{\mathsf{max}}$, we have access to the last column of $X$ (via the product $X\cdot e_{\mathsf{max}}$). We use this column such that after the $i$-th iteration, this column contains the $i$-th column of $S_{\leq}$. This is done by incrementing $X$ with $v\cdot e_{\mathsf{max}}^T$.
To construct $S_{\leq}$, in the $i$-th iteration we further increment $X$ with 
(i)~the current last column in $X$ (via $X\cdot e_{\mathsf{max}}\cdot v^T$) which holds
the $(i-1)$-th column of $S_{\leq}$; and (ii)~the current canonical vector (via $v\cdot v^T$). Hence, after iteration $i$, $X$ contains the first $i$ columns of $S_{\leq}$ and holds the $i$th column of $S_{\leq}$ in its last column. It is now readily verified that $X=S_{\leq}$ after the $n$th iteration.

Using the matrix $S_\leq$, we can now define several other order-related operators over canonical vectors which will be useful in the remainder of the paper. For instance, using the expression 
$$
\EDIT{\islessorequal}(u,v) \coloneqq  u^T\cdot e_{\leq} \cdot v,
$$
we obtain an order relation that allows us to discern whether one canonical vector comes before 
the other in the order given by $S_{\leq}$. Namely, we have that \EDIT{$$\sem{\EDIT{\islessorequal}(u,v)}{\I[u\coloneqq b_i^n,v\coloneqq b_j^n]} = \begin{cases}
1, & \text{ if } i\leq j\\
0, & \text{ otherwise}.
\end{cases}$$
}

If we want a strict order, we can just use the expression
$e_< \coloneqq  e_{\leq} - e_{\mathsf{Id}}$, where $e_{\mathsf{Id}} \coloneqq  \ffor{v}{X}{v\cdot v^T}$, is an expression in \langfor which returns the identity matrix (of appropriate dimension). Given this, we define
$$\EDIT{\isless}(u,v) \coloneqq  u^T\cdot e_{<} \cdot v.$$

We can also determine whether we are dealing with the last canonical vector by using:
$$
\mathsf{max}(u)\coloneqq u^T\cdot e_{\mathsf{max}}.
$$
Indeed, $\mathsf{max}(u)$ be equal $1$ if and only if $u$ is $b_n^n$, and is $0$ otherwise.

\begin{figure}
	\[
	S_{\leq} = \begin{bmatrix}
		1 & 1 & \cdots &  1 \\
		0 & 1 & \cdots & 1\\
		\vdots & \vdots & \ddots & 1 \\
		0 & 0 & \cdots & 1 
	\end{bmatrix} \quad\mathsf{Prev} = \begin{bmatrix}
		0 & 1 & \cdots &  0 \\
		0 & \ddots & \ddots & \vdots \\
		\vdots & \ddots & \ddots& 1 \\
		0 & \cdots & \cdots & 0
	\end{bmatrix},
	\quad \mathsf{Next} = \begin{bmatrix}
		0 & 0 & \cdots &  0 \\
		1 & \ddots & \ddots & 0\\
		\vdots & \ddots & \ddots &0 \\
		0 & \cdots & 1 & 0
	\end{bmatrix},
	\] 
	\caption{Matrices encoding order information which are computable in \langfor.}\label{fig:ordermat}
\end{figure}

We can also define the \textit{previous} relation between canonical vectors. For this, we need
the matrix $\mathsf{Prev}$ shown in Fig.~\ref{fig:ordermat}.
The matrix 
$\mathsf{Prev}$ can be defined using the following \langfor expression:
$$e_{\mathsf{Prev}}\coloneqq  \ffor{v}{X}{X + \bigl((1 - \mathsf{max}(v))\times v\cdot e_{\mathsf{max}}^T - (X\cdot e_{\mathsf{max}})\cdot e_{\mathsf{max}}^T + (X\cdot e_{\mathsf{max}})\cdot v^T\bigr)}.$$
Here, $X$ is initialized as $\mathbf{0}$ and thus in the first iteration we put
 $b_1$ in the last column of $X$ (note that $X\cdot e_{\mathsf{max}}$ is also zero in the first iteration). Next, in iteration two, we add a matrix that has the stored vector $X\cdot e_{\mathsf{max}}$ (the previous canonical vector) in the column indicated by $v$ (the current canonical vector) and $v-X\cdot e_{\mathsf{max}}$ in the last column, to replace the vector stored. As a consequence, $b_2$ is now stored in the last column. In the last iteration, we have $b_{n-1}$ already in the last column, so no further update of $X$ is required.
Using this matrix, we have that for a canonical vector $b_i^n$:
\[
\mathsf{Prev}\cdot b_i^n=\begin{cases}
               b_{i-1}^n & \text{ if } i > 1 \\
              \mathbf{0} & \text{ if } i = 1,
            \end{cases}
\]
where $\mathbf{0}$ is a vector of zeros of the same type as $b_i$. Notice also that $e_{\ones}(u)^T\cdot \mathsf{Prev} \cdot u$ is equal to zero, for a canonical vector $u$, if and only if $u$ is the first canonical vector $b_1^n$, and is equal to one for any other canonical vector. Indeed, $e_{\ones}(u)^T\cdot \mathsf{Prev}$ equals to $[0\ 1\ \cdots\ 1]$, whenever $u$ is replaced by some canonical vector. Using this observation, the expression $\mathsf{min}(u)$, which evaluates to one on the first canonical vector, and to zero on any other canonical vector, can be  defined as $$\mathsf{min}(u) \coloneqq  e_{\ones}(u) - e_{\ones}(u)^T\cdot e_{\mathsf{Prev}} \cdot u.$$
Therefore, to access the first canonical vector in the order given by \texttt{for}, we can write:
$$e_{\mathsf{min}} \coloneqq  \ffor{v}{X}{X + \mathsf{min}(v)\times v}.$$


 
Analogously, we can obtain the \textit{next} relation between canonical vectors by using $e_{\mathsf{Next}} = e_{\mathsf{Prev}}^T$. This defines the matrix $\mathsf{Next}= \mathsf{Prev}^T$, shown in Fig.~\ref{fig:ordermat}, which, for a canonical vector $b_i^n$, satisfies:
\[
{\mathsf{Next}}\cdot b_i^n=\begin{cases}
               b_{i+1}^n & \text{ if } i < n \\
              \mathbf{0} & \text{ if } i = n.
            \end{cases}
\]
In this way, we also can obtain the following operators for a canonical vector $v$: 
$$\mathsf{prev}(v)\coloneqq e_{\mathsf{Prev}}\cdot v \text{ and }
\mathsf{next}(v)\coloneqq e_{\mathsf{Next}}\cdot v$$
which, when evaluated on a canonical vector $b_i^n$ return $b_{i-1}^n$, and $b_{i+1}^n$, respectively. 
To access an arbitrary canonical vector by moving $i$ positions forward from the first canonical vector, or by moving $i$ positions backwards from the last canonical vector we define:
\[
e_{\mathsf{min}+i}\coloneqq \underbrace{\mathsf{next}(\cdots \mathsf{next}}_{i\text{ times}}(e_{\mathsf{min}}))
\text{ and }
e_{\mathsf{max}-i}\coloneqq \underbrace{\mathsf{prev}(\cdots \mathsf{prev}}_{i\text{ times}}(e_{\mathsf{max}})).
\]

Finally, later on in the proof of Proposition~\ref{prop:inverse} we will need  matrix powers $\mathsf{Next}^i$ of $\mathsf{Next}$, where $i$ is specified as
a canonical vector $b_i^n$. We can express this in \langfor by using
$$e_{\mathsf{getNextMatrix}}(v)\coloneqq \ffor{w}{X = e_\mathsf{Id}}{ X \cdot \Big( \EDIT{\islessorequal}(w,v)\times e_{\mathsf{Next}} + (1 - \EDIT{\islessorequal}(w,v))\times e_{\mathsf{Id}} \Big) }$$
which, when $v$ is interpreted as canonical vector $b_i^n$, output 
$\mathsf{Next}^i$. 
Intuitively, in computing  $e_{\mathsf{getNextMatrix}}(b_i^n)$, we start the for-loop by initializing $X$ to the identity via the expression $e_{\mathsf{Id}}$. Following this, the matrix stored in $X$ is multiplied by $\mathsf{Next}$ for the first $i$ canonical vectors (as specified by the subexpression $\EDIT{\islessorequal}(w,v)\times e_{\mathsf{Next}}$), and by the identity matrix for the canonical vectors $b_{i+1}^n, \ldots , b_n^n$. 


We thus see that having order information available results in \langfor to be quite expressive. And indeed,
we heavily rely on order information in the next sections when computing the inverse of matrices and more generally to simulate low complexity Turing machines and arithmetic circuits.


\subsection{Useful shorthands and simplifications}\label{sec:queries:simp}
\EDIT{Before providing further examples of algorithms expressible in \langfor, we introduce several \langfor expression macros which are commonly used in linear algebra, programming, and which will be crucial for our constructions throughout the paper.}
\EDIT{
\paragraph{Flow control}
When defining linear algebra algorithms, one of the most common operations is to have an if-then-else block of statements. This can be simulated in \langfor as follows:
%The following definition proves to be useful.
$$\IfThenElse{e_1}{e_2}{e_3}\coloneqq e_1\times e_2 + (1-e_1)\times e_3.$$

Intuitively, if $e_1$ stores the value 1, then we will use the expression $e_2$, while $e_3$ will be used if $e_1$ stores a zero.
%
For example, the expression $e_{\mathsf{getNextMatrix}}(v)$ defined above can be written as:
$$e_{\mathsf{getNextMatrix}}(v)\coloneqq \ffor{w}{X = e_\mathsf{Id}}{ X \cdot \Big( \IfThenElse{\islessorequal(w,v)}{e_{\mathsf{Next}}}{e_{\mathsf{Id}}} \Big) }.$$
}

\paragraph{Iteration based on sum and product.} When defining $4$-cliques (Example \ref{ex:fourcliques}), the diagonal matrix (Example \ref{ex:diag}), or the ones vector algorithm (Example \ref{ex:onevec}), we 
 only update $X$ by adding some matrix to it. This restricted form of for-loop will prove useful throughout the paper, and we therefore introduce it as a special operator. That is, we define:
$$\Sigma v. \, e \coloneqq  \ffor{v}{X}{X + e}.$$
Here, $e=e(v)$ is an expression that can depend on the matrix variable $v$. 
Using this new notation, we can now define the ones-vector expression $e_{\ones}$ from Example \ref{ex:onevec}, and $e_\mathsf{Id}$ from Example \ref{ex:diag} as:
\[ e_{\ones} \coloneqq  \Sigma v. \, v \ \ \ \text{ and } \ \ \ 
e_\mathsf{Id} \coloneqq  \Sigma v. \, (v^T\cdot e)\times v\cdot v^T.
\]

In a similar way, we can consider an operator in which the for-loops can update the computed matrix only by taking the product with the value of the matrix in the previous iteration. Formally, for an expression $e$ we define the operator:
$$
\sprod v.\,  e \coloneqq \ffor {v}{X = e_{\mathsf{Id}}}{X\cdot e}.
$$
where $e_{\mathsf{Id}}$ is the expression defining the identity matrix and $e=e(v)$ is an expression that can depend on $v$. Notice that when updating the computed matrix via products it makes little sense to start with the zero matrix (since the result would always be zero), so we initialize the matrix to be equal to identity. 
As an example, using this operator it is easy to define the product of the diagonal elements of a square matrices using the following expression $e$:
$$
e \coloneqq  \sprod v. \, v^T\cdot V \cdot v.
$$

We will come back to these operators, and restrictions of \langfor which only use for-loops defined via $\ssum v.e$ and $\sprod v.e$, in Section \ref{sec:restrict}.

\paragraph{Simplifying function application.} When showing results based on induction of expressions in \langfor, it is often convenient to assume that function applications $f(e_1,\ldots,e_k)$ for $f\in\Fun_k$ are restricted to
the case when all expressions $e_1,\ldots,e_k$ have type $1\times 1$. This will not result in any loss of generality. Indeed,
for general function applications $f(e_1,\ldots,e_k)$, over expressions of arbitrary type, if we have $\ssum$, scalar product and function application on scalars (here denoted by $f_{1\times 1}$), we can simulate full function application, as follows:
 $$
f(e_1,\ldots, e_k) \coloneqq \Sigma v_i \Sigma v_j. \, f_{1\times 1}(v_i^T\cdot e_1\cdot v_j, \ldots ,v_i^T\cdot e_k\cdot v_j) \times v_i\cdot v_j^T.
$$

Furthermore, it also convenient at times to use the pointwise functions
$f_\odot^k:\RR^k\mapsto \RR:(x_1,\ldots,x_k)\mapsto x_1\times \cdots \times x_k$ and 
$f_\oplus^k:\RR^k\mapsto \RR:(x_1,\ldots,x_k)\mapsto x_1+\cdots + x_k$. In fact, it is readily observed that adding these functions does not extend the expressive power of \langfor:
\begin{lemma}
\label{lm-prod-sum}
We have that $\langforf{\emptyset} \equiv \langforf{\{f_\odot^k,f_\oplus^k \ | \ k\in \mathbb{N}\}}$.
\end{lemma}
In fact, this lemma also holds for the smaller fragments we consider.
%
We also observe that having $f_\odot^2:\RR^2\to\RR$ allows us to define scalar multiplication:
$$
e_1\times e_2 \coloneqq f_{\kprod}(\ones(e_2)^T\cdot e_1 \cdot \ones(e_2)^T, e_2).
$$
Conversely, $f_\odot^k$ can be expressed using scalar multiplication, as can be seen from our simulation of general function applications by pointwise function application on scalars.
