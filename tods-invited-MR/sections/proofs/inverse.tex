% !TeX spellcheck = en_US
%!TEX root = ../../main.tex

We begin by showing that the result holds in a special case when considering non-singular lower or upper triangular matrices.
\begin{lemma}\label{prop:upperlowerinverse}
There are $\langforf{f_/}$ expressions $e_{\mathsf{upperDiagInv}}(V)$ and $e_{\mathsf{lowerDiagInv}}(V)$
such that $\sem{e_{\mathsf{upperDiagInv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
to an invertible upper triangular matrix $A$ and, similarly, $\sem{e_{\mathsf{lowerDiagInv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
to an invertible lower triangular matrix $A$.
\end{lemma}

\begin{proof} We start by considering the expression:
    $$
    e_{\mathsf{ps}}(V)\coloneqq  e_{\mathsf{Id}} + \ssum v.\sprod w. \left[ \EDIT{\islessorequal}(w,v)\times V + (1 - \EDIT{\islessorequal}(w,v))\times e_{\mathsf{Id}} \right].
    $$
    Here, $e_{\mathsf{ps}}(A)$ results in $I+A+A^2+\cdots + A^n$ for any matrix $A$. In the expression, the outer loop defines which power we compute. 
    That is, when $v$ is the $i$th canonical vector, we compute $A^i$.
    Computing $A^i$ is achieved via the inner product loop, which uses $\EDIT{\islessorequal}(w,v)$ to 
    determine whether $w$ comes before or is $v$ in the ordering of canonical vectors.
    When this is the case, we multiply the current result by $A$, and when $w$ is greater 
    than $v$, we use the identity as not to affect the already computed result. We add the identity at the end.

    Now, let $A$ be an $n\times n$ matrix that is upper triangular and let $D_A$ be the diagonal matrix consisting of the diagonal elements of $A$.
	% , i.e.,
%     \[
%     D_A = \begin{bmatrix}
%         a_{11} & \cdots & \cdots &  0 \\
%         0 & a_{22} & \cdots &  0 \\
%         0 & \ddots & \vdots & \vdots \\
%         \vdots & \cdots& \cdots & a_{nn}
%     \end{bmatrix}.
%     \]
    We can compute $D_A$ by the expression:
    $$
    e_{\mathsf{getDiag}}(V) \coloneqq  \ssum v. (v^T\cdot V\cdot v) \times v\cdot v^T.
    $$
    Let $T=A-D_A$. We can write
    $$
    A^{-1}=\left[ D_A+T \right]^{-1}= \left[ D_A\left( I+D_A^{-1}T\right) \right]^{-1} = \left( I+D_A^{-1}T\right)^{-1}D_A^{-1}.
    $$
    We now observe that $D_{A}^{-1}$ simply consists of the inverses of the elements on the diagonal. This can be expressed, as follows:
    $$
    e_{\mathsf{diagInverse}}(V)\coloneqq \ssum v. f_{/}(1,v^T\cdot V\cdot v)\times v\cdot v^T
    $$
    where $f_{/}$ is the division function. In the last equality we take advantage of the fact that the diagonals of $A$ and $D_A$ are the same.

    We now focus on the computation of $\left( I+D_A^{-1}\cdot T\right)^{-1}$. First, by construction, $D_A^{-1}\cdot T$ is strictly upper triangular and thus nilpotent, i.e.,   $\left( D_A^{-1}\cdot T\right)^n=0$  where $n$ is the dimension of $A$.
    Recall the following algebraic identity 
    $$(1+x)\left( \sum_{i=0}^{m}(-x)^i \right)=1-(-x)^{m+1}.$$
    By choosing $m=n-1$ and applying it to $x=D_A^{-1}\cdot T$, we have
    $$
    \left(I+D_A^{-1}\cdot T \right)\left( \sum_{i=0}^{n-1}(-D_A^{-1}\cdot T)^i \right)=I- \left( -D_A^{-1}\cdot T\right)^n =I.
    $$
    Hence,
    $$
    \left(I+D_A^{-1}\cdot T \right)^{-1}=\sum_{i=0}^{n-1}(-D_A^{-1}\cdot T)^i=\sum_{i=0}^{n}(-D_A^{-1}\cdot T)^i.
    $$
    We now observe that
    $$
    e_{\mathsf{ps}}(-1\times D_A^{-1}\cdot T)=\sum_{i=0}^{n}(-D_A^{-1}\cdot T)^i=\left(I+D_A^{-1}\cdot T \right)^{-1},
    $$
    and thus 
    $$
    A^{-1}= e_{\mathsf{ps}}\left(-1\times \left[e_{\mathsf{diagInverse}}(A)\cdot(A-e_{\mathsf{getDiag}}(A))\right] \right)\cdot e_{\mathsf{diagInverse}}(A).
    $$
    Seeing this as a \langfor\ expression:
    $$
    e_{\mathsf{upperDiagInv}}(V)\coloneqq  e_{\mathsf{ps}}\left(-1\times \left[e_{\mathsf{diagInverse}}(V)\cdot(V-e_{\mathsf{getDiag}}(V))\right] \right)\cdot e_{\mathsf{diagInverse}}(V),
    $$
    we see that  when interpreting $V$ as an  upper triangular invertible matrix, 
    $e_{\mathsf{upperDiagInv}}(A)$ evaluates to $A^{-1}$.
   To deal with invertible lower triangular matrices $A$, we observe that  $\left(A^{-1}\right)^T=\left(A^T\right)^{-1}$ and $A^T$ is upper triangular.
    Hence, it suffices to define
    $$
    e_{\mathsf{lowerDiagInv}}(V)\coloneqq  e_{\mathsf{upperDiagInv}}(V^T)^T.
    $$
    This concludes the proof of the lemma.
\end{proof}

\medskip 
\noindent{\textsc{Proof of Proposition \ref{prop:inverse}.}}
With this result at hand, we are now ready to prove Proposition \ref{prop:inverse}. More specifically, we rely on Csanky's algorithm for computing the inverse of a matrix~\cite{Csanky76}.
This algorithm uses the characteristic polynomial $p_A(x)=\mathsf{det}(xI-A)$ of a matrix. When expanded as a polynomial $p_A(x)=\sum_{i=0}^{n} c_i x^i$, it is known that
$A^{-1}=\frac{1}{c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i},$ if $c_n\neq 0$. It also holds that $c_n=(-1)^n\mathsf{det}(A)$, and   $c_0 = 1$. We can thus write $p_A(x)=1 + \sum_{i=1}^n c_ix^i$. Furthermore, the coefficients $c_1,\ldots,c_n$ are known to satisfy the system of equations $S\cdot \bar c=\bar s$ given by:
    $$
    \underbrace{\left(\begin{matrix}
    1 & 0 & 0 & \cdots & 0 & 0\\
    S_1 & 1 & 0 & \cdots  &0 & 0\\
    S_2 & S_1 & 1 & \cdots  &0 & 0\\
    \vdots & \vdots & \vdots & \vdots & \vdots & 0\\
    S_{n-1} & S_{n-2} & S_{n-3} & \cdots & S_1 & 1\\
    \end{matrix}\right)}_{S}\cdot
    \underbrace{\left(\begin{matrix}
    c_1\\
    c_2\\
    c_3\\
    \vdots\\
    c_n\\
    \end{matrix}\right)}_{\bar c}=\underbrace{\left(\begin{matrix}
    S_1\\
    S_2\\
    S_3\\
    \vdots\\
    S_n\\
    \end{matrix}\right)}_{\bar s},
    $$
with $S_i\coloneqq \frac{1}{i+1}\mathsf{tr}(A^i)$, where $\mathsf{tr}(\cdot)$ is the trace operator that sums up the diagonal elements of a matrix. 

    We can now compute the $S_i's$ in $\langfor$ as follows. First, for
    $i=1,\ldots,n$ we can consider
    $$
    e_{\mathsf{powTr}}(V,v)\coloneqq \ssum w. w^T\cdot\left(e_{\mathsf{pow}}(V,v)\cdot V\right)\cdot w
    $$
    with 
    $$
    e_{\mathsf{pow}}(V,v)\coloneqq  \sprod w. (\EDIT{\islessorequal}(w,v)\times V+(1-\EDIT{\islessorequal}(w,v))\times e_{\mathsf{Id}}).
    $$
    We have that $e_{\mathsf{pow}}(A,b_j)=A^{j}$ and thus $e_{\mathsf{powTr}}(A,b_j)=\mathsf{tr}(A^{j})$. Define:
    $$
    e_{S}(V,v)\coloneqq f_{/}(1, 1 + \ssum w. \EDIT{\islessorequal}(w,v))\times e_{\mathsf{powTr}}(V,v).
    $$
    Here $e_{S}(A,b_i)=S_i$. Note that $i+1$ is computed summing up to the dimension indicated by $v$, and adding 1.
    We can now easily construct the vector $\bar s$ used in the system of equations by means of the expression:
    $$
    e_{\bar s}(V)\coloneqq \ssum w.e_{S}(V,w)\times w.
    $$
    We next construct the matrix $S$. We need to be able to \textit{shift} a vector $a$ in $k$ positions, i.e.,
    such that $(a_1,\ldots,a_n)\mapsto (0,\ldots,a_1,\ldots,a_{n-k})$. We use $e_{\mathsf{getNextMatrix}}$ 
    defined in Section~\ref{sec:formatlang:design}, i.e., we define:
    $$
    e_{\mathsf{shift}}(a,v)\coloneqq \ssum w.(w^T\cdot a)\times(e_{\mathsf{getNextMatrix}}(v)\cdot w)
    $$
    performs the desired shift when $u$ is assigned a vector $a$ and $v$ is $b_k$. 
    The matrix $S$ is now obtained as follows:
    $$
    S(V)\coloneqq  e_{\mathsf{Id}} + \ssum v. e_{\mathsf{shift}}(e_{\bar s}(V), v)\cdot v^T
    $$
    We now observe that $S$ is lower triangular with nonzero diagonal entries. So,
    Lemma~\ref{prop:upperlowerinverse} tells us that we can invert it, i.e.,
    $e_{\mathsf{lowerDiagInv}}(S)=S^{-1}$. As a consequence,
    $$
    e_{\bar c}(V)\coloneqq e_{\mathsf{lowerDiagInv}}(S(V))\cdot e_{\bar s}(V).
    $$
    outputs $\bar c$ when $V$ is interpreted as matrix $A$. Observe that we only use the division operator. We now have all coefficients of the characteristic polynomial of $A$.


    We can now define
    $$
    e_{\mathsf{det}}(V)\coloneqq \left( \left(\left( \sprod w. (-1)\times e_{\ones}(V)\right)^T\cdot e_{\mathsf{max}}\right) \times e_{\bar c}(V) \right)^T\cdot e_{\mathsf{max}},
    $$
    an expression that, when $V$ is interpreted as any matrix $A$, outputs $\mathsf{det}(A)$.
    Here, $(\sprod w. (-1)\times e_{\ones}(V))$ is the $n$ dimensional vector with $(-1)^n$ in all of its entries.
    Since $c_n=(-1)^n\mathsf{det}(A)$, we extract $(-1)^n(-1)^n\mathsf{det}(A)=\mathsf{det}(A)$ with $e_{\mathsf{max}}$.

    For the inverse, we have that
    $$
    A^{-1}=\frac{1}{c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i} = \frac{1}{c_n}A^{n-1} + \sum_{i=1}^{n-1}\frac{c_i}{c_n}A^{n-1-i}.
    $$
    We compute $\frac{1}{c_n}A^{n-1}$ as
    $$
    f_{/}(1, e_{\bar c}(A)^T\cdot e_{\mathsf{max}})\times e_{\mathsf{pow}}(A, e_{\mathsf{max}})
    $$
    and $\sum_{i=1}^{n-1}\frac{c_i}{c_n}A^{n-1-i}$ as
    $$
    \ssum v. f_{/}\left( e_{\bar c}(A)^T\cdot v, e_{\bar c}(A)^T\cdot e_{\mathsf{max}} \right)\times e_{\mathsf{invPow}}(A, v),
    $$
    where
    $$
    e_{\mathsf{invPow}}(V, v)\coloneqq  \sprod w. (1-\mathsf{max}(w)) \times \left[ (1 - \EDIT{\islessorequal}(w,v))\times V + \EDIT{\islessorequal}(w,v)\times e_{\mathsf{Id}} \right] + \mathsf{max}(w)\times e_{\mathsf{Id}}.
    $$
    Here, $e_{\mathsf{invPow}}(A, b_i)=A^{n-1-i}$ and $e_{\mathsf{invPow}}(A, b_n)=I$.
    Note that we always multiply by $e_{\mathsf{Id}}$ in the last step.
    To conclude, we define:
    $$
    e_{\mathsf{inv}}(V)\coloneqq  f_{/}(1, e_{\bar c}(V)^T\cdot e_{\mathsf{max}})\times e_{\mathsf{pow}}(V, e_{\mathsf{max}}) + \left[ \ssum v. f_{/}\left( e_{\bar c}(V)^T\cdot v, e_{\bar c}(V)^T\cdot e_{\mathsf{max}} \right)\times e_{\mathsf{invPow}}(V, v) \right],
    $$
    an expression that, when $V$ is interpreted as any invertible matrix $A$, computes $A^{-1}$. \qed

Note that we use a slightly different, but equivalent, system of equations to the originally one proposed in \cite{Csanky76}. Indeed,
for each equation $i=1, \ldots, n$, we divide the equality by $i$ and push the negative sign to the solution $c_1, \ldots, c_n$.

We conclude by observing that we here only use operators $\ssum$ and $\sprod$ defined in section \ref{sec:restrict} and
also assume access to order information.
