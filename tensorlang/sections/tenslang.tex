%!TEX root = ../main/main.tex
\section{TENSORLANG: syntax and semantics}\label{sec:tensorlang}

In the following, for $k\in\mathbb{N}$ we denote by $[k]$ the set $\{1,\ldots,k\}$.

We regard tensors as multi-dimensional arrays, taking values from the reals (or complex numbers).
More precisely, a tensor $T$ of \textit{order} $p$ and dimensions $n_1\times n_2\times\cdots\times n_p$,
with $p,n_1,\ldots,n_p\in \mathbb{N}$, consists of $\prod_{i=1}^p n_i$ elements
$$T[i_1,\ldots,i_p]\in\mathbb{R}, \text{ for $i_j\in [n_j]$, $j\in[p]$}.$$

The following operations seem to form a solid starting point:

\begin{itemize}
\item \textbf{Addition $(+)$, scalar multiplication $(\times)$ and general pointwise functions ($f(\cdot,\ldots,\cdot)$)} These are defined as usual, in an entry-wise way. The tensors involved need to have the same order and dimensions.

\item \textbf{Tensor product} Let $S$ be a tensor of order $p$ and of dimension $n_1\times n_2\times\cdots\times n_p$. Let 
$T$ be a tensor of order $q$ of dimension $m_1\times m_2\times\cdots\times m_p$. Then \textit{tensor product} of $S$ and $T$, denoted by $S\otimes T$, is a tensor of order $p+q$ and of dimension 
$n_1\times\cdots\times n_p\times m_1\times\cdots\times m_q$, and whose entries are defined as
$$
(S\otimes T)[i_1,\ldots,i_p,j_1,\ldots,j_q]:=S[i_1,\ldots,i_p]\cdot T[j_1,\ldots,j_k].
$$
Of particular intrest is that any tensor $T$ of order $p$ and of dimension $n_1\times\cdots\times n_p$ can be written as 
$$
T=\sum_{e_{i_1}^{n_1}}\cdots \sum_{e_{i_p}^{n_p}} T[i_1,\ldots,i_p]\times e_{i_1}^{n_1}\otimes \cdots \otimes e_{i_p}^{n_p},$$
where $e_j^n$ denotes a basis vector (tensor of order $1$ of dimension $n$).

\item \textbf{Contracted Tensor Multiplication.} Let $S$ be a tensor of order $p$ and of dimension $n_1\times n_2\times\cdots\times n_p$. Let 
$T$ be a tensor of order $q$ of dimension $m_1\times m_2\times\cdots\times m_p$. Consider ordered sequences $I$ of $[p]$ and $J$ of $[q]$ such that $I$ and $J$
are of the same length, and furthermore, neither $I$ nor $J$ contain duplicate entries.
For example, let $I=\langle i_1,\ldots,i_s\rangle$ and $J=\langle j_1,\ldots,j_s\rangle$. Assume furthermore that for all $k\in[s]$, the dimensions satisfy $n_{i_k}=m_{j_k}$. We denote the
indices for $S$ by $\mathbf{i}=(i_1,\ldots,i_p)$, those of $T$ by $\mathbf{j}=(j_1,\ldots,j_q)$. Given a sequence
$J$ of length $s$, as above, $\mathbf{i}_{\bar I}$ is the subsequence of $\mathbf{i}$ corresponding to indices not in $I$, similarly for $\mathbf{j}_{\bar J}$. Note that $\mathbf{i}_{\bar I}$ and $\mathbf{j}_{\bar J}$ are sequence of length $p-s$ and $q-s$, respectively.

Then the \textit{contracted tensor multiplication} of $S$ and $T$, relative to $I$ and $J$, is defined as the tensor
of order $p+q-2s$ and of dimension $\bigtimes_{ i\not\in I} n_i\times \bigtimes_{j\not \in J} m_j$, and whose entries are defined as
$$
S\cdot_{I,J} T[\mathbf{i}_{\bar I},\mathbf{j}_{\bar J}]:=\sum_{i_1=1}^{n_1}\sum_{i_2=1}^{n_2}\cdots\sum_{i_s=1}^{n_s} T[\mathbf{i}]\cdot T[\mathbf{j}]
$$
See more details: \url{https://dl.acm.org/doi/pdf/10.1145/1186785.1186794}. Example of this are matrix matrix multiplication, matrix vector multiplication, and dot product of vectors.

\item \textbf{Permutation, $\pi$-Transposition}. Let $T$ be a tensor of order $p$ and dimension $n_1\times n_2\times\cdots\times n_p$. Then, given a permutation 
$\pi:[p]\to [p]$, we define its \textit{permutation} $T^\pi$ as the tensor of order $p$ and of dimension
$n_{\pi(1)}\times n_{\pi(2)}\times\cdots\times n_{\pi(p)}$, such that for all $i_j\in[n_j]$, $j\in[p]$
$$
T^{\pi}[i_{\pi(1)},\ldots,i_{\pi(p)}]:=T[i_1,\ldots,i_p].
$$
Note that when $T$ is a tensor of order $2$ (and thus a matrix), for $\pi:[2]\to[2]$ such that $\pi(1)=2$ and $\pi(2)=1$,
$T^\pi[j,i]:=T[i,j]$ and this the standard matrix transposition. \url{https://arxiv.org/pdf/1411.1503.pdf} has more info.

Note that this does not cover transposition of a vector. That is, one typically regards a vector as a tensor of order $p$, so the above definition cannot mimick transpose of a vector. You can also see a vector of order $2$ and of dimension $n\times 1$, then it works. But then, when you consider tensor product of vectors $e_i$ and $e_j$, you do not want a $4\times 4$-matrix, so here you prefer $e_i$ and $e_j$ to be order 1 tensors.
Of course, this issue stems from the fact that tensor are not just arrays. A row vector is actually a covector, and tensors can have covariant and contravariant dimensions. I don't know whether we want to get into this. Recall also that when modeling vectors as relations we had two dimensions...


\item Kronecker...

\item For v. e(X,v)
 \end{itemize}


%
% We assume that we have a supply of matrix variables. The definiton of an instance $I$ on MATLANG is a function defined on a nonempty set $var(I)=\lbrace A, B, M, C,  \ldots\rbrace$, that assigns a concrete matrix to each element (matrix \textit{name}) of $var(I)$.
%
%
% Every expression $e$ is a matrix, either a matrix of $var(I)$ (\textit{base} matrix, if you will) or a result of an operation over matrices.
%
% The syntax of MATLANG expressions is defined by the following grammar. Every sentence is an expression itself.
%
% \begin{align*}
% 	e=M\hspace{1em}&\text{(matrix variable)} \\
% 	\text{let } M=e_1 \text{ in } e_2\hspace{1em}&\text{(local binding)} \\
% 	e^*\hspace{1em}&\text{(conjugate transpose)} \\
% 	\mathbf{1}(e)\hspace{1em}&\text{(one-vector)} \\
% 	\text{diag}(e)\hspace{1em}&\text{(diagonalization of a vector)} \\
% 	e_1\cdot e_2\hspace{1em}&\text{(matrix multiplication)} \\
% 	\text{apply}\left[ f \right](e_1, \ldots, e_n)\hspace{1em}&\text{(pointwise application of $f$)}
% \end{align*}
%
% The operations used in the semantics of the language are defined over complex numbers.
%
% \begin{itemize}
% 	\item \textbf{Transpose:} if $A$ is a matrix then $A^*$ is its conjugate transpose.
% 	\item \textbf{One-vector:} if $A$ is a $n\times m$ matrix then $\mathbf{1}(A)$ is the $n\times 1$ column vector full of ones.
% 	\item \textbf{Diag:} if $v$ is a $m\times 1$ column vector then diag$(v)$ is the matrix
% 		\[
% 			\begin{bmatrix}
% 			    v_{1}       & 0 & 0 & \dots & 0 \\
% 			    0       & v_{2} & 0 & \dots & 0 \\
% 			    \hdotsfor{5} \\
% 			    0       & 0 & 0 & \dots & v_{m}
% 			\end{bmatrix}
% 		\]
% 	\item \textbf{Matrix multiplication:} if $A$ is a $n\times m$ matrix and $B$ is a $m\times p$ matrix then $A\cdot B$ is the $n\times p$ matrix with $(AB)_{ij}=\sum_{k=1}^n A_{ik}B_{kj}$.
% 	\item \textbf{Pointwise application:} if $A^{(1)}, \ldots, A^{(n)}$ are $m\times p$ matrices, then apply$\left[ f \right](A^{(1)}, \ldots, A^{(n)})$ is the $m\times p$ matrix $C$ where $C_{ij}=f(A^{(1)}_{ij}, \ldots, A^{(n)}_{ij})$.
% \end{itemize}
%
% 	The formal semantics have a set of rules for an expression $e$ to be valid on an instance $I$, this is, $e$ succesfully evaluates to a matrix $A$ on the instance $I$. This success is denoted as $e(I)=A$. Here $I\left[ M:=A\right]$ denotes the instance that is equal to $I$ except that maps $M$ to the matrix $A$.
%
% 	\begin{center}
%     \begin{tabular}{ r | l }
%     \hline
%     \textbf{Expression} & \textbf{Condition for validity}\\ \hline
%     $(\text{let } M=e_1 \text{ in } e_2)(I)=B$ & $e_1(I)=A,\hspace{1ex}e_2(I\left[ M:=A\right])=B$ \\
%     $e^*(I)=A^*$ & $e(I)=A$ \\
%     $\mathbf{1}(e)(I)=\mathbf{1}(A)$ & $e(I)=A$ \\
%     $\text{diag}(e)(I)=\text{diag}(A)$ & $e(I)=A$, $A$ is a column vector \\
%     $e_1\cdot e_2(I)=A\cdot B$ & \# columns of $A$ $=$ \# rows of $B$ \\
%     apply$\left[ f\right](e_1, \ldots, e_n)(I)=\text{apply}\left[ f\right](A_1, \ldots, A_n)$ & $\forall k, e_k(I)=A$ and all $A_k$ have the same dimentions \\
%     \end{tabular}
% \end{center}
%
% For example, $$\text{let }N=\mathbf{1}(M)^*\text{ in apply}[c](\mathbf{1}(N)),$$ is an expression, where $c$ denotes the constant function $c:x\rightarrow c$. The result is a $1\times 1$ matrix with $c$ as its entry. This expression is equivalent to apply$[c](\mathbf{1}(\mathbf{1}(M)^*))$.
%
% An example of what can be computed in MATLANG is the mean of a vector:
% \begin{align*}
% 	&\text{let }N=\mathbf{1}(v)^*\cdot\mathbf{1}(v)\text{ in} \\
% 	&\text{let }S=v^*\cdot\mathbf{1}(v)\text{ in} \\
% 	&\text{let }R=\text{apply}[\div](S, N)\text{ in } R.
% \end{align*}
% Here, $N$ is the $1\times 1$ matrix with the dimention of $v$ as its entry. S computes the sum of all the entries of $v$. Finally, in $R$ we store the result of the sum divided by the dimention. \\
%
%
% One thing that is worth keeping in mind is that pointwise function application is powerful. With the expression apply[$f$]($\cdot$) one can compute other elemental operations over matrices that can be studied separately, such as:
% \begin{itemize}
% 	\item Scalar multiplication: we compute $c\cdot A$ as
% 		\begin{align*}
% 			&\text{let }C=\text{apply}[c](\mathbf{1}(\mathbf{1}(A)^*))\text{ in} \\
% 			&\text{let }M=\mathbf{1}(A)\cdot C\cdot\mathbf{1}(A^*)^*\text{ in apply}[\times](M,A).
% 		\end{align*}
% 	\item Addition: we compute $A+B$ as $$\text{let apply}[+](A,B).$$
% 	\item Trace: let
% 		\[
%   			m(x, y)=\begin{cases}
%                x \text{ if } x-y>0 \\
%                0 \text{ if }x-y\leq 0
%             \end{cases}
% 		\]
% 		Then we compute the trace of $A$, $tr(A)$, as
% 		\begin{align*}
% 			&\text{let }I=\text{diag}(\mathbf{1}(A))\text{ in} \\
% 			&\text{let }B=\text{apply}[-](A, I)\text{ in} \\
% 			&\text{let }C=\text{apply}[m](A,B)\text{ in} \\
% 			&\text{let }T=\text{apply}[+](A,I)\text{ in } \mathbf{1}(A)^*\cdot T\cdot\mathbf{1}(A)\\
% 		\end{align*}
% \end{itemize}
%
