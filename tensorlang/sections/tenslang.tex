%!TEX root = ../main/main.tex
\section{TENSORLANG: syntax and semantics}\label{sec:tensorlang}

In the following, for $k\in\mathbb{N}$ we denote by $[k]$ the set $\{1,\ldots,k\}$.

We regard tensors as multi-dimensional arrays, taking values from the reals (or complex numbers).
More precisely, a tensor $T$ of \textit{order} $p$ and dimensions $n_1\times n_2\times\cdots\times n_p$,
with $p,n_1,\ldots,n_p\in \mathbb{N}$, consists of $\prod_{i=1}^p n_i$ elements
$$T[i_1,\ldots,i_p]\in\mathbb{R}, \text{ for $i_j\in [n_j]$, $j\in[p]$}.$$

The following operations seem to form a solid starting point:

\begin{itemize}
\item \textbf{Addition $(+)$, scalar multiplication $(\times)$ and general pointwise functions ($f(\cdot,\ldots,\cdot)$)} These are defined as usual, in an entry-wise way. The tensors involved need to have the same order and dimensions.

\item \textbf{Tensor product} Let $S$ be a tensor of order $p$ and of dimension $n_1\times n_2\times\cdots\times n_p$. Let 
$T$ be a tensor of order $q$ of dimension $m_1\times m_2\times\cdots\times m_p$. Then \textit{tensor product} of $S$ and $T$, denoted by $S\otimes T$, is a tensor of order $p+q$ and of dimension 
$n_1\times\cdots\times n_p\times m_1\times\cdots\times m_q$, and whose entries are defined as
$$
(S\otimes T)[i_1,\ldots,i_p,j_1,\ldots,j_q]:=S[i_1,\ldots,i_p]\cdot T[j_1,\ldots,j_k].
$$
Of particular intrest is that any tensor $T$ of order $p$ and of dimension $n_1\times\cdots\times n_p$ can be written as 
$$
T=\sum_{e_{i_1}^{n_1}}\cdots \sum_{e_{i_p}^{n_p}} T[i_1,\ldots,i_p]\times e_{i_1}^{n_1}\otimes \cdots \otimes e_{i_p}^{n_p},$$
where $e_j^n$ denotes a basis vector (tensor of order $1$ of dimension $n$).

\item \textbf{Contracted Tensor Multiplication.} Let $S$ be a tensor of order $p$ and of dimension $n_1\times n_2\times\cdots\times n_p$. Let 
$T$ be a tensor of order $q$ of dimension $m_1\times m_2\times\cdots\times m_p$. Consider ordered sequences $I$ of $[p]$ and $J$ of $[q]$ such that $I$ and $J$
are of the same length, and furthermore, neither $I$ nor $J$ contain duplicate entries.
For example, let $I=\langle i_1,\ldots,i_s\rangle$ and $J=\langle j_1,\ldots,j_s\rangle$. Assume furthermore that for all $k\in[s]$, the dimensions satisfy $n_{i_k}=m_{j_k}$. We denote the
indices for $S$ by $\mathbf{i}=(i_1,\ldots,i_p)$, those of $T$ by $\mathbf{j}=(j_1,\ldots,j_q)$. Given a sequence
$J$ of length $s$, as above, $\mathbf{i}_{\bar I}$ is the subsequence of $\mathbf{i}$ corresponding to indices not in $I$, similarly for $\mathbf{j}_{\bar J}$. Note that $\mathbf{i}_{\bar I}$ and $\mathbf{j}_{\bar J}$ are sequence of length $p-s$ and $q-s$, respectively.

Then the \textit{contracted tensor multiplication} of $S$ and $T$, relative to $I$ and $J$, is defined as the tensor
of order $p+q-2s$ and of dimension $\bigtimes_{ i\not\in I} n_i\times \bigtimes_{j\not \in J} m_j$, and whose entries are defined as
$$
S\cdot_{I,J} T[\mathbf{i}_{\bar I},\mathbf{j}_{\bar J}]:=\sum_{i_1=1}^{n_1}\sum_{i_2=1}^{n_2}\cdots\sum_{i_s=1}^{n_s} T[\mathbf{i}]\cdot T[\mathbf{j}]
$$
See more details: \url{https://dl.acm.org/doi/pdf/10.1145/1186785.1186794}. Example of this are matrix matrix multiplication, matrix vector multiplication, and dot product of vectors.

\item \textbf{Permutation, $\pi$-Transposition}. Let $T$ be a tensor of order $p$ and dimension $n_1\times n_2\times\cdots\times n_p$. Then, given a permutation 
$\pi:[p]\to [p]$, we define its \textit{permutation} $T^\pi$ as the tensor of order $p$ and of dimension
$n_{\pi(1)}\times n_{\pi(2)}\times\cdots\times n_{\pi(p)}$, such that for all $i_j\in[n_j]$, $j\in[p]$
$$
T^{\pi}[i_{\pi(1)},\ldots,i_{\pi(p)}]:=T[i_1,\ldots,i_p].
$$
Note that when $T$ is a tensor of order $2$ (and thus a matrix), for $\pi:[2]\to[2]$ such that $\pi(1)=2$ and $\pi(2)=1$,
$T^\pi[j,i]:=T[i,j]$ and this the standard matrix transposition. \url{https://arxiv.org/pdf/1411.1503.pdf} has more info.

Note that this does not cover transposition of a vector. That is, one typically regards a vector as a tensor of order $p$, so the above definition cannot mimick transpose of a vector. You can also see a vector of order $2$ and of dimension $n\times 1$, then it works. But then, when you consider tensor product of vectors $e_i$ and $e_j$, you do not want a $4\times 4$-matrix, so here you prefer $e_i$ and $e_j$ to be order 1 tensors.
Of course, this issue stems from the fact that tensor are not just arrays. A row vector is actually a covector, and tensors can have covariant and contravariant dimensions. I don't know whether we want to get into this. Recall also that when modeling vectors as relations we had two dimensions...


\item Kronecker...

\item For v. e(X,v)
 \end{itemize}


\section{Related papers}

\subsection{Theory}
\begin{itemize}
\item ``The complexity of tensor circuit evaluation''
 \url{https://link.springer.com/article/10.1007/s00037-007-0222-0} 
Tensor circuits and formulas are considered, evaluation problem is studied for semirings, algebraic Turing machines and complexity classes are tied to the evaluation problem.
 
Although they work with tensors, everything is a matrix. That is, they use tensor product, but actually always work with the \textit{matricisation} of tensors.
For example, the result of the tensor product is represented in matrix terms, by the Kronecker product.
More precisely, if $A$ and $B$ are matrices of dimension $k\times \ell$ and $m\times n$, respectively,
$$
(A\otimes B)_{ij}=A_{q,r}\cdot B_{s,t}
$$ 
where $i=m(q-1)+s$ and $j=n(r-1)+t$.
 
 They define a tensor circuit $C=(e_{1},\ldots,e_{n})$ 
  where each $e_{i}$ is an instruction of the form
 $$ M \,|\, e_{j}+e_{k} \,|\, e_{j}\cdot e_{k} \,|\, e_{j}\otimes e_{k} $$
 with $j,k\in \{1,\ldots,i-1\}$ and $M$ is a matrix variable. They call it a tensor formula
 if the underlying DAG is a tree.
 
 Typing rules: tensor addition $e_{j}+e_{k}$, $e_j$ and $e_k$ must be of type $\alpha\times\beta$, result is of type $\alpha\times\beta$;
matrix multiplication $e_{j}\cdot e_{k}$, $e_j$ of type $\alpha\times\beta$, $e_k$ of type
$\beta\times \gamma$, result is of type $\alpha\times\gamma$;
tensor product $e_{j}\otimes e_{k}$, if $e_j$ is of type $\alpha\times\beta$ and $e_k$ is of type $\gamma\times\delta$, then result is of type $(\alpha\cdot \gamma)\times (\beta\cdot\delta)$.

Seems not cited very well, only other relevant paper is 
``Circuits arithm√©tiques et calculs tensoriels'' \url{https://doi.org/10.1017/S1474748008000248}.
  
\item ``The Arithmetic Complexity of Tensor Contraction''
\url{https://link.springer.com/article/10.1007%2Fs00224-015-9630-8}

Tensor are multidimensional arrays, they simple refer to tensors of order $(n_1,\ldots,n_k)$ and refer to $k$ as their dimension (so different what I defined above). A tensor $T$ is mappings from $[n_1]\times [n_k]\to \mathbb{K}$. A special case of contraction is considered.

First, $S$ and $T$ are $k$ and $\ell$ dimensional of order $(n_1,\ldots,n_k)$ and $(m_1,\ldots,m_\ell)$.
take indexes $i\in[k]$ and $j\in[\ell]$ such that $n_i=m_j$. Then,
$$
(S\star_{i,j} T)[\mathbf{i}_1,\mathbf{i}_2,\mathbf{i_3},\mathbf{i}_4]:=\sum_{r=1}^{n_i} S[\mathbf{i}_1,r,\mathbf{i}+2]\cdot S[\mathbf{i}_3,r,\mathbf{i}_4],
$$
with $\mathbf{i}_1\in [n_1]\times \cdots\times [n_{i-1}]$, 
$\mathbf{i}_2\in [n_{i+1}]\times \cdots\times [n_k]$, $\mathbf{i}_3\in [m_1]\times \cdots\times [m_{j-1}]$ and
$\mathbf{i}_4\in [m_{j+1}]\times \cdots\times [m_\ell]$.

So, this is like the contracted tensor multiplication defined earlier, but only on two indices. \textbf{Not sure whether the general contracted tensor multiplication can be expressed as repeated two index contracted version.}

The consider $\star$-formulas, which are trees whose leaves are labeled by tensors or variables, and internal nodes are $\star_{i,j}$. Bottom up evaluation results in a polynomial in  $\mathbb{K}[X_1,\ldots,X_n]$ and they link $\star$-formulas to arithmetic circuits and the class $\mathsf{VP}$.
  
 \end{itemize}

\subsection{Systems}
\begin{itemize}
\item \textbf{Matlab}: \url{https://dl.acm.org/doi/pdf/10.1145/1186785.1186794} need to check.
\item \textbf{Maple}: \url{https://www.maplesoft.com/support/help/maple/view.aspx?path=DifferentialGeometry%2FTensor}
\item \textbf{Tensorflow}:??
\item \url{https://arxiv.org/abs/1405.7786}
\end{itemize}
%

% We assume that we have a supply of matrix variables. The definiton of an instance $I$ on MATLANG is a function defined on a nonempty set $var(I)=\lbrace A, B, M, C,  \ldots\rbrace$, that assigns a concrete matrix to each element (matrix \textit{name}) of $var(I)$.
%
%
% Every expression $e$ is a matrix, either a matrix of $var(I)$ (\textit{base} matrix, if you will) or a result of an operation over matrices.
%
% The syntax of MATLANG expressions is defined by the following grammar. Every sentence is an expression itself.
%
% \begin{align*}
% 	e=M\hspace{1em}&\text{(matrix variable)} \\
% 	\text{let } M=e_1 \text{ in } e_2\hspace{1em}&\text{(local binding)} \\
% 	e^*\hspace{1em}&\text{(conjugate transpose)} \\
% 	\mathbf{1}(e)\hspace{1em}&\text{(one-vector)} \\
% 	\text{diag}(e)\hspace{1em}&\text{(diagonalization of a vector)} \\
% 	e_1\cdot e_2\hspace{1em}&\text{(matrix multiplication)} \\
% 	\text{apply}\left[ f \right](e_1, \ldots, e_n)\hspace{1em}&\text{(pointwise application of $f$)}
% \end{align*}
%
% The operations used in the semantics of the language are defined over complex numbers.
%
% \begin{itemize}
% 	\item \textbf{Transpose:} if $A$ is a matrix then $A^*$ is its conjugate transpose.
% 	\item \textbf{One-vector:} if $A$ is a $n\times m$ matrix then $\mathbf{1}(A)$ is the $n\times 1$ column vector full of ones.
% 	\item \textbf{Diag:} if $v$ is a $m\times 1$ column vector then diag$(v)$ is the matrix
% 		\[
% 			\begin{bmatrix}
% 			    v_{1}       & 0 & 0 & \dots & 0 \\
% 			    0       & v_{2} & 0 & \dots & 0 \\
% 			    \hdotsfor{5} \\
% 			    0       & 0 & 0 & \dots & v_{m}
% 			\end{bmatrix}
% 		\]
% 	\item \textbf{Matrix multiplication:} if $A$ is a $n\times m$ matrix and $B$ is a $m\times p$ matrix then $A\cdot B$ is the $n\times p$ matrix with $(AB)_{ij}=\sum_{k=1}^n A_{ik}B_{kj}$.
% 	\item \textbf{Pointwise application:} if $A^{(1)}, \ldots, A^{(n)}$ are $m\times p$ matrices, then apply$\left[ f \right](A^{(1)}, \ldots, A^{(n)})$ is the $m\times p$ matrix $C$ where $C_{ij}=f(A^{(1)}_{ij}, \ldots, A^{(n)}_{ij})$.
% \end{itemize}
%
% 	The formal semantics have a set of rules for an expression $e$ to be valid on an instance $I$, this is, $e$ succesfully evaluates to a matrix $A$ on the instance $I$. This success is denoted as $e(I)=A$. Here $I\left[ M:=A\right]$ denotes the instance that is equal to $I$ except that maps $M$ to the matrix $A$.
%
% 	\begin{center}
%     \begin{tabular}{ r | l }
%     \hline
%     \textbf{Expression} & \textbf{Condition for validity}\\ \hline
%     $(\text{let } M=e_1 \text{ in } e_2)(I)=B$ & $e_1(I)=A,\hspace{1ex}e_2(I\left[ M:=A\right])=B$ \\
%     $e^*(I)=A^*$ & $e(I)=A$ \\
%     $\mathbf{1}(e)(I)=\mathbf{1}(A)$ & $e(I)=A$ \\
%     $\text{diag}(e)(I)=\text{diag}(A)$ & $e(I)=A$, $A$ is a column vector \\
%     $e_1\cdot e_2(I)=A\cdot B$ & \# columns of $A$ $=$ \# rows of $B$ \\
%     apply$\left[ f\right](e_1, \ldots, e_n)(I)=\text{apply}\left[ f\right](A_1, \ldots, A_n)$ & $\forall k, e_k(I)=A$ and all $A_k$ have the same dimentions \\
%     \end{tabular}
% \end{center}
%
% For example, $$\text{let }N=\mathbf{1}(M)^*\text{ in apply}[c](\mathbf{1}(N)),$$ is an expression, where $c$ denotes the constant function $c:x\rightarrow c$. The result is a $1\times 1$ matrix with $c$ as its entry. This expression is equivalent to apply$[c](\mathbf{1}(\mathbf{1}(M)^*))$.
%
% An example of what can be computed in MATLANG is the mean of a vector:
% \begin{align*}
% 	&\text{let }N=\mathbf{1}(v)^*\cdot\mathbf{1}(v)\text{ in} \\
% 	&\text{let }S=v^*\cdot\mathbf{1}(v)\text{ in} \\
% 	&\text{let }R=\text{apply}[\div](S, N)\text{ in } R.
% \end{align*}
% Here, $N$ is the $1\times 1$ matrix with the dimention of $v$ as its entry. S computes the sum of all the entries of $v$. Finally, in $R$ we store the result of the sum divided by the dimention. \\
%
%
% One thing that is worth keeping in mind is that pointwise function application is powerful. With the expression apply[$f$]($\cdot$) one can compute other elemental operations over matrices that can be studied separately, such as:
% \begin{itemize}
% 	\item Scalar multiplication: we compute $c\cdot A$ as
% 		\begin{align*}
% 			&\text{let }C=\text{apply}[c](\mathbf{1}(\mathbf{1}(A)^*))\text{ in} \\
% 			&\text{let }M=\mathbf{1}(A)\cdot C\cdot\mathbf{1}(A^*)^*\text{ in apply}[\times](M,A).
% 		\end{align*}
% 	\item Addition: we compute $A+B$ as $$\text{let apply}[+](A,B).$$
% 	\item Trace: let
% 		\[
%   			m(x, y)=\begin{cases}
%                x \text{ if } x-y>0 \\
%                0 \text{ if }x-y\leq 0
%             \end{cases}
% 		\]
% 		Then we compute the trace of $A$, $tr(A)$, as
% 		\begin{align*}
% 			&\text{let }I=\text{diag}(\mathbf{1}(A))\text{ in} \\
% 			&\text{let }B=\text{apply}[-](A, I)\text{ in} \\
% 			&\text{let }C=\text{apply}[m](A,B)\text{ in} \\
% 			&\text{let }T=\text{apply}[+](A,I)\text{ in } \mathbf{1}(A)^*\cdot T\cdot\mathbf{1}(A)\\
% 		\end{align*}
% \end{itemize}
%
