%!TEX root = ../main.tex
% !TeX spellcheck = en_US
We conclude the paper by zooming in on some special fragments of \langfor and in which matrices can take values from an arbitrary (commutative) semiring $K$. In particular, we first consider \langsum, in which iterations can only perform
additive updates, and show that it is equivalent in expressive power to the (positive)
relational algebra on $K$-relations. We then extend \langsum such that also updates involving pointwise-multiplication (called Hadamard product) are allowed. The resulting fragment, \langprod, is shown to be equivalent in expressive power to weighted logics. Finally, we consider the fragment \langmprod in which updates involving sum and matrix multiplication, and possibly order information, is allowed. From the results in Section~\ref{sec:queries}, we infer that the latter fragment suffices to compute matrix inversion. An overview of the fragments and their relationships are depicted in Figure~\ref{thefigure}.

\subsection{Summation \lang and relational algebra}
\label{ss:sumML}
When defining $4$-cliques and in several other expressions we have seen so far, we 
 only update $X$ by adding some matrix to it. This restricted form of for-loop proved useful throughout the paper, and we therefore introduce it as a special operator. That is, we define:
$$\Sigma v. e := \ffor{v}{X}{X + e}.$$
Here, $e=e(v)$. We define the subfragment of \langfor, called \langsum, to consist of the $\Sigma$ operator plus the ``core'' operators in \lang, namely, transposition, matrix multiplication and addition, scalar multiplication, and pointwise function applications.

One property of \langsum is that it only allows expressions of polynomial degree. Indeed, one can easily show that \langsum can only create matrix entries that are polynomial in the dimension $n$ of the expression. More precisely, we can show the following:
\cristian{Check if this is used. Otherwise, just say it without proposition.}
\begin{proposition}\label{prop:poly}
Every expression in \langsum is of polynomial degree.
\end{proposition}

Interestingly enough, this restricted version of for-loop already allows us to capture the \lang\ operators that are not present in the syntax of \langsum. More precisely, we see from Examples~\ref{ex:onevec} and~\ref{ex:diag} that the one-vector and $\diag$ operator are expressible in \langsum. Combined with the observation that the $4$-clique
expression of Example~\ref{ex:fourcliques} is in \langsum, the following result is immediate.

\begin{corollary}
\lang\ is strictly subsumed by \langsum.
\end{corollary}

What operations over matrices can be defined with \langsum that is beyond \lang? In~\cite{brijder2019matrices}, it was shown that \lang\ is strictly included in the (positive) relational algebra on $K$-relations, denoted by $\mathsf{RA}_{K}^+$~\cite{GreenKT07}.\footnote{The algebra used in~\cite{brijder2019matrices} differs slightly from the one given in~\cite{GreenKT07}. In this paper we work with the original algebra $\mathsf{RA}_{K}^+$ as defined in~\cite{GreenKT07}.} 
It thus seems natural to compare the expressive power of \langsum with $\mathsf{RA}_{K}^+$. 
The main result in this section is that \langsum\ and $\mathsf{RA}_{K}^+$ 
are equally expressive over binary schemas. 
To make this equivalence precise, we next give the 
definition of $\mathsf{RA}_{K}^+$~\cite{GreenKT07} and then show how to connect both formalisms.

A semiring $(K, \ksum, \kprod, \kzero, \kone)$ is an algebraic structure where $K$ is a non-empty set, $\ksum$ and $\kprod$ are binary operations over $K$, and $\kzero, \kone \in K$. Furthermore,  $\ksum$ and $\kprod$ are associative operations, $\kzero$ and $\kone$ are the identities of $\ksum$ and $\kprod$ respectively, $\ksum$ is a commutative operation, $\kprod$ distributes over $\ksum$, and $\kzero$ annihilates $K$ (i.e. $\kzero \kprod k = k \kprod \kzero = \kzero$). As usual, we assume that all semirings in this paper are commutative, namely, $\kprod$ is also commutative. We use $\bigksum_X$ or $\bigkprod_X$ for the $\ksum$- or $\kprod$-operation over all elements in $X$, respectively. Typical examples of semirings are the reals $(\RR, +, \times, 0,1)$, the natural numbers $(\NN, +, \times, 0,1)$, and the boolean semiring $(\{0,1\}, \vee, \wedge, 0, 1)$. 

Let $\ddom$ be a data domain and $\att$ a set of attributes. A relational signature is a finite subset of $\att$. A relational schema is a function $\cR$ on finite set of symbols $\fdom(\cR)$ such that $\cR(R)$ is a relation signature for each $R \in \fdom(\cR)$. To simplify the notation, from now on we write $R$ to denote both the symbol $R$ and the relational signature $\cR(R)$.
Furthermore, we write $R \in \cR$ to say that $R$ is a symbol of $\cR$. 
For $R \in \cR$, an $R$-tuple is a function $t: R \rightarrow \ddom$. We denote by $\tuples(R)$ the set of all $R$-tuples. Given $X \subseteq R$, we denote by $t[X]$ the restriction of $t$ to the set $X$.

Fix a semiring $(K, \ksum, \kprod, \kzero, \kone)$ and a relational schema $\cR$. A $K$-relation of $R \in \cR$ is a function $r: \tuples(R) \rightarrow K$ such that the support  $\supp(r) = \{t \in \tuples(R) \mid r(t) \neq \kzero\}$ is finite. 
A $K$-instance $\cJ$ of $\cR$ is a function that assigns relational signatures of $\cR$ to $K$-relations. Given $R \in \cR$, we denote by $R^\cJ$ the $K$-relation associated to $R$. Recall that $R^\cJ$ is a function and hence  $R^\cJ(t)$ is the value in $K$ assigned to $t$. 
Given a $K$-relation $r$ we denote by $\adom(r)$ the active domain of $r$ defined as $\adom(r) = \{t(a) \mid t \in \supp(r) \wedge a \in R\}$.
Then the active domain of an $K$-instance $\cJ$ of $\cR$ is defined as $\adom(\cJ) = \bigcup_{R \in \cR} \adom(R^\cJ)$. 

An $\mathsf{RA}_{K}^+$  expression $\arae$ over $\cR$ is given by the following syntax:
$$
\begin{array}{rcl}
\arae & := & R \ \mid \ \arae \cup \arae \ \mid \  \pi_X(\arae) \ \mid \  \sigma_X(\arae) \ \mid \ \rho_f(\arae) \ \mid \ \arae \bowtie \arae
\end{array}
$$
where $R \in \cR$, $X \subseteq \att$ is finite, and $f: X \rightarrow Y$ is a one to one mapping with $Y \subseteq \att$. One can extend the schema $\cR$ to any expression over $\cR$ recursively as follows: $\cR(R) = R$, $\cR(\arae \cup \arae') = \cR(\arae)$, $\cR(\pi_X(\arae)) = X$, $\cR(\sigma_X(\arae)) = \cR(\arae)$, $\cR(\rho_f(\arae)) = X$ where $f:X \rightarrow Y$, and $\cR(\arae \bowtie \arae') = \cR(\arae) \cup \cR(\arae')$ for every expressions $\arae$ and $\arae'$.
We further assume that any expression $\arae$ satisfies the following syntactic restrictions: $\cR(\arae') = \cR(\arae'')$ whenever $\arae = \arae' \cup \arae''$, $X \subseteq \cR(\arae')$ whenever $\arae = \pi_X(\arae')$ or $\arae = \sigma_X(\arae')$, and $Y = \cR(\arae')$ whenever $\arae = \rho_f(\arae')$ with $f: X \rightarrow Y$.

Given an $\mathsf{RA}_{K}^+$ expression $\arae$ and a $K$-instance $\cJ$ of $\cR$, we define the semantics $\ssem{\arae}{\cJ}$ as a $K$-relation of $\cR(\arae)$ as follows. For $X \subseteq \att$, let $\operatorname{Eq}_X(t) = \kone$ when $t(a) = t(b)$ for every $a, b \in X$, and $\operatorname{Eq}_X(t) = \kzero$ otherwise. For every tuple $t \in \cR(\arae)$:
$$
\begin{array}{ll}
\!\!\!\text{if $\arae = R$, then} & \ssem{\arae}{\cJ}(t) = R^\cJ(t) \\
\!\!\!\text{if $\arae = \arae_1 \cup \arae_2$, then} & \ssem{\arae}{\cJ}(t) = \ssem{\arae_1}{\cJ}(t) \ksum \ssem{\arae_2}{\cJ}(t)  \\
\!\!\!\text{if $\arae = \pi_X(\arae')$, then} & \ssem{\arae}{\cJ}(t) = \bigksum_{t': t'[X] = t} \ssem{\arae'}{\cJ}(t') \\
\!\!\!\text{if $\arae = \sigma_X(\arae')$, then} & \ssem{\arae}{\cJ}(t) = 
\ssem{\arae'}{\cJ}(t) \kprod \operatorname{Eq}_X(t)  \\
\!\!\!\text{if $\arae = \rho_f(\arae')$, then} & \ssem{\arae}{\cJ}(t) = 
\ssem{\arae'}{\cJ}(t \circ f)  
\\
\!\!\!\text{if $\arae = \arae_1 \bowtie \arae_2$, then} & \ssem{\arae}{\cJ}(t) =  \ssem{\arae_1}{\cJ}(t[Y]) \kprod  \ssem{\arae_2}{\cJ}(t[Z])
\end{array}
$$
where $Y = \cR(\arae_1)$ and $Z = \cR(\arae_2)$. It is important to note that the $\bigksum$-operation in the semantics of $\pi_X(\arae')$ is well-defined given that the support of $\ssem{\arae'}{\cJ}$ is always finite. 

We are now ready for comparing \langsum with $\mathsf{RA}_{K}^+$. For a fair comparison, we need to extend \langsum from $\RR$ to any semiring. Let $\mtr{K}$ denote the set of all $K$-matrices. 
Similarly as for \lang\ over $\RR$, given a \lang\ schema $\Sch$, a $K$-instance $\I$ over $\Sch$ is a pair $\I = (\dom,\conc)$, where $\dom : \DD \mapsto \NN$ assigns a value to each size symbol, and $\conc : \Mnam \mapsto \mtr{K}$ assigns a concrete $K$-matrix to each matrix variable. Then it is straightforward to extend the semantics of \lang, \langfor, and \langsum from $(\RR, +, \times, 0, 1)$ to $(K, \ksum, \kprod, \kzero, \kone)$ by switching $+$ with $\ksum$ and $\times$ with $\kprod$. 

The next step for comparing \langsum with $\mathsf{RA}_{K}^+$  is to encode $K$-matrices as $K$-relations.
Let $\Sch=(\Mnam,\size)$ be a \lang\ schema. On the relational side
we have for each size symbol $\alpha\in\DD\setminus\{1\}$, attributes $\alpha$, $\row_\alpha$, and $\col_\alpha$ in $\att$. Furthermore, for each $V\in\Mnam$ and $\alpha \in \DD$ we denote
by $R_V$ and $R_\alpha$ its corresponding relation name, respectively. Then, given $\Sch$ we define the relational schema $\text{Rel}(\Sch)$ such that $\fdom(\text{Rel}(\Sch)) =  \{R_\alpha \mid \alpha\in\DD\} \cup \{R_V \mid V \in \Mnam\}$ where $\text{Rel}(\Sch)(R_\alpha) = \{\alpha\}$ and:
\[
\text{Rel}(\Sch)(R_V) = \begin{cases}
\lbrace\row_\alpha,\col_\beta \rbrace & \text{ if $ \size(V)=(\alpha,\beta)$} \\
\lbrace\row_\alpha \rbrace & \text{ if $ \size(V)=(\alpha,1)$} \\
\lbrace\col_\beta \rbrace  &
\text{ if $ \size(V)=(1,\beta)$} \\
\lbrace\rbrace & \text{ if $\size(V)=(1,1)$}.
\end{cases}
\]
Consider now a matrix instance $\I = (\dom,\conc)$ over $\Sch$.
Let $V\in\Mnam$ with $\size(V)=(\alpha,\beta)$ and let $\conc(V)$ be its corresponding $K$-matrix of dimension $\dom(\alpha)\times \dom(\beta)$.
To encode $\I$ as a $K$-instance in $\mathsf{RA}_{K}^+$, we use as data domain $\ddom = \mathbb{N} \setminus \{0\}$. Then we construct the $K$-instance $\text{Rel}(\I)$ such that for each $V\in\Mnam$ we define 
$R_V^{\text{Rel}(\I)}(t):=\conc(V)_{ij}$ whenever $t(\row_\alpha) = i \leq \dom(\alpha)$ and $t(\col_\beta) = j \leq \dom(\beta)$, and $\kzero$ otherwise. Furthermore, for each $\alpha \in \DD$ we define $R_\alpha^{\text{Rel}(\I)}(t):=\kone$ whenever $t(\alpha) \leq \dom(\alpha)$, and $\kzero$ otherwise. In other words, $R_\alpha$ and $R_\beta$ encodes the active domain of a matrix variable $V$ with $\size(V)=(\alpha,\beta)$. Given that the $\mathsf{RA}_{K}^+$ framework of \cite{GreenKT07} represents the ``absence'' of a tuple in the relation with $\kzero$, we need to separately encode the indexes in a matrix.
This is where $R_\alpha^{\text{Rel}(\I)}$ and $R_\beta^{\text{Rel}(\I)}$ are used for.
We are now ready to state the first connection between \langsum and $\mathsf{RA}_{K}^+$  by using the previous encoding. 
\begin{proposition}\label{prop:sum_to_ara} 
	For each \langsum expression $e$ over schema $\Sch$ such that $\Sch(e)=(\alpha,\beta)$ with $\alpha\neq 1\neq\beta$, there exists an $\mathsf{RA}_{K}^+$  expression $\Phi(e)$ over relational schema $\text{Rel}(\Sch)$ such that $\text{Rel}(\Sch)(\Phi(e))=\{\row_\alpha,\row_\beta\}$ and 
	such that for any instance $\I$ over~$\Sch$,
	$$
	\sem{e}{\I}_{i,j}=\ssem{\Phi(e)}{\text{Rel}(\I)}(t)
	$$
	for tuple $t(\mathrm{row}_\alpha)=i$ and $t(\mathrm{col}_\beta)=j$. Similarly for when $e$ has schema $\Sch(e)=(\alpha,1)$, $\Sch(e)=(1,\beta)$ or $\Sch(e)=(1,1)$, then $\Phi(e)$ has schema $\text{Rel}(\Sch)(\Phi(e))=\{\mathrm{row}_\alpha\}$,
	$\text{Rel}(\Sch)(\Phi(e))=\{\mathrm{col}_\alpha\}$, or
	$\text{Rel}(\Sch)(\Phi(e))=\{\}$, respectively.
\end{proposition}

\input{sections/proofs/sum-to-ara.tex}

We now move to the other direction.
To translate $\mathsf{RA}_{K}^+$  into \langsum, we must restrict our comparison to $\mathsf{RA}_{K}^+$  over $K$-relations with at most two attributes. Given that linear algebra works over vector and matrices, it is reasonable to restrict to unary or binary relations as input. Note that this is only a restriction on the input relations and not on intermediate relations, namely, expressions can create relation signatures of arbitrary size from the binary input relations. 
Thus, from now we say that a relational schema $\cR$ is binary if $|R| \leq 2$ for every $R \in \cR$. We also make the assumption that there is an (arbitrary) order, denoted by $<$, on the attributes in $\att$. 
This is to identify which attributes correspond to rows and columns when moving to matrices. 
Then, given that relations will be  either unary or binary and there is an order on the attributes, we write $t = (v)$ or $t = (u,v)$ to denote a tuple over a unary or binary relation $R$, respectively, where $u$ and $v$ is the value of the first and second attribute with respect to $<$.

Consider a binary relational schema $\cR$. With each $R\in \cR$ we associate a matrix variable $V_R$ such that, if $R$ is a binary relational signature, then $V_R$ represents a (square) matrix, and, if not (i.e. $R$ is unary), then $V_R$ represents a vector. Formally, fix a symbol $\alpha \in \DD \setminus \{1\}$. Let $\text{Mat}(\cR)$ denote the \lang \ schema
$(\Mnam_\cR,\size_\cR)$ such that $\Mnam_\cR = \{ V_R \mid R \in \cR\}$ and $\size_\cR(V_R) = (\alpha, \alpha)$ whenever $|R| = 2$, and $\size_\cR(V_R) = (\alpha, 1)$ whenever $|R|=1$. 
Take now a $K$-instance $\cJ$ of $\cR$ and suppose that $\adom(\cJ) = \{d_1, \ldots, d_n\}$ is the active domain of $\cJ$ (the order over $\adom(\cJ)$ is arbitrary). Then we define the matrix instance $\text{Mat}(\cJ) = (\dom_\cJ,\conc_\cJ)$ such that $\dom_\cJ(\alpha) = n$, $\conc_\cJ(V_R)_{i,j} = R^{\cJ}((d_i, d_j))$ whenever $|R|=2$, and $\conc_\cJ(V_R)_{i} = R^{\cJ}((d_i))$ whenever $|R|=1$. 
Note that, although each $K$-relation can have a different active domain, we encode them as square matrices by considering the active domain of the $K$-instance. By again using an inductive proof on the structure of 
$\mathsf{RA}_{K}^+$ expressions, we obtain the following result.
\begin{proposition}\label{prop:ara_to_sum} 
	Let $\cR$ be a binary relational schema. For each $\mathsf{RA}_{K}^+$  expression $\arae$ over $\cR$  such that $|\cR(\arae)| = 2$, there exists a \langsum  expression $\Psi(\arae)$ over \lang \ schema $\text{Mat}(\cR)$ such that for any $K$-instance $\cJ$ with $\adom(\cJ) = \{d_1, \ldots, d_n\}$ over $\cR$,
	$$
	\ssem{\arae}{\cJ}((d_i, d_j))=\sem{\Psi(\arae)}{\text{Mat}(\cJ)}_{i,j}.
	$$
	Similarly for when $|\cR(\arae)| = 1$, or $|\cR(\arae)| = 0$ respectively.
\end{proposition}

\input{sections/proofs/ara-to-sum.tex}

It is important to remark that the expression $\arae$ of the previous result can have intermediate expressions that are not necessarily binary, given that the proposition only restricts that the input relation and the schema of $\arae$ must have arity at most two. We recall from~\cite{brijder2019matrices} that \lang\ corresponds to $\mathsf{RA}_{K}^+$ where intermediate expressions are at most ternary, and this underlies, e.g., the inability of \lang\ to check for $4$-cliques. In \langsum, we can deal with intermediate relations of arbitrary arity. In fact, each new attribute can be seen to correspond to an application of the $\Sigma$ operator. For example, in the $4$-clique expression, four $\Sigma$ operators are needed, in analogy to how
$4$-clique is expressed in $\mathsf{RA}_{K}^+$.

Given the previous two propositions we derive the following conclusion which is the first characterization of relational algebra with a (sub)-fragment of linear algebra.
\begin{corollary}
	\langsum and $\mathsf{RA}_{K}^+$  over binary relational schemas are equally expressive. 
\end{corollary}

As a direct consequence, we have that \langsum cannot compute matrix inversion. Indeed, using similar arguments as
in~\cite{matlang-journal}, i.e., by embedding $\mathsf{RA}_{K}^+$  in (infinitary) first-order logic with counting and by leveraging its locality, one can show that \langsum cannot compute the transitive closure of an adjacency matrix. By contrast, the transitive closure can be expressed by means of matrix inversion~\cite{matlang-journal}. We also note that the evaluation of the $\Sigma$ operator is
independent of the order in which the canonical vectors are considered. This is because $\oplus$ is commutative.
Hence, \langsum cannot express the order predicates mentioned in Section~\ref{sec:formatlang}.



\subsection{Hadamard product and weighted logics}\label{subsec:langprod}
Similarly to using sum, we can use other operations to update $X$ in the for-loop. The next natural choice is to consider products of matrices. In contrast to matrix sum, we have two options: either we can choose to use matrix product or to use the pointwise matrix product, also called the Hadamard product. We treat  matrix product in the next subsection and first explain here the connection of sum and Hadamard product operators to weighted logics~\cite{DrosteG05}.

For the rest of this section, fix a semiring $(K, \ksum, \kprod, \kzero, \kone)$. The Hadamard product over $K$-matrices can be defined as the pointwise application of $\kprod$ between two matrices of the same size. Formally, we define the expression $e \hadprod e'$ where $e, e'$ are expressions with respect to $\cS$ and $\ttype(e) = \ttype(e')$ for some schema $\Sch=(\Mnam,\size)$. Then the semantics of $e \hadprod e'$ is the pointwise application of $\kprod$, namely, $\sem{e \hadprod e'}{\I}_{ij} = \sem{e}{\I}_{ij} \kprod \sem{e'}{\I}_{ij}$ for any instance $\I$ of $\cS$. This enables us to define, similar as for the operator $\Sigma v$, the  pointwise-product quantifier $\qhadprod v$ as follows:
$$
\qhadprod v. \  e := \ffor{v}{X\!=\!e_{\kone}}{X \circ e}.
$$
where $e_\kone$ is the \langfor expression for the matrix with the same type as $X$ and all entries equal to the $\kone$-element of $K$ (i.e., we need to initialize $X$ accordingly with the $\kprod$-operator).
We call \langprod  the subfragment of \langfor that consists of \langsum \ extended with $\qhadprod v$.

\begin{example}
	Similar to the trace of a matrix, a useful function in linear algebra is to compute the product of the values on the diagonal. 
	Using the $\qhadprod v$ operator, this can be easily expressed:
\begin{equation*}
	 e_{\mathsf{dp}}(V) := \qhadprod v. \ v^T\cdot V \cdot v. \tag*{$\qed$}
\end{equation*}
\end{example}

Clearly, the inclusion of this new operator extends the expressive power to \langsum. For example,  $\sem{e_{\mathsf{dp}}}{\I}$ can be an exponentially large number in the dimension $n$ of the input.
By contrast, one can easily show that all expressions in \langsum can only return numbers polynomial in  $n$. That is, \langprod is more expressive than \langsum and $\mathsf{RA}_{K}^+$. 

To measure the expressive power of \langprod, we use weighted logics~\cite{DrosteG05} (WL) as a yardstick. Weighted logics extend monadic second-order logic from the boolean semiring to any semiring~$K$. Furthermore, it has been used extensively to characterize the expressive power of weighted automata in terms of logic~\cite{droste2009handbook}. We use here the first-order subfragment of weighted logics to suit our purpose and, moreover, we extend its semantics over weighted structures (similar as in~\cite{GradelV17}).

A relational vocabulary $\Gamma$ is a finite collection of relation symbols such that each $R \in \Gamma$ has an associated arity, denoted by $\arity(R)$.
A $K$-weighted structure over $\Gamma$ (or just structure) is a pair $\cA = (A, \{R^\cA\}_{R \in \Gamma})$ such that $A$ is a non-empty finite set (i.e. the domain) and, for each $R \in \Gamma$, $R^\cA: A^{\arity(R)} \rightarrow K$ is a function that associates to each tuple in $A^{\arity(R)}$ a weight in $K$.

Let $X$ be a set of first-order variables. A $K$-weighted logic (WL) formula $\varphi$ over $\Gamma$ is defined by the following syntax:
$$
\begin{array}{rcl}
\varphi & := & x = y \ \mid \ R(\bar{x}) \ \mid \ \varphi \ksum \varphi \ \mid \ \varphi \kprod \varphi \ \mid \ \Sigma x. \varphi \ \mid \ \Pi x. \varphi
\end{array}
$$ 
where $x, y \in X$, $R \in \Gamma$, and $\bar{x} = x_1, \ldots, x_k$ is a sequence of variables in $X$ such that $k=\arity(R)$. As usual, we say that $x$ is a free variable of $\varphi$, if $x$ is not below $\Sigma x$ or $\Pi x$ quantifiers (e.g. $x$ is free in $\Sigma y. R(x,y)$ but $y$ is not). 
Given that $K$ is fixed, from now on we talk about structures and formulas without mentioning $K$ explicitly.  

An assignment $\sigma$ over a structure $\cA = (A, \{R^\cA\}_{R \in \Gamma})$ is a function $\sigma: X \rightarrow A$. Given $x \in X$ and $a \in A$, we denote by $\sigma[x \mapsto a]$ a new assignment such that $\sigma[x \mapsto a](y) = a$ whenever $x = y$ and $\sigma[x \mapsto a](y) = \sigma(y)$ otherwise. For $\bar{x} = x_1, \ldots, x_k$,  we write $\sigma(\bar{x})$ to say $\sigma(x_1),\ldots, \sigma(x_k)$. Given a structure $\cA = (A, \{R^\cA\}_{R \in \Gamma})$ and an assignment $\sigma$, we define the semantics $\ssem{\varphi}{\cA}(\sigma)$ of $\varphi$:
$$
\begin{array}{ll}
\text{if $\varphi := x = y$, then} & \ssem{\varphi}{\cA}(\sigma) = 
\left\{
\begin{array}{ll}
\kone & \text{if $\sigma(x) = \sigma(y)$} \\
\kzero & \text{otherwise}
\end{array}
\right. \\
\text{if $\varphi := R(\bar{x})$, then} & \ssem{\varphi}{\cA}(\sigma) = R^\cA(\sigma(\bar{x})) \\
\text{if $\varphi := \varphi_1 \ksum \varphi_2$, then} & \ssem{\varphi}{\cA}(\sigma) = \ssem{\varphi_1}{\cA}(\sigma) \ksum \ssem{\varphi_2}{\cA}(\sigma)  \\
\text{if $\varphi := \varphi_1 \kprod \varphi_2$, then} & \ssem{\varphi}{\cA}(\sigma) = \ssem{\varphi_1}{\cA}(\sigma) \kprod \ssem{\varphi_2}{\cA}(\sigma)  \\
\text{if $\varphi := \Sigma x. \, \varphi'$, then} & \ssem{\varphi}{\cA}(\sigma) =  \bigksum_{a \in A} \ssem{\varphi'}{\cA}(\sigma[x \mapsto a]) \\
\text{if $\varphi := \Pi x. \, \varphi'$, then} & \ssem{\varphi}{\cA}(\sigma) =  \bigkprod_{a \in A} \ssem{\varphi'}{\cA}(\sigma[x \mapsto a])
\end{array}
$$
When $\varphi$ contains no free variables, we omit $\sigma$ and write $\ssem{\varphi}{\cA}$ instead of $\ssem{\varphi}{\cA}(\sigma)$.

For comparing the expressive power of \langprod with WL, we have to show how to encode \lang\ instances into structures and vice versa. For this, we make two assumptions to put both languages at the same level: (1) we restrict structures to relation symbols of arity at most two and (2) we restrict instances to square matrices. The first assumption is for the same reasons as when comparing \langsum with $\mathsf{RA}_K^+$, and the second assumption is to have a crisp translation between both languages. Indeed, understanding the relation of \langprod with WL for non-square matrices is slightly more complicated and we leave this for future work. 

Let $\Sch=(\Mnam,\size)$ be a schema of square matrices, that is, there exists an $\alpha$ such that $\size(V) \in \{1, \alpha\} \times \{1,\alpha\}$ for every $V \in \Mnam$.
Define the relational vocabulary $\text{WL}(\Sch) = \{R_V \mid V \in \Mnam\}$ such that $\arity(R_V) = 2$ if $\size(V) = (\alpha, \alpha)$, $\arity(R_V) = 1$ if $\size(V) \in \{(\alpha,1), (1,\alpha)\}$, and $\arity(R_V) = 0$ otherwise.
Then given a matrix instance $\I = (\dom,\conc)$ over $\Sch$ define the structure $\text{WL}(\I) = (\{1, \ldots, n\}, \{R_V^{\I}\} )$ such that $\dom(\alpha) = n$ and $R_V^{\I}(i, j) = \conc(V)_{i,j}$ if $\size(V) = (\alpha, \alpha)$, $R_V^{\I}(i) = \conc(V)_{i}$ if $\size(V) \in \{(\alpha,1), (1,\alpha)\}$, and $R_V^{\I} = \conc(V)$ if $\size(V) = (1,1)$.

To encode weighted structures into matrices and vectors, the story is similar as for $\mathsf{RA}_K^+$. Let $\Gamma$ be a relational vocabulary where $\arity(R) \leq 2$. 
Define $\text{Mat}(\Gamma) = (\Mnam_\Gamma,\size_\Gamma)$ such that $\Mnam_\Gamma = \{ V_{R} \mid R \in \Gamma\}$ and $\size_\Gamma(V_{R})$ is equal to $(\alpha, \alpha), (\alpha, 1)$, or $(1,1)$ if $\arity(R)=2$, $\arity(R)=1$, or $\arity(R)=0$, respectively, for some $\alpha \in \DD$. Similarly, let $\cA = (A, \{R^{\cA}\}_{R \in \Gamma})$ be a structure with $A = \{a_1, \ldots, a_n\}$, ordered arbitrarily.
Then we define the matrix instance $\text{Mat}(\cA) = (\dom,\conc)$ such that $\dom(\alpha) = n$, $\conc(V_{R})_{i,j} = R^{\cA}(a_i, a_j)$ if $\arity(R)=2$, $\conc(V_{R})_{i} = R^{\cA}(a_i)$ if $\arity(R)=1$, and $\conc(V_{R}) = R^{\cA}$ otherwise.

\cristian{TODO: divide this into two proposition.}

Let $\Sch$ be a \lang\ schema of square matrices and $\Gamma$ a relational vocabulary of relational symbols of arity at most $2$. We can then show the equivalence of \langprod and WL as follows. 
\begin{proposition} \label{prop:wl}
Weighted logics over $\Gamma$ and \langprod over $\Sch$ have the same expressive power. More specifically,
\begin{itemize}
	\item for each \langprod expression $e$ over $\Sch$ such that $\Sch(e)=(1,1)$, there exists a WL-formula $\Phi(e)$ over $\text{WL}(\Sch)$ such that for every instance $\I$ of~$\Sch$, 
	$
	\sem{e}{\I} = \ssem{\Phi(e)}{\text{WL}(\I)}
	$.
	\item for each WL-formula $\varphi$ over $\Gamma$ without free variables, there exists a \langprod expression $\Psi(\varphi)$ such that for any structure $\cA$ over~$\text{Mat}(\Gamma)$,
	$
	\ssem{\varphi}{\cA}=\sem{\Psi(\varphi)}{\text{Mat}(\cA)}
	$.
\end{itemize}	
\end{proposition}

\input{sections/proofs/sp-wl.tex}

\subsection{Matrix multiplication as a quantifier}\label{subsec:langlinear}
In a similar way, we can consider a fragment in which sum and the usual product of matrices can be used
in for-loops. Formally, for an expression $e$ we define the operator:
$$
\sprod v.\,  e=\ffor {v}{X = e_{\mathsf{Id}}}{X\cdot e}.
$$
where $e_{\mathsf{Id}}$ is the identity matrix and $e=e(v)$. We call \langmprod the subfragment of \langfor that consists of \langsum extended with $\sprod v$. It is readily verified that  $\qhadprod v$ can be expressed in terms of $\sprod v$.
Furthermore, by contrast to the Hadamard product, matrix multiplication is a non-commutative operator. As a consequence, one can formulate expressions that are not invariant under the order in which the canonical vectors
are processed.

\begin{proposition}
	Every expression in \langprod can be defined in \langmprod. Moreover, there exists an expression that uses the $\sprod v$ quantifier that cannot be defined in \langprod.
\end{proposition}

\input{sections/proofs/prodlang-and-order.tex}

What is interesting is that \langsum extended with $\sprod v$ suffices to compute the transitive closure,
 provided that we allow for the $f_{>0}$ function. Indeed, one can use the expression $e_{\mathsf{TC}}(V):=f_{>0}\bigl(\sprod v.\, (e_{\mathsf{Id}}+V)\bigr)$ for this purpose because
 $\sem{e_{\mathsf{TC}}}{\I}=f_{>0}\bigl((I+A)^n\bigr)$ when $\I$ assigns an $n\times n$ adjacency matrix $A$ to $V$, and non-zero entries in $(I+A)^n$ coincide with non-zero entries in  the transitive closure of $A$.
Furthermore, if we extend this fragment with access to the matrix $S_{<}$, defining the (strict) order on canonical vectors, then Csanky's matrix inversion algorithm becomes expressible (if $f_/$ is allowed). We do not know if the inversion and determinant algorithms are expressible in \langprod.
 We leave the study of \langmprod and, in particular, the relationship to full \langfor, for future work.
 
Finally, in Figure~\ref{thefigure} we show a diagram of all the fragments of \langfor over square matrices introduced in this section and their corresponding equivalent formalisms.

\begin{figure}
	
	\begin{tikzpicture}[->,>=stealth, semithick, auto, initial text= {}, initial distance= {3mm}, accepting distance= {4mm}, node distance=0.5cm, semithick]
	
	\node [rectangle, draw=black, fill=white, minimum height=4mm, minimum width=2cm, rounded corners] (ML) at (0, 0) {$\texttt{ML}$};
	
	\node [inner sep=0mm] (SML) at ($(ML) + (0,0.65)$) {$\texttt{sum}\text{-}\texttt{ML}\equiv \texttt{RA}^+_K$};
	
	\node [circle, radius=4mm,draw=black, fill=black, inner sep=0mm] (pCLIQUE) at ($(ML) + (1.4,0.1)$) {};
	\node [right of=pCLIQUE,inner sep=0mm, node distance=0.65cm] (CLIQUE) {$\textsc{4Clique}$};
	
	\begin{pgfonlayer}{background}
	\node (SMLc)[draw=black, inner sep=1.5mm, rounded corners,fit=(SML)(ML)(CLIQUE)] {};
	\end{pgfonlayer}
	
	
	\node [inner sep=0mm] (sp-ML) at ($(SML) + (0,0.7)$) {$\texttt{sp}\text{-}\texttt{ML} \equiv \texttt{WL}$};
	
	\node [circle, radius=4mm,draw=black, fill=black, inner sep=0mm] (pDP) at ($(pCLIQUE) + (1.7,0.2)$) {};
	\node [right of=pDP,inner sep=0.5mm, node distance=0.3cm] (DP) {$\textsc{DP}$};
	
	
	\begin{pgfonlayer}{background}
	\node (sp-MLc) [draw=black, inner sep=1.5mm, rounded corners,fit=(SMLc)(ML)(sp-ML)(DP)] {};
	\end{pgfonlayer}
	
	
	\node [inner sep=0mm] (PML)  at ($(sp-ML) + (0,0.7)$) {$\texttt{prod}\text{-}\texttt{ML} + S_{<}$};
	
	\node [circle, radius=4mm,draw=black, fill=black, inner sep=0mm] (pINV) at ($(pDP) + (1,1)$) {};
	\node [right of=pINV,inner sep=0mm, node distance=0.35cm] (INV) {$\textsc{Inv}$};
	
	\node [circle, radius=4mm,draw=black, fill=black, inner sep=0mm] (pDET) at ($(pDP) + (1,0.2)$) {};
	\node [right of=pDET,inner sep=0mm, node distance=0.35cm] (DET) {$\textsc{Det}$};
	
	
	\begin{pgfonlayer}{background}
	\node (PMLc) [draw=black, inner sep=1.5mm, rounded corners,fit=(SMLc)(ML)(sp-MLc)(PML)(INV)(DET)] {};
	\end{pgfonlayer}
	
	
	\node [inner sep=0mm] (forML) at ($(PML) + (1,0.7)$) {$\texttt{for}\text{-}\texttt{ML} \equiv \text{Arithmetic Circuits}$};
	
	\node [circle, radius=4mm,draw=black, fill=black, inner sep=0mm] (pPALU) at ($(pINV) + (1, 0.5)$) {};
	\node [right of=pPALU,inner sep=0mm, node distance=0.5cm] (PALU) {$\textsc{PLU}$};
	
	\begin{pgfonlayer}{background}
	\node [draw=black, inner sep=1.5mm, rounded corners,fit=(SMLc)(ML)(sp-MLc)(PMLc)(forML)(PALU)] {};
	\end{pgfonlayer}
	
	\end{tikzpicture}
	
	\caption{Fragments of \langfor over square matrices and their equivalences. The functions \textsc{4Clique}, \textsc{DP} (diagonal product), \textsc{Inv}, \textsc{Det}, and \textsc{PLU} decomposition are placed in their fragments.} \label{thefigure}
\end{figure}
