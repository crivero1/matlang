% !TeX spellcheck = en_US
%!TEX root = ../../main.tex

\begin{proof}
%
%We start from a matrix schema $\Sch=(\Mnam,\size)$, where $\Mnam\subset \Mvar$ is a finite set of matrix variables, 
%and $\size: \Mvar \mapsto \DD\times \DD$ is a function that maps each matrix variable to a pair of size symbols. 
%On the relational side we have for each size symbol $\alpha\in\DD\setminus\{1\}$, attributes $\alpha$, $\row_\alpha$, 
%and $\col_\alpha$ in $\att$. We also reserve some special attributes $\gamma_1,\gamma_2,\ldots$ whose role will become clear shortly.
%For each $V\in\Mnam$ and $\alpha \in \DD$ we denote
%by $R_V$ and $R_\alpha$ its corresponding relation name, respectively. 
%
%Then, given $\Sch$ we define the relational 
%schema $\text{Rel}(\Sch)$ such that $\fdom(\text{Rel}(\Sch)) =  \{R_\alpha \mid \alpha\in\DD\} \cup \{R_V \mid V \in \Mnam\}$
%where $\text{Rel}(\Sch)(R_\alpha) = \{\alpha\}$ and for all $V\in\Mnam$:
%\[
%\text{Rel}(\Sch)(R_V) = \begin{cases}
%\lbrace\row_\alpha,\col_\beta \rbrace & \text{ if $ \size(V)=(\alpha,\beta)$} \\
%\lbrace\row_\alpha \rbrace & \text{ if $ \size(V)=(\alpha,1)$} \\
%\lbrace\col_\beta \rbrace  &
%\text{ if $ \size(V)=(1,\beta)$} \\
%\lbrace\rbrace & \text{ if $\size(V)=(1,1)$}.
%\end{cases}
%\]
%
%Next, for a matrix instance $\I = (\dom,\conc)$ over $\Sch$,
%let $V\in\Mnam$ with $\size(V)=(\alpha,\beta)$ and let $\conc(V)$ be its corresponding $K$-matrix of dimension $\dom(\alpha)\times \dom(\beta)$.
%The $K$-instance in $\mathsf{RA}_{K}^+$ according to $\I$ is $\text{Rel}(\I)$ with data domain $\ddom = \mathbb{N} \setminus \{0\}$. For each $V\in\Mnam$ we define 
%$R_V^{\text{Rel}(\I)}(t):=\conc(V)_{ij}$ whenever $t(\row_\alpha) = i \leq \dom(\alpha)$ and $t(\col_\beta) = j \leq \dom(\beta)$, and $\kzero$ otherwise. 
%Also, for each $\alpha \in \DD$ we define $R_\alpha^{\text{Rel}(\I)}(t):=\kone$ whenever $t(\alpha) \leq \dom(\alpha)$, and $\kzero$ otherwise.
%If $\size(V)=(\alpha,1)$ then $R_V^{\text{Rel}(\I)}(t):=\conc(V)_{i1}$ whenever $t(\row_\alpha) = i \leq \dom(\alpha)$ and $\kzero$ otherwise.
%Similarly, if $\size(V)=(1,\beta)$ then $R_V^{\text{Rel}(\I)}(t):=\conc(V)_{1j}$ whenever $t(\col_\beta) = j \leq \dom(\beta)$ and $\kzero$ otherwise.
%If $\size(V)=(1,1)$ then $R_V^{\text{Rel}(\I)}(()):=\conc(V)_{11}$.
%
%
The proof of the proposition is by induction on the structure of a \langsum expression~$e$. In the following we need to distinguish between matrix variables $v$
that occur in $e$ as part of a sub-expression $\ssum v. (\cdot)$, i.e. those variables that will be used to iterate over by means of canonical vectors, and those that are not. To make this distinction clear, we use $v_1,v_2,\ldots$ for those ``iterator'' variables, and capital $V$ for the matrix variables occurring in $e$. For simplicity, we assume that each occurrence of $\ssum$ has its own iterator variable associated with it. 

To state the induction hypothesis we define a notion of \textit{free} (iterator) \textit{variables}, as follows:
$\mathsf{free}(V):=\emptyset$, $\mathsf{free}(v):=\{v\}$, $\mathsf{free}(e^T):=\mathsf{free}(e)$, $\mathsf{free}(e_1+e_2):=\mathsf{free}(e_1)\cup \mathsf{free}(e_2)$, $\mathsf{free}(e_1\cdot e_2):=\mathsf{free}(e_1)\cup \mathsf{free}(e_2)$,
 $\mathsf{free}(f_\odot(e_1,\ldots,e_k)):=\mathsf{free}(e_1)\cup\cdots \cup \mathsf{free}(e_k)$, and $\mathsf{free}(\ssum v. e_1)=\mathsf{free}(e_1)\setminus\{v\}$. We will explicitly denote the free variables in an expression $e$ by writing $e(v_1,\ldots,v_k)$.
On the relational side, we will also use attribute $\gamma_p$ corresponding to iterator variable $v_p$.

We now use the following induction hypotheses:
\begin{itemize}
	\item If $e(v_1,\ldots,v_k)$ is of type $(\alpha,\beta)$ then there exists an
	\rak expression $\arae$ such that $\text{Rel}(\Sch)(\arae(e))=\{\row_\alpha,\col_\beta,\gamma_1,\ldots,\gamma_k\}$
	and such that 
	$$
	\ssem{\arae(e)}{\text{Rel}(\I)}(t)=\sem{e}{\I[v_1\gets b_{i_1},\ldots,v_k\gets b_{i_k}]}_{i,j}
	$$
	for tuple $t(\mathrm{row}_\alpha)=i$, $t(\mathrm{col}_\beta)=j$ and $t(\gamma_s)=i_s$ for $s=1,\ldots, k$.
	\item If $e(v_1,\ldots,v_k)$ is of type $(\alpha,1)$ then there exists an
	\rak expression $\arae$ such that $\text{Rel}(\Sch)(\arae(e))=\{\row_\alpha,\gamma_1,\ldots,\gamma_k\}$
	and such that 
	$$
	\ssem{\arae(e)}{\text{Rel}(\I)}(t)=\sem{e}{\I[v_1\gets b_{i_1},\ldots,v_k\gets b_{i_k}]}_{i,1}
	$$
	for tuple $t(\mathrm{row}_\alpha)=i$,  and $t(\gamma_s)=i_s$ for $s=1,\ldots, k$.
	And similarly for when $e$ is type $(1,\beta)$.
	\item If $e(v_1,\ldots,v_k)$ is of type $(1,1)$ then there exists an
	\rak expression $\arae$ such that $\text{Rel}(\Sch)(\arae(e))=\{\gamma_1,\ldots,\gamma_k\}$
	and such that 
	$$
	\ssem{\arae(e)}{\text{Rel}(\I)}(t)=\sem{e}{\I[v_1\gets b_{i_1},\ldots,v_k\gets b_{i_k}]}_{1,1}
	$$
	for tuple $t(\gamma_s)=i_s$ for $s=1,\ldots, k$.
\end{itemize}
Clearly, this suffices to show the proposition since we consider expressions $e$ for which $\mathsf{free}(e)=\emptyset$, in which case the above statements reduce to the one given in the proposition.


The proof is by induction on the structure of \langsum expressions. In line with the simplifications in Section~\ref{sec:queries:simp}, it suffices to consider pointwise function application with $f_\odot$ instead of scalar multiplication. Indeed, we note that we can express the one-vector operator in \langsum, so scalar multiplication can be expressed using $f_\odot$ in \langsum.

Let $e$ be a \langsum expression.
\begin{itemize}
  \item If $e=V$ then $\arae (e):=R_V$.
  \item If $e=v_p$ then $\arae (e):=\sigma_{\lbrace \row_\alpha,\gamma_p\rbrace}\bigl(\rho_{\row_\alpha\to \alpha}(R_\alpha)\bowtie \rho_{\gamma_p\to \alpha}(R_\alpha)\bigr)$ when  $v_p$ is of type $(\alpha,1)$. It is here that we introduce the attribute $\gamma_p$ associated with iterator variable $v_p$.
 We note that 
$$ \ssem{\arae(v_p)}{\text{Rel}(\I)}(t)=\sem{v_p}{\I[v_p\gets b_{j}]}_{i,1}=(b_j)_{i,1}
$$
for $t(\mathrm{row}_\alpha)=i$ and $t[\gamma_p]=j$. Indeed, $(b_j)_{i,1}=\kone$ if $j=i$
and this holds when $t(\mathrm{row}_\alpha)=t[\gamma_p]=j$, and $(b_j)_{i,1}=\kzero$ if $j\neq i$
and this also holds when $t(\mathrm{row}_\alpha)\neq t[\gamma_p]=j$.

  \item If $e(v_1,\ldots,v_k)=(e_1(v_1,\ldots,v_k))^T$ with $\Sch (e_1)=(\alpha, \beta)$ then \[
\arae(e) :=
\begin{cases}
\rho_{\mathrm{row}_\alpha \to \mathrm{col}_\alpha,\mathrm{col}_\beta \to \mathrm{row}_\beta}\bigl(\arae(e_1)\bigr) & \text{if } \alpha \neq 1 \neq \beta; \cr
\rho_{\mathrm{row}_\alpha \to \mathrm{col}_\alpha}\bigl(\arae(e_1)\bigr) & \text{if } \alpha \neq 1 = \beta; \cr
\rho_{\mathrm{col}_\beta \to \mathrm{row}_\beta}\bigl(\arae(e_1)\bigr) & \text{if } \alpha = 1 \neq \beta; \cr
\arae(e_1) & \text{if } \alpha = 1 = \beta.
\end{cases}
\]
\item If $e=e_1(v_1,\ldots,v_k)+e_2(v_1,\ldots,v_k)$ with $\Sch (e_1)=\Sch (e_2)=(\alpha, \beta)$ then $\arae (e):=\arae (e_1)\cup \arae (e_2)$. We assume here that $e_1$ and $e_2$ have the same free variables. This is without loss of generality. Indeed, as an example, suppose that we have $e_1(v_1,v_2)$
and $e_2(v_2,v_3)$. Then, we can replace $e_1$ by  $e_1(v_1,v_2,v_3)=(v_3^T\cdot v_3)\times e_1(v_1,v_2)$
and similarly, $e_2$ by $e_2(v_1,v_2,v_3)=(v_1^T\cdot v_1)\times e_2(v_2,v_3)$, where in addition we replace scalar multiplication with its simulation using $f_{\odot}$ and the ones vector, as mentioned earlier. 

  \item If $e=f_\odot(e_1,\ldots, e_k)$ with $\Sch(e_i)=\Sch(e_j)$ for all $i,j\in[1,k]$, then $\arae(e):=\arae(e_1)\Join \cdots \Join\arae(e_k)$.

  \item If $e=e_1\cdot e_2$ with $\Sch (e_1)=(\alpha, \gamma)$ and $\Sch (e_2)=(\gamma, \beta)$, we have two cases. If $\gamma = 1$ then $\arae (e):=\arae (e_1)\Join \arae (e_2)$.
If $\gamma\neq 1$ then
$$
\arae (e) := \pi_{\lbrace \row_{\alpha},\col_{\beta}, \gamma_1,\ldots,\gamma_k \rbrace}\left(\rho_{C\to \col_\gamma}(\arae (e_1))\Join \rho_{C\to \row_\gamma}(\arae (e_2)) \right),
$$
when $\text{Rel}(\Sch)(\arae(e_1))=\{\row_\alpha,\col_\gamma,\gamma_1',\ldots,\gamma_{\ell}'\}$,
$\text{Rel}(\Sch)(\arae(e_2))=\{\row_\gamma,\col_\beta,\gamma_1'',\ldots,\gamma_{m}''\}$ and $\{\gamma_1,\ldots,\gamma_k\}:=\{\gamma_1',\ldots,\gamma_\ell',\gamma_1'',\ldots,\gamma_m''\}$.

  \item If $e(v_1,\ldots,v_{p-1},v_{p+1},\ldots,v_k)=\ssum v_p. \, e_1(v_1,\ldots,v_k)$ where $\Sch(e_1)=(\alpha,\beta)$ and $\Sch(v_p)=(\gamma_p,1)$. Then we define
  $$
  \arae (e):=\pi_{\text{Rel}(\Sch)(\arae(e_1))\setminus\{\gamma_p\}} \arae (e_1).
  $$
 Indeed, by induction we know that 
 $$
\ssem{\arae(e_1)}{\text{Rel}(\I)}(t)=\sem{e}{\I[v_1\gets b_{i_1},\ldots,v_k\gets b_{i_k}]}_{i,j}
$$
for tuple $t(\mathrm{row}_\alpha)=i$, $t(\mathrm{col}_\beta)=j$ and $t(\gamma_s)=i_s$ for $s=1,\ldots, k$.
Hence, for $t(\mathrm{row}_\alpha)=i$, $t(\mathrm{col}_\beta)=j$ and $t(\gamma_s)=i_s$ for $s=1,\ldots, k$ and $s\neq p$,
$$
\ssem{\arae(e)}{\text{Rel}(\I)}(t):=\bigksum_{i_p=1,\ldots,\dom(\gamma)} \sem{e_1}{\I[v_1\gets b_{i_1},\ldots,v_{p}\gets b_{i_p},\ldots v_k\gets b_{i_k}]}_{i,j},$$
which precisely corresponds to 
$$
\sem{\ssum v_p. e_1(v_1,\ldots,v_k)}{\I[v_1\gets b_{i_1},\ldots,v_{p-1}\gets b_{i_{p-1}},v_{p+1}\gets b_{i_{p+1}},\ldots,v_k\gets b_{i_k}]}_{i,j}.
$$

\end{itemize}
Expressions of type $(\alpha,1)$, $(1,\beta)$ or $(1,1)$  can be dealt with in a similar way.
\end{proof}
