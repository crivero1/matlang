% !TeX spellcheck = en_US
%!TEX root = ../../main.tex

To show the previous result we first show that it holds when considering non-singular lower or upper triangular matrices.
\begin{lemma}\label{prop:upperlowerinverse}
There are $\langforf{f_/}$ expressions $e_{\mathsf{upperDiagInv}}(V)$ and $e_{\mathsf{lowerDiagInv}}(V)$
such that $\sem{e_{\mathsf{upperDiagInv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
to an invertible upper triangular matrix $A$ and, similarly, $\sem{e_{\mathsf{lowerDiagInv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
to an invertible lower triangular matrix $A$.
\end{lemma}

\begin{proof} We start by considering the expression:
    $$
    e_{\mathsf{ps}}(V):= e_{\mathsf{Id}} + \ssum v.\sprod w. \left[ \mathsf{succ}(w,v)\times V + (1 - \mathsf{succ}(w,v))\times e_{\mathsf{Id}} \right].
    $$
    Here, $e_{\mathsf{ps}}(A)$ results in $I+A+A^2+\cdots + A^n$ for any matrix $A$. In the expression, the outer loop defines which power we compute. 
    That is, when $v$ is the $i$th canonical vector, we compute $A^i$.
    Computing $A^i$ is achieved via the inner product loop, which uses $\mathsf{succ}(w,v)$ to 
    determine whether $w$ comes before or is $v$ in the ordering of canonical vectors.
    When this is the case, we multiply the current result by $A$, and when $w$ is greater 
    than $v$, we use the identity as not to affect the already computed result. We add the identity at the end.

    Now, let $A$ be an $n\times n$ matrix that is upper triangular and let $D_A$ be the matrix consisting of the diagonal elements of $A$, i.e.,
    \[
    D_A = \begin{bmatrix}
        a_{11} & \cdots & \cdots &  0 \\
        0 & a_{22} & \cdots &  0 \\
        0 & \ddots & \vdots & \vdots \\
        \vdots & \cdots& \cdots & a_{nn}
    \end{bmatrix}.
    \]
    We can compute $D_A$ by the expression:
    $$
    e_{\mathsf{getDiag}}(V) := \ssum v. (v^T\cdot V\cdot v) \times v\cdot v^T.
    $$
    Let $T=A-D_A$, then
    $$
    A^{-1}=\left[ D_A+T \right]^{-1}= \left[ D_A\left( I+D_A^{-1}T\right) \right]^{-1} = \left( I+D_A^{-1}T\right)^{-1}D_A^{-1}.
    $$
    We now observe that $D_{A}^{-1}$ simply consists of the inverses of the elements on the diagonal. This can be expressed, as follows:
    $$
    e_{\mathsf{diagInverse}}(V):=\ssum v. f_{/}(1,v^T\cdot V\cdot v)\times v\cdot v^T
    $$
    where $f_{/}$ is the division function. In the last equality we take advantage of the fact that the diagonals of $A$ and $D_A$ are the same.

    We now focus on the computation of $\left( I+D_A^{-1}\cdot T\right)^{-1}$. First, by construction, $D_A^{-1}\cdot T$ is strictly upper triangular and thus nilpotent, 
    such that $\left( D_A^{-1}\cdot T\right)^n=0$, where $n$ is the dimension of $A$.
    Recall the following algebraic identity 
    $$(1+x)\left( \sum_{i=0}^{m}(-x)^i \right)=1-(-x)^{m+1}.$$
    By choosing $m=n-1$ and applying it to $x=D_A^{-1}\cdot T$, we have
    $$
    \left(I+D_A^{-1}\cdot T \right)\left( \sum_{i=0}^{n-1}(-D_A^{-1}\cdot T)^i \right)=I- \left( -D_A^{-1}\cdot T\right)^n =I.
    $$
    Hence,
    $$
    \left(I+D_A^{-1}\cdot T \right)^{-1}=\sum_{i=0}^{n-1}(-D_A^{-1}\cdot T)^i=\sum_{i=0}^{n}(-D_A^{-1}\cdot T)^i.
    $$
    We now observe that
    $$
    e_{\mathsf{ps}}(-1\times D_A^{-1}\cdot T)=\sum_{i=0}^{n}(-D_A^{-1}\cdot T)^i=\left(I+D_A^{-1}\cdot T \right)^{-1},
    $$
    and thus 
    $$
    A^{-1}= e_{\mathsf{ps}}\left(-1\times \left[e_{\mathsf{diagInverse}}(A)(A-e_{\mathsf{getDiag}}(A))\right] \right)e_{\mathsf{diagInverse}}(A).
    $$
    Seeing this as an expression:
    $$
    e_{\mathsf{upperDiagInv}}(V):= e_{\mathsf{ps}}\left(-1\times \left[e_{\mathsf{diagInverse}}(V)(V-e_{\mathsf{getDiag}}(V))\right] \right)e_{\mathsf{diagInverse}}(V),
    $$
    we see that  when interpreting $V$ as an  upper triangular invertible matrix, 
    $e_{\mathsf{upperDiagInv}}(A)$ evaluates to $A^{-1}$.


    To deal with invertible lower triangular matrices $A$, we observe that  $\left(A^{-1}\right)^T=\left(A^T\right)^{-1}$ and $A^T$ is upper triangular.
    Hence, it suffices to define
    $$
    e_{\mathsf{lowerDiagInv}}(V):= e_{\mathsf{upperDiagInv}}(V^T)^T.
    $$
    This concludes the proof of the lemma.
\end{proof}

We are now ready to prove Proposition \ref{prop:inverse}.

\begin{proof}
    Let $A$ be an $n \times n$ matrix. As mentioned in the main body of the paper, we will implement Csanky's algorithm. Let $p_A(x):=\mathsf{det}(xI-A)$ denote  characteristic polynomial of $A$.  We write $p_A(x)=1 + \sum_{i=1}^n c_ix^i$ and let  $S_i:=\frac{1}{i+1}\mathsf{tr}(A^i)$ 
    \cristian{In the body above, $S_i$ is defined as $S_i:=\mathsf{tr}(A^i)$. Which is the correct one? $S_i:=\frac{1}{i+1}\mathsf{tr}(A^i)$ or $S_i:=\mathsf{tr}(A^i)$?}
    with $\mathsf{tr}(\cdot)$ the trace operator which sums up the diagonal elements of a matrix.
    Then, the coefficients $c_1,\ldots,c_n$ are known to satisfy\footnote{We use a slightly different, but equivalent, system of equations than the one mentioned in the paper.} 
    $$
    \underbrace{\left(\begin{matrix}
    1 & 0 & 0 & \cdots & 0 & 0\\
    S_1 & 1 & 0 & \cdots  &0 & 0\\
    S_2 & S_1 & 1 & \cdots  &0 & 0\\
    \vdots & \vdots & \vdots & \vdots & \vdots & 0\\
    S_{n-1} & S_{n-2} & S_{n-3} & \cdots & S_1 & 1\\
    \end{matrix}\right)}_{S}\cdot
    \underbrace{\left(\begin{matrix}
    c_1\\
    c_2\\
    c_3\\
    \vdots\\
    c_n\\
    \end{matrix}\right)}_{\bar c}=\underbrace{\left(\begin{matrix}
    S_1\\
    S_2\\
    S_3\\
    \vdots\\
    S_n\\
    \end{matrix}\right)}_{\bar b}
    $$
    and furthermore, $c_n=(-1)^n\mathsf{det}(A)$ and if $c_{n}\neq 0$, then
    $$
    A^{-1}=\frac{1}{c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i},
    $$
    with $c_0=1$. It is now easy to see that we can compute the $S_i's$ in $\langfor$. Indeed, for
    $i=1,\ldots,n$ we can consider
    $$
    e_{\mathsf{powTr}}(V,v):=\ssum w. w^T\cdot\left(e_{\mathsf{pow}}(V,v)\cdot V\right)\cdot w
    $$
    with 
    $$
    e_{\mathsf{pow}}(V,v):= \sprod w. (\mathsf{succ}(w,v)\times V+(1-\mathsf{succ}(w,v))\times e_{\mathsf{Id}}).
    $$
    We have that $e_{\mathsf{pow}}(A,b_j)=A^{j}$ and thus $e_{\mathsf{powTr}}(A,b_j)=\mathsf{tr}(A^{j})$. Define:
    $$
    e_{S}(V,v):=f_{/}(1, 1 + \ssum w. \mathsf{succ}(w,v))\times e_{\mathsf{powTr}}(V,v).
    $$
    Here $e_{S}(A,b_i)=S_i$. Note that $i+1$ is computed summing up to the dimension indicated by $v$, and adding 1.
    We can now easily construct the vector $\bar b$ used in the system of equations by means of the expression:
    $$
    e_{\bar b}(V):=\ssum w.e_{S}(V,w)\times w.
    $$
    We next construct the matrix $S$. We need to be able to \textit{shift} a vector $a$ in $k$ positions, i.e.,
    such that $(a_1,\ldots,a_n)\mapsto (0,\ldots,a_1,\ldots,a_{n-k})$. We use $e_{\mathsf{getNextMatrix}}$ 
    defined in Section~\ref{sec:formatlang:design}, i.e., we define:
    $$
    e_{\mathsf{shift}}(a,v):=\ssum w.(w^T\cdot a)\times(e_{\mathsf{getNextMatrix}}(v)\cdot w)
    $$
    performs the desired shift when $u$ is assigned a vector $a$ and $v$ is $b_k$. 
    The matrix $S$ is now obtained as follows:
    $$
    S(V):= e_{\mathsf{Id}} + \ssum v. e_{\mathsf{shift}}(e_{\bar b}(V), v)\cdot v^T
    $$
    We now observe that $S$ is lower triangular with nonzero diagonal entries. So,
    Lemma~\ref{prop:upperlowerinverse} tells us that we can invert it, i.e.,
    $e_{\mathsf{lowerDiagInv}}(S)=S^{-1}$. As a consequence,
    $$
    e_{\bar c}(V):=e_{\mathsf{lowerDiagInv}}(S(V))\cdot e_{\bar b}(V).
    $$
    outputs $\bar c$ when $V$ is interpreted as matrix $A$. Observe that we only use the division operator. We now have all coefficients of the characteristic polynomial of $A$.


    We can now define
    $$
    e_{\mathsf{det}}(V):=\left( \left(\left( \sprod w. (-1)\times e_{\ones}(V)\right)^T\cdot e_{\mathsf{max}}\right) \times e_{\bar c}(V) \right)^T\cdot e_{\mathsf{max}},
    $$
    an expression that, when $V$ is interpreted as any matrix $A$, outputs $\mathsf{det}(A)$.
    Here, $(\sprod w. (-1)\times e_{\ones}(V))$ is the $n$ dimensional vector with $(-1)^n$ in all of its entries.
    Since $c_n=(-1)^n\mathsf{det}(A)$, we extract $(-1)^n(-1)^n\mathsf{det}(A)=\mathsf{det}(A)$ with $e_{\mathsf{max}}$.

    For the inverse, we have that
    $$
    A^{-1}=\frac{1}{c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i} = \frac{1}{c_n}A^{n-1} + \sum_{i=1}^{n-1}\frac{c_i}{c_n}A^{n-1-i}.
    $$
    We compute $\frac{1}{c_n}A^{n-1}$ as
    $$
    f_{/}(1, e_{\bar c}(A)^T\cdot e_{\mathsf{max}})\times e_{\mathsf{pow}}(A, e_{\mathsf{max}})
    $$
    and $\sum_{i=1}^{n-1}\frac{c_i}{c_n}A^{n-1-i}$ as
    $$
    \ssum v. f_{/}\left( e_{\bar c}(A)^T\cdot v, e_{\bar c}(A)^T\cdot e_{\mathsf{max}} \right)\times e_{\mathsf{invPow}}(A, v),
    $$
    where
    $$
    e_{\mathsf{invPow}}(V, v):= \sprod w. (1-\mathsf{max}(w)) \times \left[ (1 - \mathsf{succ}(w,v))\times V + \mathsf{succ}(w,v)\times e_{\mathsf{Id}} \right] + \mathsf{max}(w)\times e_{\mathsf{Id}}.
    $$
    Here, $e_{\mathsf{invPow}}(A, b_i)=A^{n-1-i}$ and $e_{\mathsf{invPow}}(A, b_n)=I$.
    Note that we always multiply by $e_{\mathsf{Id}}$ in the last step.
    To conclude, we define:
    $$
    e_{\mathsf{inv}}(V):= f_{/}(1, e_{\bar c}(V)^T\cdot e_{\mathsf{max}})\times e_{\mathsf{pow}}(V, e_{\mathsf{max}}) + \left[ \ssum v. f_{/}\left( e_{\bar c}(V)^T\cdot v, e_{\bar c}(V)^T\cdot e_{\mathsf{max}} \right)\times e_{\mathsf{invPow}}(V, v) \right],
    $$
    an expression that, when $V$ is interpreted as any invertible matrix $A$, computes $A^{-1}$.
\end{proof}

As an observation, here we only use operators $\ssum$ and $\sprod$ defined in section \ref{sec:restrict}. 
We also assume access to order.
