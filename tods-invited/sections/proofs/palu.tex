\begin{proof}

    We assume that $f_{/}$ and $f_{>0}$ are in $\mathcal{F}$. Let $A$ be an arbitrary matrix.
    By contrast to when $A$ is LU-factorizable, during the LU-decomposition process we may need row interchange (pivoting) in each step of the iteration. Let us assume that row interchange is needed immediately before 
    step $k$, $1\leq k\leq n$. In other words, we now aim to reduce the $k$-th column of $A_k=T_{k-1}\cdots T_1\cdot A$, 
    or $A_k=A$ if $k=1$, but now $A_k$ has a zero pivot, i.e., $(A_{k})_{kk}=0$. 
    Let $P$ be the matrix that denotes the necessary row interchange. If we know
    $P$, then 
    % We aim to reduce the $k$-th column of $PA_{k}$ since $A_{k}$ has a zero pivot.
    to compute $T_k$ we need to perform $\red{P\cdot X\cdot A}{v}$ in this iteration,
    where $\red{\cdot}$ is the expression in \langfor reducing a column, as defined in the proof of Proposition \ref{prop:gauss}.
    Furthermore, we need to apply the permutation $P$ to the current result, resulting in the 
    expression $\initf{e_{\mathsf{Id}}}{v}{X}{\red{P\cdot X\cdot A}{v}\cdot P\cdot X}$. We now remark that
    $P$ is a permutation matrix of the  form $P = I - u\cdot u^T$ and it denotes an interchange (if multiplied by left) of rows $i$ and $j$ if $u=(b_{i}-b_{j})$. Note that we are performing a row interchange for column $k$ and thus $i=k$ and $j>k-1$. If no interchange is needed, $i=j=k$ and $P=I$.
    Also note that when $k=n$ no interchange takes place. Furthermore, if no suitable $b_j$ can
    be found, this implies that no interchange is required as well and we can move on to next column.

    To find the vector $u$ in $P$, we can, for example, find the first entry $j\geq k$ in column $k$ of $A_k$ that holds a non-zero value. More generally, we can find the first entry in a vector $a$ that holds a non-zero value by using the function $f_{>0}$. Indeed, consider the following expression:

    \begin{align*}
       \nneq{a}{u} := &\texttt{for}\, v,X \texttt{.}\, \left( 1-e_{\ones}(v)^T\cdot X \right) \times f_{>0}\left( ( v^T\cdot a )^2 \right)\times v \\
        &\hspace{4em}+ \mathsf{max}(v)\times\left( 1-e_{\ones}(v)^T\cdot X \right)\times \left( 1 - f_{>0}\left( ( v^T\cdot a )^2 \right) \right) \times u \\
    \end{align*}
where $e_{\ones}(v)$ is the expression for the $\diag(v)$ operator (see Example~\ref{ex:onevec}).
Here, $\nneq{a}{u}$ receives two $n$ dimensional vectors $a$ and $u$ and outputs a 
    canonical vector $b_j$ such that $a_j$ is the first non-zero entry of $a$, or $u$ if such non-zero value does not exist in $a$. We check for $f_{>0}((v^T\cdot a)^2)$ 
    in case a negative number is tested. The above expression simply checks in each iteration
    whether $X$ already holds a canonical vector. If so, then $X$ is not updated. Otherwise,
    $X$ is replaced by the current canonical vector $b_j$ if and only if $b_j^T\cdot a$ is non-zero. Furthermore, when the final canonical vector is considered and $X$ does not hold
    a canonical vector yet and $b_n^T\cdot a$ is zero, the vector $u$ is returned.

    We use $\nneq{a}{u}$ to find a pivot for a specific column. Let us assume again that we
    want to find a pivot in column $k$ of $A_k$. We can then first make all entries in that column, with indexes smaller or equal to $k$, zero, just as we did by means of $\ccol{\cdot}{\cdot}$ in the
    definition of $\red{\cdot}{\cdot}$. Except, now we also need to make the $k$th entry zero as well.
    Let us denote by $\ccoleq{\cdot}{\cdot}$ the operation $\ccol{\cdot}{\cdot}$, as defined in the proof of Proposition \ref{prop:gauss}, but using $\mathsf{succ}$ instead of $\mathsf{succ}^+$ (to include the $k$ entry). Given this, we can construct $P=I-u\cdot u^T$ as follows:
    $$
    e_{P_u}(A,u) := e_{\mathsf{Id}} - \left[ u - \nneq{ \ccoleq{A}{u} }{u} \right]\cdot \left[ u - \nneq{ \ccoleq{A}{u} }{u} \right]^T.
    $$ 
    From the explanations given above, it should be clear that $e_{P_u}(A,u)$ computes the necessary permutation matrix of $A_k$ for the column indicated by $u$, or $I$
    if no permutation is needed, or if such permutation does not exist (so we skip the current column). Also, we have to modify the $\red{V}{y}$ operator, resulting in $\redd{V}{y}$, as follows:

    \begin{align*}
        &\redd{V}{y}:= e_{\mathsf{Id}}+ f_{>0}\left( ( y^T\cdot V\cdot y)^2 \right)\times f_/\Big(\ccoleq{V}{y}, \\
        &\hspace{4em}\left[ -(y^T\cdot V\cdot y)\times e_{\ones}(y) + \left( 1 - f_{>0}\left( ( y^T\cdot V\cdot y)^2 \right) \right)\times e_{\ones}(y) \right]\Big)\cdot y^T. \\
    \end{align*}

%\domagoj{Should $\ccol{}{}$ be $\ccoleq{}{}$ here? Also, since we overload division by zero, it should not matter if it's zero, right? Below we treat this as important, but t should not be.}    
    
    So, when $V$ is interpreted by a matrix $B$ and $y=b_i$, it returns $I+c_ib_i^T$ if $B_{ii}$ is not zero. 
    If $B_{ii}=0$ then we divide $\ccoleq{B}{b_i}$ by $e_{\ones}(b_i)$ (so we don't get \textit{undefined}), 
    but we don't add $c_ib_i^T$ precisely because $B_{ii}=0$, and return the identity so nothing happens. We check 
    for $f_{>0}((\cdot)^2)$ in case a negative number is tested.

    Finally, we define
    $$
    e_{L^{-1}P}(V):=\initf{e_{\mathsf{Id}}}{v}{X}{\redd{e_{P_v}(X\cdot V,v)\cdot X\cdot V}{v}\cdot e_{P_v}(X\cdot V,v)\cdot X}
    $$
    and $e_{\mathsf{U}}(V):=e_{L^{-1}P}(V)\cdot V$ as the desired expressions.

    As a final observation, in the definition of $e_{L^{-1}P}(V)$ 
    we interlaced permutation matrices with the $T_i$'s. More specifically, 
    $A_k=T_k\cdot P\cdot T_{k-1}\cdots T_1\cdot A$. We observe, however, that for $\ell\leq k-1$ and
    $T_{\ell}=I-c_\ell\cdot b_\ell^T$, we have that  $b_\ell^T\cdot P=b_\ell$ because $b_\ell$ has zeroes in positions in the rows involved in the row exchange $P$. Also, note that  $P^2=I$ and thus 
    $$P\cdot T_\ell\cdot P=P^2-P\cdot c_\ell\cdot b_\ell^T\cdot P=I-\widehat{c}_\ell\cdot b_l^T=\widehat{T}_\ell.$$

    As a consequence,
    $$
    T_k\cdot P\cdot T_{k-1}\cdots T_1=T_k\cdot P\cdot T_{k-1}\cdot P^2\cdot T_{k-2}\cdot P^2\cdots P^2 \cdot T_1\cdot P^2=T_k\cdot (P\cdot T_{k-1}\cdot P)\cdots (P\cdot T_1\cdot P)\cdot P=\widehat{T}_{k-1}\cdots \widehat{T}_1\cdot P,
    $$
    and thus we may assume that $P$ occurs at the end. Hence, we obtain $L^{-1}\cdot P\cdot A=U$.
\end{proof}
