%!TEX root = ../main.tex
One of our main motivations to introduce for-loops is to be able
to express classical linear algebra algorithms in a natural way. We have seen that \langfor is
quite expressive as it can check for cliques, compute the transitive closure, and can even
leverage a successor relation on canonical vectors. The big question is how expressive \langfor
actually is. We will answer this in the next section by connecting \langfor with 
arithmetic circuits of polynomial degree. Through this connection, one can move back and forth between \langfor and arithmetic circuits, and as a consequence, anything computable by such a circuit can be
computed by \langfor as well. When it comes to specific linear algebra algorithms, the detour via circuits
can often be avoided. Indeed, in this section we illustrate that \langfor is able to
compute LU decompositions of matrices. These decompositions form the basis of many other algorithms, such as solving linear systems of equations. We further show that \langfor is expressive enough to compute matrix inversion and the determinant. We recall that matrix inversion and determinant need to be explicitly added as separate operators in \lang~\cite{matlang-journal} and that the LARA language is unable to invert matrices under usual complexity-theoretic assumptions~\cite{BarceloH0S20}.

\subsection{Function Application Simplifications}\label{sec:queries:simp}

When showing results based on induction of expressions in \langfor, it is often convenient to assume that function applications $f(e_1,\ldots,e_k)$ for $f\in\Fun_k$ are restricted to
the case when all expressions $e_1,\ldots,e_k$ have type $1\times 1$. This does not loose generality. Indeed,
for general function applications $f(e_1,\ldots,e_k)$, if we have $\ssum$, scalar product and function application on scalars (here denoted by $f_{1\times 1}$), we can simulate full function application, as follows:
 $$
f(e_1,\ldots, e_k) :=\Sigma v_i \Sigma v_j. f_{1\times 1}(v_i^T\cdot e_1\cdot v_j, \ldots ,v_i^T\cdot e_k\cdot v_j) \times v_i\cdot v_j^T.
$$

Furthermore, it also convenient at times to use the pointwise functions
$f_\odot^k:\RR^k\mapsto \RR:(x_1,\ldots,x_k)\mapsto x_1\times\cdots \cdot x_k$ and 
$f_\oplus^k:\RR^k\mapsto \RR:(x_1,\ldots,x_k)\mapsto x_1+\cdots + x_k$. In fact, it is readily observed that adding these functions does not extend the expressive power of \langfor:
\begin{lemma}
\label{lm-prod-sum}
We have that $\langforf{\emptyset} \equiv \langforf{\{f_\odot^k,f_\oplus^k \ | \ k\in \mathbf{N}\}}$.
\end{lemma}
In fact, this lemma also holds for the smaller fragments we consider.
%
We also observe that having $f_\odot^2:\RR^2\to\RR$ allows us to define scalar multiplication:
$$
e_1\times e_2 :=f_{\kprod}(\ones(e_2)^T\cdot e_1 \cdot \ones(e_2)^T, e_2).
$$
Conversely, $f_\odot^k$ can be expressed using scalar multiplication, as can be seen from our simulation of general function applications by pointwise function application on scalars.

Finally, a notational simplification is that, when using scalars $a\in\RR$ in our expressions, we write sometimes
$a$ instead of $[a]$. For example,  $(1-e_{\ones}(v)^T\cdot v)$ stands for  $([1]-e_{\ones}(v)^T\cdot v)$.

\subsection{LU decomposition}
A lower-upper (LU) decomposition factors a matrix $A$ as the product of a lower triangular matrix $L$ and upper triangular matrix $U$.  
This decomposition, and more generally LU decomposition with row pivoting (PLU),  underlies many linear algebra algorithms and 
we next show that \langfor can compute these decompositions.

\smallskip
\noindent
\textbf{LU decomposition by Gaussian elimination.} LU decomposition can be seen as a matrix form of Gaussian elimination in which the columns of $A$
are reduced, one by one, to obtain the matrix $U$. The reduction of columns of $A$ is achieved
as follows. Consider the first column $[A_{11},\ldots,A_{n1}]^T$ of $A$ and  define 
$c_1 := [0, \alpha_{21},\ldots, \alpha_{n1}]^T$ 
with $\alpha_{j1} := -\frac{A_{j1}}{A_{11}}$. Let $T_1:=I+ c_1\cdot b_1^T$ and consider
$T_1\cdot A$. That is, the $j$th row of $T_1\cdot A$ is obtained by multiplying the first row of $A$ by $\alpha_{j1}$ and adding it to the $j$th row of $A$. As a result, the first column of $T_1\cdot A$ is equal to $[A_{11},0,\ldots,0]^T$, i.e., 
all of its entries below the diagonal are zero.  One then iteratively performs a similar computation, using a matrix $T_i:=I+c_i\cdot b_i^T$, where $c_i$ now depends on the $i$th column in $T_{i-1}\cdots T_1\cdot A$. As a consequence, $T_i\cdot T_{i-1}\cdots T_1\cdot A$ is upper triangular
in its first $i$ columns. At the end of this process, $T_n\cdots T_1\cdot A=U$ where $U$ is the desired upper triangular matrix.
Furthermore, it is easily verified that each $T_i$ is invertible and by defining $L:=T_1^{-1}\cdot\cdots\cdot T_n^{-1}$ one obtains a lower triangular matrix satisfying $A=L\cdot U$. The above procedure is only successful when the denominators used in the definition of the vectors $c_i$ are non-zero. When this is the case we call a matrix $A$ \textit{LU-factorizable}. 

In case when such a denominator is zero in one of the reduction steps, one can remedy this situation by \textit{row pivoting}. That is, when the $i$th entry of the
$i$th row in $T_{i-1}\cdots T_1\cdot A$ is zero, one replaces the $i$th row by  $j$th row in this matrix, with $j>i$, provided that the $i$th entry of the $j$th row is non-zero. If no such row exists, this implies that all elements below the diagonal are zero already in column $i$ and one can proceed with the next column. One can formulate this in matrix terms by stating that there exists a permutation matrix $P$, which pivots rows, such that $P\cdot A=L\cdot U$. Any matrix $A$ is LU-factorizable \textit{with pivoting}.

\smallskip
\noindent
\textbf{Implementing LU decomposition in \langfor.} 
We first assume that the input matrices are LU-factorizable. We deal with general matrices later on.
To implement the above procedure, we need to compute the vector $c_i$ for each column $i$. We do this in two steps. First, we extract from our input matrix its $i$th column and set all its upper diagonal entries to zero
by means of 
 the 
 expression:
$$\ccol{V}{y} := \ffor{v}{X}{\mathsf{succ}^+(y,v)\cdot(v^T\cdot V \cdot y)\cdot v + X}.$$
Indeed, when $V$ is assigned to a matrix $A$ and $y$ to $b_i$, we have that $X$ will be initially assigned
$A_0=\mathbf{0}$ and in consecutive iterations,  $A_j=A_{j-1}+ b_j^T\cdot A\cdot b_i$ if $j>i$ (because $\mathsf{succ}^+(b_i,b_j)=1$ if $j>i$) and $A_j=A_{j-1}$ otherwise (because $\mathsf{succ}^+(b_i,b_j)=0$ for $j\leq i$). The result of this evaluation is the desired column vector.
Using $\ccol{V}{y}$, we can now compute $T_i$ by the following expression:
$$\red{V}{y} := e_{\mathsf{Id}}+ f_/(\ccol{V}{y},-(y^T\cdot V\cdot y)\cdot \ones(y))\cdot y^T,$$
where $f_/:\RR^2\to\RR:(x,y)\mapsto x/y$ is the division function\footnote{We set division by zero to zero.}. 
When $V$ is assigned to $A$ and $y$ to $b_i$, $f_/(\ccol{A}{b_i},-(b_i^T\cdot A\cdot b_i)\cdot \ones(b_i))$ is equal to the vector $c_i$ used in the definition of $T_i$. To perform the reduction steps for all columns, we consider
the expression:
$$
e_{U}(V) :=  \left( \initf{e_{\mathsf{Id}}}{y}{X}{\red{X\cdot V}{y}\cdot X} \right) \cdot V.
$$
That is, when $V$ is assigned $A$, $X$ will be initially $A_0=e_{\mathsf{Id}}$, and then
$A_i=\red{A_{i-1}\cdot A}{b_i}=T_i\cdot T_{i-1}\cdots T_1\cdot A$, as desired.
One can show, see~\cite{geerts2020expressive} for details, that because we can obtain the matrices $T_i$ in \langfor and that these
are easily invertible, we can also construct an expression $e_L(V)$ which evaluates to $L$ when $V$ is assigned to
$A$. We may thus conclude the following.
\begin{proposition}\label{prop:gauss}
There exists $\langforf{f_/}$ expressions $e_L(V)$ and $e_U(V)$ such that
$\sem{e_L}{\I}=L$ and $\sem{e_U}{\I}=U$ form an LU-decomposition of $A$,
where $\conc(V)=A$ and $A$ is LU-factorizable.
\end{proposition}

\input{./sections/proofs/gauss.tex}

We remark that the proposition holds when division is added as a function in $\Fun$
in \langfor.

\smallskip

When row pivoting is needed, we can also obtain a permutation matrix
$P$ such that $P\cdot A=L\cdot U$ holds by means of an expression in \langfor, provided
that we additionally allow the function $f_{>0}$, 
where $f_{>0}:\RR\to\RR$ is such that $f_{>0}(x):=1$ if $x>0$ and $f_{>0}(x):=0$ otherwise.

\begin{proposition}\label{prop:palu}
There exists expressions $e_{L^{-1}P}(M)$ and $e_U(M)$ in $\langforf{f_/,f_{>0}}$  such that
$L^{-1}\cdot P=\sem{e_{L^{-1}P}}{\I}$ and $U=\sem{e_U}{\I}$, satisfy $L^{-1}\cdot P\cdot A=U$.
\end{proposition}

\input{sections/proofs/palu.tex}

Intuitively, by allowing $f_{>0}$ a limited form of \texttt{if-then-else} is introduced in \langfor, which is needed
to continue reducing columns only when the right pivot has been found.


\subsection{Determinant and inverse}\label{sec:queries:inverse}
Other key linear algebra operations include the computation of the determinant and
the inverse of a matrix (if the matrix is invertible). As a consequence of the expressibility
in $\langforf{f_/,f_{>0}}$ of LU-decompositions with pivoting, it can be shown that the determinant
and inverse can be expressed as well. 

However, the results
in the next section (connecting \langfor with arithmetic circuits) imply that the determinant
and inverse of a matrix can already be defined in $\langforf{f_/}$. So instead of using LU decomposition with pivoting for matrix inversion and computing the determinant, we provide an alternative solution.

More specifically, we rely on Csanky's algorithm for computing the inverse of a matrix~\cite{Csanky76}. This algorithm uses the characteristic
polynomial $p_A(x)=\mathsf{det}(xI-A)$ of a matrix. When expanded as a polynomial
$p_A(x)=\sum_{i=0}^{n} c_i x^i$ and it is known that $A^{-1}=\frac{-1}{\phantom{-1}c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i}$
if $c_n\neq 0$. Furthermore, $c_0=1$, $c_n=(-1)^n\mathsf{det}(A)$ and the coefficients $c_i$ of $p_A(x)$
are known to satisfy the system of equations $S\cdot c=s$ given by:
$$
\left(\begin{matrix}
1 & 0 & 0 & \cdots & 0 & 0\\
S_1 & 2 & 0 & \cdots  &0 & 0\\
S_2 & S_1 & 3 & \cdots  &0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & 0\\
S_{n-1} & S_{n-2} & S_{n-3} & \cdots & S_1 & n\\
\end{matrix}\right)\cdot
\left(\begin{matrix}
c_1\\
c_2\\
c_3\\
\vdots\\
c_n\\
\end{matrix}\right)=\left(\begin{matrix}
S_1\\
S_2\\
S_3\\
\vdots\\
S_n\\
\end{matrix}\right),
$$
with $S_i=\mathsf{tr}(A^i)$. We can construct all ingredients of this system of equations in $\langforf{f_/}$. By observing that the matrix $S$ is a lower triangular matrix with non-zero elements on its diagonal, we can write it in the form $D_S+(S-D_{S})=D_S\cdot(I+D_S^{-1}\cdot (S-D_S))$ with $D_S$ the diagonal matrix consisting of the diagonal entries of $S$.
Hence $S^{-1}=(I+D_{S}^{-1}\cdot(S-D_{S}))^{-1}\cdot D_S^{-1}$. 
We remark $D_S^{-1}$ can simply be obtained by inverting the (non-zero) elements on the diagonal by means of $f_/$ in $\langforf{f_/}$. Furthermore, we observe that $(I+D_S^{-1}(S-D_S))^{-1}=\sum_{i=0}^{n}(D_S^{-1}(S-D_S))^i$ which is something
we can compute in $\langforf{f_/}$ as well. Hence, we can invert $S$ and obtain the vector $(c_1,\ldots,c_n)^T$ as $S^{-1}\cdot s$. To compute $A^{-1}$ it now suffices to compute
$\frac{-1}{\phantom{-1}c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i}$. To find the determinant,
we compute $(-1)^nc_n$. All this can be done in $\langforf{f_/}$.
We may thus conclude:
\begin{proposition}\label{prop:inverse}
    There are $\langforf{f_/}$ expressions $e_{\mathsf{det}}(V)$ and $e_{\mathsf{inv}}(V)$ such that
    $\sem{e_{\mathsf{det}}}{\I}=\mathsf{det}(A)$, and  
    $\sem{e_{\mathsf{inv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
    to $A$ and $A$ is invertible.
\end{proposition}

\input{sections/proofs/inverse.tex}
