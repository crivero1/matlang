%!TEX root = /Users/fgeerts/Documents/MLforloops/pods/main.tex
%With the raise of Machine Learning applications we have witnessed in recent years, there is an increasing need for languages that allow us to reason about matrix operations. 

%Matrix operations are at the very core of most Machine Learning algorithms deployed these days. Due to this, there is an increasing need to define languages allowing us to reason about matrix manipulation and more generally, about linear algebra. Several such proposals have been put forward by the database community, mostly focusing on how matrices are manipulated statically. On the other hand, many linear algebra operations require some sort of iteration or recursion. Examples of these are computing the Gaussian elimination, the inverse of a matrix, or its transitive closure.
%
%By observing that the recursion required in these algorithms amounts to repeating 
Linear algebra algorithms have become one of the key components in data analytical workflows. As such, there is growing interest in the database community to integrate linear algebra functionality in relational database management systems \cite{}. In particular, from a query language perspective, several proposals have recently been put forward to unify relational algebra and linear algebra. One such proposal is LARA~\cite{}, a minimalistic language in which a number of atomic operations on so-called associative tables are proposed. Since such tables generalize relational tables, matrices and tensors, the aimed unification of linear and relational algebra is obtained. A key operator in LARA is the extension operator, which is parametrized by (user-defined) functions. Hence, the capabilities of LARA are inherently tied to the allowed functions. Fragments of LARA are known to be expressive complete to first-order logic with aggregation~\cite{}. Furthermore, under complexity-theoretic assumptions, LARA can not compute the inverse of a matrix.\cite{}. This is primarily due to the absence of a recursion mechanism in LARA. 


Another proposal, on which we will built on, is \lang, a pure matrix query language. It consists of a number of atomic linear algebra operators, such as transpose, matrix multiplication, just to name a few. Also here there a are closed connection to first-order logic with aggregates. In fact, \lang\ is subsumed in the three-variable fragment of that logic, resulting in the inability of \lang to check for four-cliques in adjacency matrices and to compute the inverse of a matrix. Furthermore, \lang\ was recently shown to be equivalent to the annotated relation algebra which operates on $K$-relations, provided that input and output relations are restricted to be binary. No recursion mechanism is part of \lang.

In another line of work~\cite{}, matrices are encoded as relational tables and an extensions of SQL is proposed to carry out matrix manipulations. In particular, \cite{} extends SQL with a limited form of recursion -- alike dynamic programming - such that linear algebra-based procedures for learning feed-forward neural networks can be declaratively specified. To our knowledge, the precise expressive power of the resulting language has not been characterized. For example, it is unclear whether matrix inversion can be expressed.






What we aim for in this paper is to extend \lang with a natural notion of recursion. By inspecting any textbook on linear algebra algorithms a natural candidate pops up: for loops.


\begin{itemize}
\item Explain why this is important.
\item Say what sort of things we would like to express.
\item Give a quick tour of our minimal language (examples).
\item Stress our concrete contributions.
\item Relate to previous work \cite{matlang,BrijderGBW19,Geerts19,HutchisonHS17}.
\end{itemize}

\domagoj{A good example for the intro is all shortest paths via Floyd-Warshall.}

\noindent
$\ffor{e_k}{\Dist}{ }$
\\
\hspace*{0.5cm} $\ffor{e_i}{\Dist}{ }$
\\
\hspace*{1cm} $\ffor{e_j}{\Dist}{ }$
\\
\hspace*{1.5cm} 
$\texttt{curr} := e_i^*\cdot \Dist \cdot e_j$\\
\hspace*{1.5cm} 
$\texttt{new} := e_i^*\cdot \Dist \cdot e_k + e_k^*\cdot \Dist\cdot e_j$\\
\hspace*{1.5cm}
$\Dist + \texttt{update}(\texttt{curr},\texttt{new})\times (e_i\cdot e_j^*)$

where

\[
  			\texttt{update}(x,y)=\begin{cases}
               0, \text{ if } x<=y \\
               -x + y, \text{ if } x > y
            \end{cases}.
		\]
