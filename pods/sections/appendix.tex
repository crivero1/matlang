%!TEX root = /Users/fgeerts/Documents/MLforloops/pods/main.tex
The real work begins here.

\section{ARA}
\input{./sections/app-ara.tex}
\section{\lang$(\sum,\prod)$}
\input{./sections/app-ml-sum.tex}
\section{PALU}
\input{./sections/app-palu.tex}
\section{Determinant and inverse}
\input{./sections/app-inverse.tex}
\section{Circuit Result}
\input{./sections/app-circuit-result.tex}
\section{Logspace Stuff}

\section*{Simulating Logspace Turing machine on input $o^n$.}

We are given a logspace Turing Machine $T=\left(Q,\{0\},\ell,\rhd,\lhd,\Delta,q_0,q_m\right)$ where $Q=\{q_1,\ldots,q_m\}$ are the states, $\ell$ denotes the number of heads, $q_0$ and $q_m$ denote initial and final state, respectively, $\{0\}$ is the tape alphabet, and $\rhd$ and $\lhd$ are special symbols denoting the beginning and the end of the tape, respectively. Finally,
$\Delta=(\Delta_Q,\Delta_1,\ldots,\Delta_\ell)$ is the transition function of $T$, where $\Delta_Q:Q\times \{\rhd,0,\lhd\}^\ell\mapsto Q$ and for $i\in[\ell]$,
$\Delta_i:Q\times  \{\rhd,0,\lhd\}^\ell\mapsto\{\gets,\to\}$. In other words, when $T$ is in state $q$ and the $\ell$ heads read symbols $b_1,\ldots,b_\ell$, $\Delta_Q(q,b_1,\ldots,b_\ell)$ indicates to which state $T$ will transition, and moreover, $\Delta_i(q,b_1,\ldots,b_\ell)$ says in which direction (left or right) the $i$th head will move. We assume that when $T$ is in the initial state $q_0$ all heads point to the first position (i.e., they all read symbol $\rhd$). 

In our setting, the tape contents will always be of the form $w_n:=\rhd 0^n \lhd$ for some $n\in\mathbb{N}$. As usual, $T$ cannot move beyond the begin and end markers, $\rhd$ and $\lhd$, respectively. We assume that $T$ accepts or rejects the input $w_n$ using $\mathcal{O}(n^k)$ steps. In other words, there exists a constant $c$ such that $T$ runs in at most $cn^k$ steps. For simplicity, we assume that $T$ runs for at most $n^{k+1}$ steps. For technical reasons, that will become clear below, we assume that once $T$ reaches the final state $q_m$, $T$ will further transition but only to the state $q_m$. That is, $\Delta$ contains transitions that make $T$ loop in $q_m$.

\begin{proposition}
Given a logspace Turing machine $T$ with $m$ states, $\ell$ heads and which runs on $w_n=\rhd 0^n \lhd$ in time $n^{k+1}$, for $n\in\mathbb{N}$, there exists (i)~a $\mathsf{MATLANG}$ 
schema $\mathcal{S}=(\mathcal{M},\textsf{size})$ where $\mathcal{M}$ consists matrix variables\footnote{We also need a finite number of auxiliary variables, these will be specified in the proof.} $X_1,\ldots,X_m,Y_1,\ldots,Y_\ell, v_1,\ldots,v_{k+1}$ and $w$ with
$\mathsf{size}(V)=\alpha\times 1$ for all $V\in\mathcal{M}$ and $V\neq w$ and $\mathsf{size}(w)=1\times 1$; and (ii)~a $\mathsf{MATLANG}$ expression $e_T$ over $\mathcal{S}$ such that for the instance $I=(\mathcal{D},\textsf{mat})$ over $\mathcal{S}$ with $\mathcal{D}(\alpha)=n+2$ and $\mathsf{mat}(V)=\left(\begin{smallmatrix}0\\
0\\\vdots\\0\end{smallmatrix}\right)$ for all $V\in\mathcal{M}$ and
$\mathsf{mat}(w)=[0]$, we have that 
$e_T(I)=[1]$ if $T$ accepts $w_n$ and  $e_T(I)=[0]$ otherwise.
\end{proposition}
\begin{proof}
We start by explaining the semantics of the matrix variables in $\mathcal{M}$. The variables $X_1,\ldots,X_m,Y_1,\allowbreak\ldots,Y_\ell$ will be used inside for loops and will be updated using \textsf{MATLANG} expressions. Initially, all these matrix variables are instantiated with the zero column vector, as described by the instance $I$. 

With each state $q_i\in Q$ we associate matrix variable $X_i$.
Then, $T$ is in state $q_i$ when
 $X_i=\left(\begin{smallmatrix}1\\
0\\\vdots\\0\end{smallmatrix}\right)$, otherwise $X_i=\left(\begin{smallmatrix}0\\
0\\\vdots\\0\end{smallmatrix}\right)$.	Similarly, with each head $i\in[\ell]$ we
associate matrix variable $Y_i$. When the $i$th head points at position $j$ in $w_n$,
then $Y_i=\mathsf{e}_j$, i.e., it is the $j$th canonical column vector. We remark that since the dimensions is $n+2$ and $T$ cannot change the input word $w_n$, the $n+2$ canonical vectors suffice to indicate all positions in $w_n$ (which is of length $n+2$).

The variables $v_1,\ldots,v_{k+1}$ represent $k+1$ canonical vectors  which are use to iterate in for loops. By iterating over then, we can perform $(n+2)^{k+1}$ iterations, which suffices for simulating the $n^{k+1}$ steps used by $T$ on input $w_n$.

Finally, the variable $w$ is used for the output of $e_T$. It contains a scalar and will hold $1$ if $w_n$ is accepted by $T$ and $0$ otherwise.

The expression $e_T$ uses some subexpressions in $\mathsf{MATLANG}$ which use some auxiliary variables.
As a consequence, $e_T$ is an expression defined over an extended schema $\mathcal{S}'$. Hence, the instance $I$ in the statement of the Proposition is in fact an instance $I'$ of $\mathcal{S}'$ which
coincides with $I$ on $\mathcal{S}$ and in which the auxiliary matrix variables are all instantiated with zero vectors or matrices, depending on their size.

We list the used subexpressions next and explicitly denote the auxiliary matrix variables:
\begin{itemize}
	% \item $\mathsf{max}(z,Z)$, an expression over auxiliary variables $z$ and $Z$ with $\mathsf{size}(z)=\mathsf{size}(Z)=\alpha\times 1$. On input $I'$ with
	% $\mathsf{mat}(z)=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$,
	%  $\mathsf{max}(I)=\mathbf{e}_{n+2}$.
	\item $\mathsf{pred}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$, and $\mathsf{mat}(Z')$ the zero $(n+2)\times (n+2)$ matrix,
	 $\mathsf{pred}(I')$ returns an $(n+2)\times (n+2)$ matrix such that 
	 
	 $$\mathsf{pred}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i-1} & \text{if $i>1$}\\
	 \mathbf{0} & \text{if $i=1$}.
	\end{cases}
	$$
	In other words, $\mathsf{pred}$ defines a predecessor relation among canonical vectors of dimension $n+2$.
	 \item $\mathsf{succ}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$, and $\mathsf{mat}(Z')$ the zero $(n+2)\times (n+2)$ matrix,
	 $\mathsf{succ}(I')$ returns an $(n+2)\times (n+2)$ matrix such that 
	 
	 $$\mathsf{succ}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i+1} & \text{if $i<n+2$}\\
	 \mathbf{0} & \text{if $i=n+2$}.
	\end{cases}
	$$
	In other words, $\mathsf{succ}$ defines a successor relation among canonical vectors.
	\item $\textsf{ismin}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $(n+2)\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$	$$\mathsf{ismin}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_1$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\textsf{ismax}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and 
	and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $(n+2)\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$
	
	$$\mathsf{ismax}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_{n+2}$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\mathsf{min}(z,Z,z',Z',z'',Z'')$, an expressions with
	auxiliary variables $z$, $z'$, $z''$, $Z$, $Z'$ and $Z''$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(z'')=\mathsf{size}(Z)=\mathsf{size}(Z'')\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	matrix variables instantiated with zero vectors (or matrix for $Z'$),
 	 $\mathsf{min}(I')=\mathbf{e}_1$. 
	 		\item We additionally define, based on the previous expressions, 
			$$\mathsf{test}_b(v,z,Z,z',Z'):=\begin{cases} \mathsf{ismin}(v,z,Z,z,Z') & \text{if $b=\rhd$}\\
     \mathsf{ismax}(v,z,Z,z,Z') & \text{if $b=\lhd$}\\
	 (1-\mathsf{ismin}(v,z,Z,z',Z'))(1-\mathsf{ismax}(v,z,Z,z',Z')) & \text{if $b=0$}.
		\end{cases}.$$
	When evaluated on $I'[v\gets\mathbf{v}]$, $\mathsf{test}_b(I'[v\gets\mathbf{v}])$ will be $1$ when either
	$\mathbf{v}=\mathbf{e}_{1}$ and $b=\rhd$ (first position)
	$\mathbf{v}=\mathbf{e}_{n+2}$ and $b=\lhd$ (last position),
	$\mathbf{v}\neq \mathbf{e}_{1}$ and $\mathbf{v}\neq \mathbf{e}_{n+2}$  and $b=0$ (not first or last position). We use this expression below to check whether the heads are consistent with the symbols on the tape.
	
	
 \item Finally, we define
 $$
 \mathsf{move}_d(z,Z,z',Z'):=\begin{cases}
 e_{\mathsf{pred}}(z,Z,z',Z) & \text{if $d=\gets$}\\
  e_{\mathsf{succ}}(z,Z,z',Z) & \text{if $d=\to$}. 
 \end{cases},
 $$
 $\mathsf{move}_d(I')$ will simply return the predecessor matrix when $d=\gets$ and the successor matrix when $d=\to$. This expression will be used to move the heads.
\end{itemize}
We thus see that we only need $z,z',z'',Z,Z',Z''$ as auxiliary variables and these can be re-used for every occurrence of the subexpressions in $e_T$. From now one, we omit the auxiliary variables from the description of $e_T$.

We define the expression $e_T$, as follows\footnote{In the expression I uses $;w$ to indicate the output variable. That is, the for loops updates all instances for $X_1,\ldots,X_m,Y_1,\ldots,Y_\ell$ and $w$, but the result of the expression is only what is in the instance corresponding to $w$. We can simulate this again if we allow constant dimensional canonical basis vectors in $\mathsf{MATLANG}$.}:
$$
e_T:= \mathsf{for\,} v_1,\ldots,v_{k+1},X_1,\ldots,X_m,Y_1,\ldots,Y_\ell; w.(e_w,e_{X_1},\ldots,e_{X_m},e_{Y_1},\ldots,e_{Y_\ell}),
$$
with 
\begin{align*}\allowdisplaybreaks
	e_w&:=\mathsf{ismin}(X_m)\\
	e_{X_1}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+ \sum_{\substack{q,b_1,\ldots,b_\ell\\
	\Delta_Q(q,b_1,\ldots,b_\ell)=q_1}} \!\!\!\!\!\!\!\!\! \textsf{ismin}(X_q)\left(\prod_{j=1}^\ell \mathsf{test}_{b_j}(Y_j)\right)\mathsf{min}\\
	e_{X_i}&:= \sum_{\substack{q,b_1,\ldots,b_\ell\\
	\Delta_Q(q,b_1,\ldots,b_\ell)=q_i}}\!\!\!\!\!\!\!\!\! \textsf{ismin}(X_q)\left(\prod_{j=1}^\ell \mathsf{test}_{b_j}(Y_j)\right)\mathsf{min} \quad \text{for $i\neq 1$}\\
	e_{Y_i}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{q,b_1,\ldots,b_\ell\\
	\Delta_i(q,b_1,\ldots,b_\ell)=d}}\!\!\!\!\!\!\!\!\! \textsf{ismin}(X_q)\left(\prod_{j=1}^\ell \mathsf{test}_{b_j}(Y_j)\right)\mathsf{move}_d\cdot Y_i
\end{align*}

The correctness of $e_T$ is now readily verified. We do this by induction on the number of iterations in the for loop. We note that initially, all variables are assigned zero vectors and values (for $w$). 

At the start of the run of $T$, we are in state $q_1$ and all heads point to the first position. We argue that after the first 
iterations, i.e., when $v_i=\mathbf{e}_1$ for $i\in[k+1]$, we indeed have that $X_1=\mathbf{e_1}$, $X_j=\mathbf{0}$ for $j\neq 1$, and $Y_j=\mathbf{e}_1$ for $j\in[\ell]$. Indeed, in the expression for $e_{X_1}$ the test $\prod_{j=1}^{k+1} \textsf{ismin}(v_i)$ will return $1$ and hence $X_1$ is replaced by $\mathsf{min}=\mathbf{e}_1$. Since all $X_i$ are initially zero, $\mathsf{ismin}(X_i)$ evaluate to zero for all $i\in[m]$ so the second term in $e_{X_1}$ adds the zero vector to $\mathbf{e}_1$ and thus $X_1$ remains $\mathbf{e_1}$.
Similarly, $e_{X_j}$ for $j\neq 1$ will leave $X_j$ unchanged, so these remain zero vectors. For the head positions, a similar arguments shows that after the first iterations, all $Y_i$ are set of $\mathbf{e}_1$.

We next assume that up to a certain iteration $\kappa-1$, the matrix variables correctly encode a configuration of $T$ on $w_n$ and furthermore, this configuration is reachable from the initial configuration. We next show that this remains to hold in the $\kappa$th iteration.

By induction, there will be a single $X_i$ which is instantiated with $\mathbf{e}_1$. Let use assume that this $X_q$. All other $X_i$ are instantiated with the zero vector. Furthermore, $Y_j=\mathbf{e}_{i_j}$ for some $i_j\in[n+2]$. 

Suppose that
$\Delta_Q(q,b_{i_1},\ldots,b_{i\ell})=p$ and
$\Delta_j(q,b_{i_1},\ldots,b_{i\ell})=d_j$. Then, inspecting the expressions $e_{X_i}$, all $X_{q_i}$ with $q_i\neq p$ will be replaced by the zero vector. The reason is that for $X_i$ to be replaced by $\mathsf{min}$ (i.e., $\mathbf{e}_1$) when $T$ is in state $q$,
there must be a transition $\Delta_Q(q,b_{i_1}',\ldots,b_{i_\ell}')=q_i$
and such that the $b_{i_j}'$ corresponds to the positions encoded by
the $Y_i$'s. In particular, $b_{i_1},\ldots,b_{i\ell}$ and 
$b_{i_1}',\ldots,b_{i_\ell}'$ must have $\rhd$ and $\lhd$ at the same positions, and since the only remaining symbol is $0$, $b_{i_1},\ldots,b_{i\ell}$ and $b_{i_1}',\ldots,b_{i\ell}'$ must agree.
This in turn would imply that there are two possible states $p$ and $q_i$ from $q$ while reading $b_{i_1},\ldots,b_{i\ell}$. This is impossible since $T$ is deterministic.

If we next consider the expressions $e_{Y_i}$ for $i\in[\ell]$, then a similar argument shows that at most one of the terms in the second part in $e_{Y_i}$ can replace $Y_i$ with $\mathsf{move}_d\cdot Y_i$. By defining of $\mathsf{move}_d$ in terms of the predecessor or successor matrix (depending on whether $d=\gets$ or $d=\to$, respectively), and given that $Y_i$ corresponds to a canonical vector, say $\mathbf{e}_{i_s}$, then $\mathsf{move}_d\cdot Y_i$ will replace $Y_i$
with either $\mathbf{e}_{i_s-1}$ or  $\mathbf{e}_{i_s+1}$. We note that when $Y_i$ is $\mathbf{e}_1$ or $\mathbf{e}_{n+2}$, $d$ must necessarily be $\to$ or $\gets$, respectively, since $T$ does not move beyond the end markers. 

Hence, all combined we see that after the $\kappa$the iteration, $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_\ell$ indeed correspond to the next configuration of $T$.

We now remark that $w$ will be zero unless in one of the iterations populates $X_m$ with $\mathbf{e}_{1}$, i.e., the $T$ is in the final state. By assumption, $T$ will continue to be in the final state from that point on, and thus after perform our $(n+2)^k$, $w$ will remain $1$. If no final state is encountered, $w$ remains $0$, as desired.
\end{proof}

\section*{Simulating Linear Space Functions}
We consider  deterministic Turing Machines  (TM) $T$ consisting of $\ell$ read-only input tapes, denoted by $R_1,\ldots,R_\ell$,
a work tape, denoted by $W$, and a write-only output tape, denoted by $O$. The TM $T$ has a set $Q$ of $m$
states, denoted by $q_0,\ldots,q_m$. We assume that $q_0$ is the initial state and $q_m$ is the accepting state.
The input and tape alphabet are $\Sigma=\{0,1\}$ and $\Gamma=\Sigma\cup\{\rhd,\lhd\}$, respectively. The special symbol $\rhd$ denotes the beginning of each of the tapes, the symbol $\lhd$ denotes the end of the $\ell$ input tapes. The transition function $\Delta$ is defined as usual, i.e., 
$\Delta:Q\times \Gamma^{\ell+2} \to Q\times \Gamma^{2}\times \{\leftarrow,\sqcup,\rightarrow\}^{\ell+2}$ such that $\Delta(q,(a_1,\ldots,a_{\ell},b,c))=\bigl(q',(b',c'),(\mathsf{d}_1,\ldots,\mathsf{d}_{\ell+2})\bigr)$ with $\mathsf{d}_i\in \{\leftarrow,\sqcup,\rightarrow\}$, means that when $T$ is in state $q$ and the $\ell+2$ heads on the tapes read symbols $a_1,\ldots,a_{\ell},b,c$, respectively, then $T$ transitions to state $q'$, writes $b',c'$ on the work and output tapes, respectively, at the position to which the work and output tapes' heads points at, and finally moves the heads on the tapes according $\mathsf{d}_1,\ldots,\mathsf{d}_{\ell+2}$. More specifically, $\leftarrow$  indicates a move to the left, 
$\rightarrow$ a move to the right, and finally, $\sqcup$ indicates that the head does not move.

We assume that $\Delta$ is defined such that it ensures that on none of the tapes, heads can move beyond the leftmost marker $\rhd$. Furthermore, the tapes $R_1,\ldots,R_\ell$ are treated as read-only and the heads on these tapes cannot move beyond the end markers $\lhd$. Similarly, $\Delta$ ensures that the output tape $O$ is write only, i.e., its head cannot move to the left.  We also assume that $\Delta$ does not change the occurrences of $\rhd$ or writes $\lhd$ on the work and output tape.

A configuration of $T$ is defined in the usual way. That is, a configuration of the input tapes is of the form
$\rhd w_1qw_2\lhd$ with $w_1,w_2\in\Sigma^*$ and represents that the current tape content is $\rhd w_1w_2\lhd$, $T$ is in state $q$ and the head is positioned on the first symbol of $w_2$. Similarly, configurations of the work and output tape are represented by $\rhd w_1qw_2$. A configuration of $T$ is consists of configurations for all tapes. Given two configurations $c_1$ and $c_2$, we say that $c_1$ yields $c_2$ if $c_2$ is the result of applying the transition function $\Delta$ of $T$ based on the information in $c_1$. As usual, we close this ``yields'' relation transitively.

Given $\ell$ input words $w_1,\ldots,w_\ell\in\Sigma^*$, we assume that the initial configuration of $T$ is given by
 $\bigl(q_0\rhd  w_1\lhd,q_0\rhd w_2\lhd,\ldots, q_0\rhd w_\ell\lhd,q_0\rhd, q_0\rhd \bigr)$ and an accepting configuration is assumed to be of the form $\bigl(\rhd q_m w_1\lhd,\rhd q_m w_2\lhd,\ldots, \rhd q_m w_\ell\lhd,\rhd q_m,\rhd q_m w\bigr)$ for some $w\in\Sigma^*$. We say that $T$ computes the function $f:(\Sigma^*)^{\ell}\to\Sigma^*$ if for every $w_1,\ldots,w_\ell\in\Sigma^*$, the initial configuration yields (transitively) an accepting configuration such that the configuration on the output tape is
 given by $\rhd q_m f(w_1,\ldots,w_\ell)$.

We assume that once $T$ reaches an accepting configuration it stays indefinitely in that configuration (i.e., it loops). We further assume that $T$ only reaches an accepting configuration when all its input
words have the same size. Furthermore, when all inputs have the same size, $T$ will reach an accepting configuration. 


We say that $T$ is a \textit{linear space machine} when it reaches an accepting configuration 
on inputs of size $n$ by using $\mathcal{O}(n)$ space on its work tape and additionally needs  $\mathcal{O}(n^k)$ steps to do so. A \textit{linear input-output function} is a function of the form $f=\bigcup_{n\geq 0} f_n:(\Sigma^n)^\ell\to\Sigma^n$. In other words, for every $\ell$ words of the same size $n$, $f$ returns a word of size $n$. We say that a linear input-output function is a \textit{linear space input-output function} if
there exists a linear space machine  $T$ that for every $n\geq 0$, on input $w_1,\ldots,w_\ell\in\Sigma^n$ the TM $T$ has
$f_n(w_1,\ldots,w_\ell)$ on its the output tape when (necessarily) reaching an accepting configuration.

% We say that a function
% $f:\underbrace{\Sigma^n\times\cdots \times \Sigma^n}_{\text{$\ell$ times}}\to \Sigma^n$ is computable by a linear space machine $T$ if when $T$ is run on input $w_1,\ldots,w_\ell$ it halts and has $f(w_1,\ldots,w_\ell)$ on its output tape. We say that $f$ is a \textit{linear space poly function} if it is computable by a linear space TM $T$ which in addition runs in polynomial time, i.e., it hals in at most
% $\mathcal{O}(n^k)$ steps for a certain $k$ on any inputs $w_1,\ldots,w_\ell$ of size $n$.
\begin{proposition}
Let $f=\bigcup_{n\geq 0}f_n:(\Sigma^n)^\ell\to \Sigma^n$ be a linear space input-ouput function computed by a linear space  machine $T$ with $m$ states, $\ell$ input tapes, which consumes $\mathcal{O}(n)$ space and runs in $\mathcal{O}(n^{k-1})$ time on inputs of size $n$. There exists (i)~a $\mathsf{MATLANG}$ 
schema $\mathcal{S}=(\mathcal{M},\textsf{size})$ where $\mathcal{M}$ consists matrix variables\footnote{We also need a finite number of auxiliary variables, these will be specified in the proof.} $Q_1,\ldots,Q_m,R_1,\ldots,R_\ell,H_1,\ldots,H_\ell,W_1,\ldots,W_s,H_{W_1},\ldots,H_{W_s},O,H_O, v_1,\ldots,v_{k}$  with
$\mathsf{size}(V)=\alpha\times 1$ for all $V\in\mathcal{M}$; and (ii)~a $\mathsf{MATLANG}$ expression $e_f$ over $\mathcal{S}$ such that for the instance $I=(\mathcal{D},\textsf{mat})$ over $\mathcal{S}$ with $\mathcal{D}(\alpha)=n$ and 
$$\mathsf{mat}(R_i)=\mathsf{vec}(w_i)\in \mathbb{R}^n\text{, for $i\in[\ell]$ and all other matrix variables instantiated with the zero vector in $\mathbb{R}^n$} $$
for words $w_1,\ldots,w_\ell\in\Sigma^n$ and such that $\mathsf{vec}(w_i)$ is the $n\times 1$-vector encoding the word $w_i$, we have that the $\mathsf{mat}(O)=\mathsf{vec}(f_n(w_1,\ldots,w_n))\in\mathbb{R}^n$ after evaluating $e_f(I)$.
\end{proposition}
\begin{proof}
Let us first assume that $n$ is large enough such that $T$ runs in $sn$ space and $cn^{k-1}\leq n^k$ time for constants $s$ and $c$. We deal with smaller $n$ later on.	
We  first explain the semantics of the matrix variables in $\mathcal{M}$ before getting to the construction of the expression $e_f$ in detail.  The variables $R_1,\ldots,R_\ell$ will hold the input vectors, $W_1,\ldots,W_s$ will hold the contents of the work
tape, where $s$ is the constant mentioned earlier, and $O$ will hold the contents of the output tape. The vectors corresponding to the work and output tape are initially set to the zero vector. The vector for the input tape $R_i$ is set to $\mathsf{vec}(w_i)$, for $i\in[\ell]$.

 With each tape we associate a matrix variable encoding the position of the head. More specifically, $H_1,\ldots,H_\ell$ correspond to the input tape heads,
$H_{W_1},\ldots, H_{W_s}$ are the heads for the work tape, and $H_O$ is the head of the output tape. All these vectors are initialised with the zero vector. Later on, these vectors have will be zero except for the position to which the head points to. In those positions the head vectors hold either value $1$ or $2$. We use these two values to identify whether or not we are at the beginning of the tape (and thus reading $\rhd$)
or at the end of the (input) tapes (and thus reading $\lhd$). This is needed because of vectors are of size $n$ while the tape contents are of size $n+2$ (for input tapes), $n+1$ (for the output tape) and $sn+1$ (for the work tape). In other words, we do not have sufficient space to store $\rhd$ and $\lhd$. We provide more details on this later in the proof.
%
% ,Y_1,\allowbreak\ldots,Y_\ell$ will be used inside for loops and will be updated using \textsf{MATLANG} expressions. Initially, all these matrix variables are instantiated with the zero column vector, as described by the instance $I$.
Furthermore, with each state $q_i\in Q$ we associate matrix variable $Q_i$.
Then, $T$ is in state $q_i$ when
 $\mathsf{mat}(Q_i)=(1,0,\ldots,0)^t\in\mathbb{R}^n$, otherwise $\mathsf{mat}(Q_i)$ is the zero vector in $\mathbb{R}^n$.	

The variables $v_1,\ldots,v_{k}$ represent $k$ canonical vectors  which are use to iterate in for loops. By iterating over then, we can perform $n^{k}$ iterations, which suffices for simulating the $\mathcal{O}(n^{k-1})$ steps used by $T$ to reach an accepting configuration. (We recall that $T$ loops when reaching an accepting configuration.)


The expression $e_f$ relies on some subexpressions in $\mathsf{MATLANG}$ which use some auxiliary variables. As a consequence, $e_f$ is in fact an expression defined over an extended schema $\mathcal{S}'$. Hence, the instance $I$ in the statement of the Proposition is  an instance $I'$ of $\mathcal{S}'$ which
coincides with $I$ on $\mathcal{S}$ and in which the auxiliary matrix variables are all instantiated with zero vectors or matrices, depending on their size.

\floris{The expressions below are used in other proofs as well, I just wanted to check how many auxiliary variables are needed. We can extract the list below and place it somewhere else.}
We next list the used subexpressions and explicitly denote the auxiliary matrix variables:
\begin{itemize}
	% \item $\mathsf{max}(z,Z)$, an expression over auxiliary variables $z$ and $Z$ with $\mathsf{size}(z)=\mathsf{size}(Z)=\alpha\times 1$. On input $I'$ with
	% $\mathsf{mat}(z)=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$,
	%  $\mathsf{max}(I)=\mathbf{e}_{n+2}$.
	\item $\mathsf{pred}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n$, and $\mathsf{mat}(Z')$ the zero $n\times n$ matrix,
	 $\mathsf{pred}(I')$ returns an $n\times n$ matrix such that 
	 
	 $$\mathsf{pred}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i-1} & \text{if $i>1$}\\
	 \mathbf{0} & \text{if $i=1$}.
	\end{cases}
	$$
	In other words, $\mathsf{pred}$ defines a predecessor relation among canonical vectors of dimension $n$.
	 \item $\mathsf{succ}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n$, and $\mathsf{mat}(Z')$ the zero $n\times n$ matrix,
	 $\mathsf{succ}(I')$ returns an $n\times n$ matrix such that 
	 
	 $$\mathsf{succ}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i+1} & \text{if $i<n$}\\
	 \mathbf{0} & \text{if $i=n$}.
	\end{cases}
	$$
	In other words, $\mathsf{succ}$ defines a successor relation among canonical vectors.
	\item $\textsf{ismin}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $n\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$	$$\mathsf{ismin}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_1$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\textsf{ismax}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and 
	and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $n\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$
	
	$$\mathsf{ismax}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_{n}$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\mathsf{min}(z,Z,z',Z',z'',Z'')$, an expressions with
	auxiliary variables $z$, $z'$, $z''$, $Z$, $Z'$ and $Z''$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(z'')=\mathsf{size}(Z)=\mathsf{size}(Z'')=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	matrix variables instantiated with zero vectors (or matrix for $Z'$),
 	 $\mathsf{min}(I')=\mathbf{e}_1$. 
	 		
\end{itemize}
We thus see that we only need $z,z',z'',Z,Z',Z''$ as auxiliary variables and these can be re-used whenever $e_f$ calls these functions. From now one, we omit the auxiliary variables from the description of $e_f$.

To start with the description of $e_f$ we further need to be able to check which transitions of $T$ can be applied based on a current configuration. More precisely,
suppose that we want to check whether $\delta(q_i,(a_1,\ldots,a_{\ell},b,c))$ is applicable.
This pours down to checking whether $T$ is in state $q_i$, we do this by checking 
$\mathsf{ismin}(Q_i)$, and whether the heads on the tapes read symbols $a_1,\ldots,a_{\ell},b,c$. We check the latter by the following expressions:
For the input tapes $R_i$ we define
$$
\mathsf{test\_inp}^i_b:=\begin{cases}
1- O^t\cdot H_O & \text{if $b=0$}\\
O^t\cdot H_O & \text{if $b=1$}\\
\mathsf{ismin}(1/2\cdot H_i) & \text{if $b=\rhd$}\\
\mathsf{ismax}(1/2\cdot H_i) & \text{if $b=\lhd$},\\
\end{cases}
$$
which return $1$ if and only if either $b\in\{0,1\}$ is the value in $\mathsf{mat}(R_i)$ at the position encoded by $\mathsf{mat}(H_i)$, or when $b=\rhd$ and $\mathsf{mat}(H_i)$ is the vector $(2,0,\ldots,0)\in\mathbb{R}^n$, or when $b=\lhd$ and $\mathsf{mat}(H_i)$ is the vector $(0,0,\ldots,2)\in\mathbb{R}^n$. Similarly, for the output tape we define
$$
\mathsf{test\_out}_b:=\begin{cases}
1- O^t\cdot H_O & \text{if $b=0$}\\
O^t\cdot H_O & \text{if $b=1$}\\
\mathsf{ismin}(1/2\cdot H_O) & \text{if $b=\rhd$}.\\
\end{cases}
$$
Finally, for the work tapes $W_1,\ldots,W_s$ we define
$$
\mathsf{test\_work}^i_b:=\begin{cases}
1- W_i^t\cdot H_{W_i} & \text{if $b=0$}\\
W_i^t\cdot H_{W_i} & \text{if $b=1$}\\
\mathsf{ismin}(1/2\cdot H_{W_i}) & \text{if $b=\rhd$ and $i=1$}.\\
\end{cases}
$$
We combine all these expressions into a single expression 
$$
\mathsf{isconf}_{q_i,a_1,\ldots,a_\ell,b,c}:=
\mathsf{ismin}(Q_i)\cdot \left(\prod_{j=1}^{\ell} \mathsf{test\_inp}_{a_j}^j\right)
\cdot\mathsf{test\_outp}_{c}\cdot\left(\sum_{j=1}^s \mathsf{test}_b^j\right).
$$
This expression will return $1$ if and only if the vectors representing the tapes, head positions and states are such that $\mathsf{Q_i}$ is the first canonical vector (and thus $T$ is in state $q_i$), the heads point to entries in the tape vectors storing the symbols $a_1,\ldots,a_{\ell}, b,c$ or they point to the first (or last for input tapes) positions but have value $2$ (when the symbols are $\rhd$ or $\lhd$). To ensure that at the beginning of the simulation of $T$ by $e_f$ we correctly encode that we are in the initial configuration, we thus need to initialise all vectors $\mathsf{mat}(H_1),\mathsf{mat}(H_2),\ldots, \mathsf{mat}(H_\ell), \mathsf{mat}(H_{W_1}),\mathsf{mat}(H_O)$ with the vector $(2,0,0,\ldots,0)\in\mathbb{R}$ since all heads read the symbol $\rhd$. Similarly, we have to initialise $\mathsf{Q_1}$ with the first canonical vector since $T$ is in state $q_0$.

We furthermore need to be able to correctly adjust head positions. We do this by means of the predecessor and successor expressions described above. 
A consequence of our encoding is that we need to treat the border cases (corresponding to $\rhd$ and $\lhd$) differently. More specifically, for the input tapes we define 
$$
\mathsf{move\_inp}^i_{\mathsf{d}}:=
\begin{cases}
2\cdot \mathsf{ismin}(H_i)\cdot H_i + 1/2\cdot\mathsf{ismax}(1/2\cdot H_i)\cdot H_i  + (1-\mathsf{ismin}(H_i))(1-\mathsf{ismax}(1/2H_i))\cdot e_{\mathsf{pred}}\cdot H_i  
& \text{if $\mathsf{d}=\leftarrow$}\\
2\cdot \mathsf{ismax}(H_i)\cdot H_i + 1/2\cdot\mathsf{ismin}(1/2\cdot H_i)\cdot H_i  + (1-\mathsf{ismin}(1/2\cdot H_i))(1-\mathsf{ismax}(H_i))\cdot e_{\mathsf{succ}}\cdot H_i  
 & \text{if $\mathsf{d}=\rightarrow$}\\
H_i & \text{if $\mathsf{d}=\sqcup$}. 
\end{cases}
$$
In other words, we shift to the previous (or next) canonical vector when $\mathsf{d}$ is $\leftarrow$ or $\rightarrow$, respectively, unless we need to move to or from the position that will hold $\rhd$ or $\lhd$. In those case we readjust $\mathsf{mat}(H_i)$ (which will either $(1,0,\ldots,0)$, $(2,0,\ldots,0)$, $(0,\ldots,0,1)$ or $(0,\ldots,0,2)$) by either dividing or multiplying with $2$. In this way we can correctly infer whether or not the head point to the begin and end markers. For the output we proceed in a similar way, but only taking into account the begin marker and recall that we do not have moves to the left:
$$
\mathsf{move\_outp}_{\mathsf{d}}:=
\begin{cases}
1/2\cdot\mathsf{ismin}(1/2\cdot H_O)\cdot H_O  + (1-\mathsf{ismin}(1/2\cdot H_O))\cdot e_{\mathsf{succ}}\cdot H_O  
 & \text{if $\mathsf{d}=\rightarrow$}\\
H_O & \text{if $\mathsf{d}=\sqcup$}. 
\end{cases}
$$
Since we represent the work tape by $s$ vectors $W_1,\ldots,W_s$ we need to ensure that only one of the head vectors $H_{W_i}$ has a non-zero value and that by moving left or right, we need to appropriately update the right head vector. We do this as follows. We first consider the work tapes $W_i$ for $i\neq 1,s$ and define
$$
\mathsf{move\_work}^i_{\mathsf{d}}:=
\begin{cases}
	-\mathsf{ismin}(H_{W_i})H_{W_i} + (1-\mathsf{ismin}(H_{W_i}))\cdot e_{\mathsf{pred}}\cdot H_{W_i} + \mathsf{ismin}(H_{W_{i+1}})\mathsf{max} & \text{if $\mathsf{d}=\leftarrow$}\\
		-\mathsf{ismax}(H_{W_i})H_{W_i} + (1-\mathsf{ismax}(H_{W_i}))\cdot e_{\mathsf{succ}}\cdot H_{W_i} + \mathsf{ismax}(H_{W_{i-1}})\mathsf{min} & \text{if $\mathsf{d}=\rightarrow$}\\
	H_{W_i} & \text{if $\mathsf{d}=\sqcup$}. 	
\end{cases}
$$
Similarly for $i=s$ we define
$$
\mathsf{move\_work}^s_{\mathsf{d}}:=
\begin{cases}
	-\mathsf{ismin}(H_{W_s})H_{W_i} + (1-\mathsf{ismin}(H_{W_s}))\cdot e_{\mathsf{pred}}\cdot H_{W_s}  & \text{if $\mathsf{d}=\leftarrow$}\\
		-\mathsf{ismax}(H_{W_s})H_{W_s} + (1-\mathsf{ismax}(H_{W_s}))\cdot e_{\mathsf{succ}}\cdot H_{W_s} + \mathsf{ismax}(H_{W_{s-1}})\mathsf{min} & \text{if $\mathsf{d}=\rightarrow$}\\
	H_{W_s} & \text{if $\mathsf{d}=\sqcup$}. 	
\end{cases}
$$
For $i=1$ we again have to take $\rhd$ into account:
$$
\mathsf{move\_work}^1_{\mathsf{d}}:=
\begin{cases}
	2\mathsf{ismin}(H_{W_1})H_{W_i} + (1-\mathsf{ismin}(H_{W_1}))\cdot e_{\mathsf{pred}}\cdot H_{W_1} + \mathsf{ismin}(H_{W_{2}})\mathsf{max} & \text{if $\mathsf{d}=\leftarrow$}\\
		1/2\mathsf{ismin}(1/2H_{W_1})H_{W_1} + (1-\mathsf{ismax}(1/2H_{W_1}))\cdot e_{\mathsf{succ}}\cdot H_{W_1}  & \text{if $\mathsf{d}=\rightarrow$}\\
	H_{W_1} & \text{if $\mathsf{d}=\sqcup$}. 	
\end{cases}
$$
A final ingredient for defining $e_f$ are expressions which update the work and output tape.
To define these expression, we need the position and symbol to put on the tape. For the output tape we define
$$
\mathsf{write\_outp}_b:=\begin{cases}
O & \text{if $b=\rhd$}\\
(1-O^t\cdot H_O)O + (O^t\cdot H_O)\cdot (O-H_O) &\text{if $b=0$}\\
(1-O^t\cdot H_O)(O+H_O) + (O^t\cdot H_O)\cdot O &\text{if $b=1$}\\
\end{cases}
$$
and similarly for the work tapes
$$
\mathsf{write\_work}_b^i:=\begin{cases}
W_i & \text{if $b=\rhd$}\\
(1-W_i^t\cdot H_{W_i})W_i + (W_i^t\cdot H_{W_i})\cdot (W_i-H_{W_i}) &\text{if $b=0$}\\
(1-W_i^t\cdot H_{W_i})(W_i+H_{W_i}) + (W_i^t\cdot H_{W_i})\cdot W_i &\text{if $b=1$}.
\end{cases}
$$



% We additionally define, based on the previous expressions,
% 			$$\mathsf{test}_b(v,z,Z,z',Z'):=\begin{cases} \mathsf{ismin}(v,z,Z,z,Z') & \text{if $b=\rhd$}\\
%      \mathsf{ismax}(v,z,Z,z,Z') & \text{if $b=\lhd$}\\
% 	 (1-\mathsf{ismin}(v,z,Z,z',Z'))(1-\mathsf{ismax}(v,z,Z,z',Z')) & \text{if $b=0$}.
% 		\end{cases}.$$
% 	When evaluated on $I'[v\gets\mathbf{v}]$, $\mathsf{test}_b(I'[v\gets\mathbf{v}])$ will be $1$ when either
% 	$\mathbf{v}=\mathbf{e}_{1}$ and $b=\rhd$ (first position)
% 	$\mathbf{v}=\mathbf{e}_{n+2}$ and $b=\lhd$ (last position),
% 	$\mathbf{v}\neq \mathbf{e}_{1}$ and $\mathbf{v}\neq \mathbf{e}_{n+2}$  and $b=0$ (not first or last position). We use this expression below to check whether the heads are consistent with the symbols on the tape.
%
%
% Finally, we define
%  $$
%  \mathsf{move}_d(z,Z,z',Z'):=\begin{cases}
%  e_{\mathsf{pred}}(z,Z,z',Z) & \text{if $d=\gets$}\\
%   e_{\mathsf{succ}}(z,Z,z',Z) & \text{if $d=\to$}.
%  \end{cases},
%  $$
%  $\mathsf{move}_d(I')$ will simply return the predecessor matrix when $d=\gets$ and the successor matrix when $d=\to$. This expression will be used to move the heads.
% We define the expression $e_T$, as follows\footnote{In the expression I uses $;w$ to indicate the output variable. That is, the for loops updates all instances for $X_1,\ldots,X_m,Y_1,\ldots,Y_\ell$ and $w$, but the result of the expression is only what is in the instance corresponding to $w$. We can simulate this again if we allow constant dimensional canonical basis vectors in $\mathsf{MATLANG}$.}:

We are now finally ready to define $e_f$ for $n$ sufficiently large:
\begin{multline*}
e_f:= \mathsf{for\,} v_1,\ldots,v_{k},Q_1,\ldots,Q_m,H_1,\ldots,H_\ell,W_1,\ldots,W_s, H_{W_1},\ldots,H_{W_s},O,H_O . \\
(e_{Q_1},\ldots,e_{Q_m},e_{H_1},\ldots,e_{H_\ell},e_{W_1},\ldots,e_{W_s},e_{H_{W_1}},\ldots,e_{H_{W_s}},e_{O}, e_{H_O})
\end{multline*}
with expressions (we use $\star$ to mark irrelevant information in the transitions)
 \allowdisplaybreaks
\begin{align*}
	e_{Q_1}&:=\left(\prod_{j=1}^{k} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+ \sum_{\substack{(q_i,a_1,\ldots,a_\ell,b,c)\\
	\Delta(q_i,a_1,\ldots,a_\ell,b,c)=(q_1,\star)}} \!\!\!\!\!\!\!\!\! \mathsf{isconf}_{q_i,a_1,\ldots,a_\ell,b,c}\cdot \mathsf{min} \\
	e_{Q_j}&:=\sum_{\substack{(q_i,a_1,\ldots,a_\ell,b,c)\\
	\Delta(q_i,a_1,\ldots,a_\ell,b,c)=(q_j,\star)}} \!\!\!\!\!\!\!\!\! \mathsf{isconf}_{q_i,a_1,\ldots,a_\ell,b,c}\cdot \mathsf{min}
	 \quad \text{for $j\neq 1$}\\
	e_{H_i}&:=2\left(\prod_{j=1}^{k} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d_i},\star)}}\!\!\!\!\!\!\!\!\! \mathsf{isconf}_{q,a_1,\ldots,a_\ell,b,c}\cdot\mathsf{move\_inp}^i_{\mathsf{d}_i}\\
	e_{H_{W_i}}&:=2\left(\prod_{j=1}^{k} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d_{\ell+1}},\star)}}\!\!\!\!\!\!\!\!\! \mathsf{isconf}_{q,a_1,\ldots,a_\ell,b,c}\cdot\mathsf{move\_work}_{\mathsf{d}_{\ell+1}}^i\\
	e_{H_O}&:=2\left(\prod_{j=1}^{k} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d}_{\ell+2})}}\!\!\!\!\!\!\!\!\! \mathsf{isconf}_{q,a_1,\ldots,a_\ell,b,c}\cdot\mathsf{move\_outp}_{\mathsf{d}_{\ell+2}}\\
	e_{W_i}&:=\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(b',c',\star)}}\!\!\!\!\!\!\!\!\! \mathsf{isconf}_{q,a_1,\ldots,a_\ell,b,c}\cdot\mathsf{write\_work}_{b'}^i\\
	e_{O}&:=\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(b',c',\star)}}\!\!\!\!\!\!\!\!\! \mathsf{isconf}_{q,a_1,\ldots,a_\ell,b,c}\cdot\mathsf{write\_outp}_{c'}.
\end{align*}
To deal with small $n$, we observe that there are only a finite number of cases to
consider. More specifically, we for each $n<N$ and every possible input words
$w_1,\ldots,w_\ell$ of size $n$, we define MATLANG expression which check whether
$\mathsf{mat}(R)=\mathsf{vec}(w_i)$ for each $i\in[\ell]$, and then put $f_n(w_1,\ldots,w_\ell)$ as a vector in $\mathsf{mat}(O)$. It is readily (?) verified
that such an expression can be formed. Let $e_{w1,\ldots,w_\ell,f_n(w_1,\ldots,w_\ell)}$
be this expression. By considering 
$$
e_n:=\mathsf{dim\_is_n}\left(\sum_{w1,\ldots,w_\ell\in\Sigma^n} e_{w_1,\ldots,w_\ell,f_n(w_1,\ldots,w_\ell)}\right)
$$
we can thus ensure that when the dimension is $n$, we can generate the appropriate output.
By combining this into an expression
$$
e_f:=\sum_{n< N} e_n + \mathsf{dim\_is_greater\_than_N}\cdot e_f'
$$
we obtain our final expression.

The correctness of $e_T$ is now readily verified. We do this by induction on the number of iterations in the for loop. We note that initially, all variables are assigned zero vectors and values (for $w$). 

At the start of the run of $T$, we are in state $q_1$ and all heads point to the first position. We argue that after the first 
iterations, i.e., when $v_i=\mathbf{e}_1$ for $i\in[k+1]$, we indeed have that $X_1=\mathbf{e_1}$, $X_j=\mathbf{0}$ for $j\neq 1$, and $Y_j=\mathbf{e}_1$ for $j\in[\ell]$. Indeed, in the expression for $e_{X_1}$ the test $\prod_{j=1}^{k+1} \textsf{ismin}(v_i)$ will return $1$ and hence $X_1$ is replaced by $\mathsf{min}=\mathbf{e}_1$. Since all $X_i$ are initially zero, $\mathsf{ismin}(X_i)$ evaluate to zero for all $i\in[m]$ so the second term in $e_{X_1}$ adds the zero vector to $\mathbf{e}_1$ and thus $X_1$ remains $\mathbf{e_1}$.
Similarly, $e_{X_j}$ for $j\neq 1$ will leave $X_j$ unchanged, so these remain zero vectors. For the head positions, a similar arguments shows that after the first iterations, all $Y_i$ are set of $\mathbf{e}_1$.

We next assume that up to a certain iteration $\kappa-1$, the matrix variables correctly encode a configuration of $T$ on $w_n$ and furthermore, this configuration is reachable from the initial configuration. We next show that this remains to hold in the $\kappa$th iteration.

By induction, there will be a single $X_i$ which is instantiated with $\mathbf{e}_1$. Let use assume that this $X_q$. All other $X_i$ are instantiated with the zero vector. Furthermore, $Y_j=\mathbf{e}_{i_j}$ for some $i_j\in[n+2]$. 

Suppose that
$\Delta_Q(q,b_{i_1},\ldots,b_{i\ell})=p$ and
$\Delta_j(q,b_{i_1},\ldots,b_{i\ell})=d_j$. Then, inspecting the expressions $e_{X_i}$, all $X_{q_i}$ with $q_i\neq p$ will be replaced by the zero vector. The reason is that for $X_i$ to be replaced by $\mathsf{min}$ (i.e., $\mathbf{e}_1$) when $T$ is in state $q$,
there must be a transition $\Delta_Q(q,b_{i_1}',\ldots,b_{i_\ell}')=q_i$
and such that the $b_{i_j}'$ corresponds to the positions encoded by
the $Y_i$'s. In particular, $b_{i_1},\ldots,b_{i\ell}$ and 
$b_{i_1}',\ldots,b_{i_\ell}'$ must have $\rhd$ and $\lhd$ at the same positions, and since the only remaining symbol is $0$, $b_{i_1},\ldots,b_{i\ell}$ and $b_{i_1}',\ldots,b_{i\ell}'$ must agree.
This in turn would imply that there are two possible states $p$ and $q_i$ from $q$ while reading $b_{i_1},\ldots,b_{i\ell}$. This is impossible since $T$ is deterministic.

If we next consider the expressions $e_{Y_i}$ for $i\in[\ell]$, then a similar argument shows that at most one of the terms in the second part in $e_{Y_i}$ can replace $Y_i$ with $\mathsf{move}_d\cdot Y_i$. By defining of $\mathsf{move}_d$ in terms of the predecessor or successor matrix (depending on whether $d=\gets$ or $d=\to$, respectively), and given that $Y_i$ corresponds to a canonical vector, say $\mathbf{e}_{i_s}$, then $\mathsf{move}_d\cdot Y_i$ will replace $Y_i$
with either $\mathbf{e}_{i_s-1}$ or  $\mathbf{e}_{i_s+1}$. We note that when $Y_i$ is $\mathbf{e}_1$ or $\mathbf{e}_{n+2}$, $d$ must necessarily be $\to$ or $\gets$, respectively, since $T$ does not move beyond the end markers. 

Hence, all combined we see that after the $\kappa$the iteration, $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_\ell$ indeed correspond to the next configuration of $T$.

We now remark that $w$ will be zero unless in one of the iterations populates $X_m$ with $\mathbf{e}_{1}$, i.e., the $T$ is in the final state. By assumption, $T$ will continue to be in the final state from that point on, and thus after perform our $(n+2)^k$, $w$ will remain $1$. If no final state is encountered, $w$ remains $0$, as desired.
\end{proof}
