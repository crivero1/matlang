%!TEX root = /Users/fgeerts/Documents/MLforloops/pods/main.tex
The real work begins here.

\section{ARA}
\input{./sections/app-ara.tex}
\section{\lang$(\sum,\prod)$}
\input{./sections/app-ml-sum.tex}
\section{PALU}
\input{./sections/app-palu.tex}
\section{Determinant and inverse}
\input{./sections/app-inverse.tex}
\section{Logspace Stuff}

\section*{Simulating Logspace Turing machine on input $o^n$.}

We are given a logspace Turing Machine $T=\left(Q,\{0\},\ell,\rhd,\lhd,\Delta,q_0,q_m\right)$ where $Q=\{q_1,\ldots,q_m\}$ are the states, $\ell$ denotes the number of heads, $q_0$ and $q_m$ denote initial and final state, respectively, $\{0\}$ is the tape alphabet, and $\rhd$ and $\lhd$ are special symbols denoting the beginning and the end of the tape, respectively. Finally,
$\Delta=(\Delta_Q,\Delta_1,\ldots,\Delta_\ell)$ is the transition function of $T$, where $\Delta_Q:Q\times \{\rhd,0,\lhd\}^\ell\mapsto Q$ and for $i\in[\ell]$,
$\Delta_i:Q\times  \{\rhd,0,\lhd\}^\ell\mapsto\{\gets,\to\}$. In other words, when $T$ is in state $q$ and the $\ell$ heads read symbols $b_1,\ldots,b_\ell$, $\Delta_Q(q,b_1,\ldots,b_\ell)$ indicates to which state $T$ will transition, and moreover, $\Delta_i(q,b_1,\ldots,b_\ell)$ says in which direction (left or right) the $i$th head will move. We assume that when $T$ is in the initial state $q_0$ all heads point to the first position (i.e., they all read symbol $\rhd$). 

In our setting, the tape contents will always be of the form $w_n:=\rhd 0^n \lhd$ for some $n\in\mathbb{N}$. As usual, $T$ cannot move beyond the begin and end markers, $\rhd$ and $\lhd$, respectively. We assume that $T$ accepts or rejects the input $w_n$ using $\mathcal{O}(n^k)$ steps. In other words, there exists a constant $c$ such that $T$ runs in at most $cn^k$ steps. For simplicity, we assume that $T$ runs for at most $n^{k+1}$ steps. For technical reasons, that will become clear below, we assume that once $T$ reaches the final state $q_m$, $T$ will further transition but only to the state $q_m$. That is, $\Delta$ contains transitions that make $T$ loop in $q_m$.

\begin{proposition}
Given a logspace Turing machine $T$ with $m$ states, $\ell$ heads and which runs on $w_n=\rhd 0^n \lhd$ in time $n^{k+1}$, for $n\in\mathbb{N}$, there exists (i)~a $\mathsf{MATLANG}$ 
schema $\mathcal{S}=(\mathcal{M},\textsf{size})$ where $\mathcal{M}$ consists matrix variables\footnote{We also need a finite number of auxiliary variables, these will be specified in the proof.} $X_1,\ldots,X_m,Y_1,\ldots,Y_\ell, v_1,\ldots,v_{k+1}$ and $w$ with
$\mathsf{size}(V)=\alpha\times 1$ for all $V\in\mathcal{M}$ and $V\neq w$ and $\mathsf{size}(w)=1\times 1$; and (ii)~a $\mathsf{MATLANG}$ expression $e_T$ over $\mathcal{S}$ such that for the instance $I=(\mathcal{D},\textsf{mat})$ over $\mathcal{S}$ with $\mathcal{D}(\alpha)=n+2$ and $\mathsf{mat}(V)=\left(\begin{smallmatrix}0\\
0\\\vdots\\0\end{smallmatrix}\right)$ for all $V\in\mathcal{M}$ and
$\mathsf{mat}(w)=[0]$, we have that 
$e_T(I)=[1]$ if $T$ accepts $w_n$ and  $e_T(I)=[0]$ otherwise.
\end{proposition}
\begin{proof}
We start by explaining the semantics of the matrix variables in $\mathcal{M}$. The variables $X_1,\ldots,X_m,Y_1,\allowbreak\ldots,Y_\ell$ will be used inside for loops and will be updated using \textsf{MATLANG} expressions. Initially, all these matrix variables are instantiated with the zero column vector, as described by the instance $I$. 

With each state $q_i\in Q$ we associate matrix variable $X_i$.
Then, $T$ is in state $q_i$ when
 $X_i=\left(\begin{smallmatrix}1\\
0\\\vdots\\0\end{smallmatrix}\right)$, otherwise $X_i=\left(\begin{smallmatrix}0\\
0\\\vdots\\0\end{smallmatrix}\right)$.	Similarly, with each head $i\in[\ell]$ we
associate matrix variable $Y_i$. When the $i$th head points at position $j$ in $w_n$,
then $Y_i=\mathsf{e}_j$, i.e., it is the $j$th canonical column vector. We remark that since the dimensions is $n+2$ and $T$ cannot change the input word $w_n$, the $n+2$ canonical vectors suffice to indicate all positions in $w_n$ (which is of length $n+2$).

The variables $v_1,\ldots,v_{k+1}$ represent $k+1$ canonical vectors  which are use to iterate in for loops. By iterating over then, we can perform $(n+2)^{k+1}$ iterations, which suffices for simulating the $n^{k+1}$ steps used by $T$ on input $w_n$.

Finally, the variable $w$ is used for the output of $e_T$. It contains a scalar and will hold $1$ if $w_n$ is accepted by $T$ and $0$ otherwise.

The expression $e_T$ uses some subexpressions in $\mathsf{MATLANG}$ which use some auxiliary variables.
As a consequence, $e_T$ is an expression defined over an extended schema $\mathcal{S}'$. Hence, the instance $I$ in the statement of the Proposition is in fact an instance $I'$ of $\mathcal{S}'$ which
coincides with $I$ on $\mathcal{S}$ and in which the auxiliary matrix variables are all instantiated with zero vectors or matrices, depending on their size.

We list the used subexpressions next and explicitly denote the auxiliary matrix variables:
\begin{itemize}
	% \item $\mathsf{max}(z,Z)$, an expression over auxiliary variables $z$ and $Z$ with $\mathsf{size}(z)=\mathsf{size}(Z)=\alpha\times 1$. On input $I'$ with
	% $\mathsf{mat}(z)=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$,
	%  $\mathsf{max}(I)=\mathbf{e}_{n+2}$.
	\item $\mathsf{pred}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$, and $\mathsf{mat}(Z')$ the zero $(n+2)\times (n+2)$ matrix,
	 $\mathsf{pred}(I')$ returns an $(n+2)\times (n+2)$ matrix such that 
	 
	 $$\mathsf{pred}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i-1} & \text{if $i>1$}\\
	 \mathbf{0} & \text{if $i=1$}.
	\end{cases}
	$$
	In other words, $\mathsf{pred}$ defines a predecessor relation among canonical vectors of dimension $n+2$.
	 \item $\mathsf{succ}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$, and $\mathsf{mat}(Z')$ the zero $(n+2)\times (n+2)$ matrix,
	 $\mathsf{succ}(I')$ returns an $(n+2)\times (n+2)$ matrix such that 
	 
	 $$\mathsf{succ}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i+1} & \text{if $i<n+2$}\\
	 \mathbf{0} & \text{if $i=n+2$}.
	\end{cases}
	$$
	In other words, $\mathsf{succ}$ defines a successor relation among canonical vectors.
	\item $\textsf{ismin}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $(n+2)\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$	$$\mathsf{ismin}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_1$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\textsf{ismax}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and 
	and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $(n+2)\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$
	
	$$\mathsf{ismax}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_{n+2}$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\mathsf{min}(z,Z,z',Z',z'',Z'')$, an expressions with
	auxiliary variables $z$, $z'$, $z''$, $Z$, $Z'$ and $Z''$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(z'')=\mathsf{size}(Z)=\mathsf{size}(Z'')\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	matrix variables instantiated with zero vectors (or matrix for $Z'$),
 	 $\mathsf{min}(I')=\mathbf{e}_1$. 
	 		\item We additionally define, based on the previous expressions, 
			$$\mathsf{test}_b(v,z,Z,z',Z'):=\begin{cases} \mathsf{ismin}(v,z,Z,z,Z') & \text{if $b=\rhd$}\\
     \mathsf{ismax}(v,z,Z,z,Z') & \text{if $b=\lhd$}\\
	 (1-\mathsf{ismin}(v,z,Z,z',Z'))(1-\mathsf{ismax}(v,z,Z,z',Z')) & \text{if $b=0$}.
		\end{cases}.$$
	When evaluated on $I'[v\gets\mathbf{v}]$, $\mathsf{test}_b(I'[v\gets\mathbf{v}])$ will be $1$ when either
	$\mathbf{v}=\mathbf{e}_{1}$ and $b=\rhd$ (first position)
	$\mathbf{v}=\mathbf{e}_{n+2}$ and $b=\lhd$ (last position),
	$\mathbf{v}\neq \mathbf{e}_{1}$ and $\mathbf{v}\neq \mathbf{e}_{n+2}$  and $b=0$ (not first or last position). We use this expression below to check whether the heads are consistent with the symbols on the tape.
	
	
 \item Finally, we define
 $$
 \mathsf{move}_d(z,Z,z',Z'):=\begin{cases}
 e_{\mathsf{pred}}(z,Z,z',Z) & \text{if $d=\gets$}\\
  e_{\mathsf{succ}}(z,Z,z',Z) & \text{if $d=\to$}. 
 \end{cases},
 $$
 $\mathsf{move}_d(I')$ will simply return the predecessor matrix when $d=\gets$ and the successor matrix when $d=\to$. This expression will be used to move the heads.
\end{itemize}
We thus see that we only need $z,z',z'',Z,Z',Z''$ as auxiliary variables and these can be re-used for every occurrence of the subexpressions in $e_T$. From now one, we omit the auxiliary variables from the description of $e_T$.

We define the expression $e_T$, as follows\footnote{In the expression I uses $;w$ to indicate the output variable. That is, the for loops updates all instances for $X_1,\ldots,X_m,Y_1,\ldots,Y_\ell$ and $w$, but the result of the expression is only what is in the instance corresponding to $w$. We can simulate this again if we allow constant dimensional canonical basis vectors in $\mathsf{MATLANG}$.}:
$$
e_T:= \mathsf{for\,} v_1,\ldots,v_{k+1},X_1,\ldots,X_m,Y_1,\ldots,Y_\ell; w.(e_w,e_{X_1},\ldots,e_{X_m},e_{Y_1},\ldots,e_{Y_\ell}),
$$
with 
\begin{align*}\allowdisplaybreaks
	e_w&:=\mathsf{ismin}(X_m)\\
	e_{X_1}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+ \sum_{\substack{q,b_1,\ldots,b_\ell\\
	\Delta_Q(q,b_1,\ldots,b_\ell)=q_1}} \!\!\!\!\!\!\!\!\! \textsf{ismin}(X_q)\left(\prod_{j=1}^\ell \mathsf{test}_{b_j}(Y_j)\right)\mathsf{min}\\
	e_{X_i}&:= \sum_{\substack{q,b_1,\ldots,b_\ell\\
	\Delta_Q(q,b_1,\ldots,b_\ell)=q_i}}\!\!\!\!\!\!\!\!\! \textsf{ismin}(X_q)\left(\prod_{j=1}^\ell \mathsf{test}_{b_j}(Y_j)\right)\mathsf{min} \quad \text{for $i\neq 1$}\\
	e_{Y_i}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{q,b_1,\ldots,b_\ell\\
	\Delta_i(q,b_1,\ldots,b_\ell)=d}}\!\!\!\!\!\!\!\!\! \textsf{ismin}(X_q)\left(\prod_{j=1}^\ell \mathsf{test}_{b_j}(Y_j)\right)\mathsf{move}_d\cdot Y_i
\end{align*}

The correctness of $e_T$ is now readily verified. We do this by induction on the number of iterations in the for loop. We note that initially, all variables are assigned zero vectors and values (for $w$). 

At the start of the run of $T$, we are in state $q_1$ and all heads point to the first position. We argue that after the first 
iterations, i.e., when $v_i=\mathbf{e}_1$ for $i\in[k+1]$, we indeed have that $X_1=\mathbf{e_1}$, $X_j=\mathbf{0}$ for $j\neq 1$, and $Y_j=\mathbf{e}_1$ for $j\in[\ell]$. Indeed, in the expression for $e_{X_1}$ the test $\prod_{j=1}^{k+1} \textsf{ismin}(v_i)$ will return $1$ and hence $X_1$ is replaced by $\mathsf{min}=\mathbf{e}_1$. Since all $X_i$ are initially zero, $\mathsf{ismin}(X_i)$ evaluate to zero for all $i\in[m]$ so the second term in $e_{X_1}$ adds the zero vector to $\mathbf{e}_1$ and thus $X_1$ remains $\mathbf{e_1}$.
Similarly, $e_{X_j}$ for $j\neq 1$ will leave $X_j$ unchanged, so these remain zero vectors. For the head positions, a similar arguments shows that after the first iterations, all $Y_i$ are set of $\mathbf{e}_1$.

We next assume that up to a certain iteration $\kappa-1$, the matrix variables correctly encode a configuration of $T$ on $w_n$ and furthermore, this configuration is reachable from the initial configuration. We next show that this remains to hold in the $\kappa$th iteration.

By induction, there will be a single $X_i$ which is instantiated with $\mathbf{e}_1$. Let use assume that this $X_q$. All other $X_i$ are instantiated with the zero vector. Furthermore, $Y_j=\mathbf{e}_{i_j}$ for some $i_j\in[n+2]$. 

Suppose that
$\Delta_Q(q,b_{i_1},\ldots,b_{i\ell})=p$ and
$\Delta_j(q,b_{i_1},\ldots,b_{i\ell})=d_j$. Then, inspecting the expressions $e_{X_i}$, all $X_{q_i}$ with $q_i\neq p$ will be replaced by the zero vector. The reason is that for $X_i$ to be replaced by $\mathsf{min}$ (i.e., $\mathbf{e}_1$) when $T$ is in state $q$,
there must be a transition $\Delta_Q(q,b_{i_1}',\ldots,b_{i_\ell}')=q_i$
and such that the $b_{i_j}'$ corresponds to the positions encoded by
the $Y_i$'s. In particular, $b_{i_1},\ldots,b_{i\ell}$ and 
$b_{i_1}',\ldots,b_{i_\ell}'$ must have $\rhd$ and $\lhd$ at the same positions, and since the only remaining symbol is $0$, $b_{i_1},\ldots,b_{i\ell}$ and $b_{i_1}',\ldots,b_{i\ell}'$ must agree.
This in turn would imply that there are two possible states $p$ and $q_i$ from $q$ while reading $b_{i_1},\ldots,b_{i\ell}$. This is impossible since $T$ is deterministic.

If we next consider the expressions $e_{Y_i}$ for $i\in[\ell]$, then a similar argument shows that at most one of the terms in the second part in $e_{Y_i}$ can replace $Y_i$ with $\mathsf{move}_d\cdot Y_i$. By defining of $\mathsf{move}_d$ in terms of the predecessor or successor matrix (depending on whether $d=\gets$ or $d=\to$, respectively), and given that $Y_i$ corresponds to a canonical vector, say $\mathbf{e}_{i_s}$, then $\mathsf{move}_d\cdot Y_i$ will replace $Y_i$
with either $\mathbf{e}_{i_s-1}$ or  $\mathbf{e}_{i_s+1}$. We note that when $Y_i$ is $\mathbf{e}_1$ or $\mathbf{e}_{n+2}$, $d$ must necessarily be $\to$ or $\gets$, respectively, since $T$ does not move beyond the end markers. 

Hence, all combined we see that after the $\kappa$the iteration, $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_\ell$ indeed correspond to the next configuration of $T$.

We now remark that $w$ will be zero unless in one of the iterations populates $X_m$ with $\mathbf{e}_{1}$, i.e., the $T$ is in the final state. By assumption, $T$ will continue to be in the final state from that point on, and thus after perform our $(n+2)^k$, $w$ will remain $1$. If no final state is encountered, $w$ remains $0$, as desired.
\end{proof}

\section*{Simulating Linear Space Functions}
We consider  deterministic Turing Machines  (TM) $T$ consisting of $\ell$ read-only input tapes, denoted by $R_1,\ldots,R_\ell$,
a work tape, denoted by $W$, and a write-only output tape, denoted by $O$. The TM $T$ has a set $Q$ of $m$
states, denoted by $q_0,\ldots,q_m$. We assume that $q_0$ is the initial state and $q_m$ is the accepting state.
The input and tape alphabet are $\Sigma=\{0,1\}$ and $\Gamma=\Sigma\cup\{\rhd,\lhd\}$, respectively. The special symbol $\rhd$ denotes the beginning of each of the tapes, the symbol $\lhd$ denotes the end of the $\ell$ input tapes. The transition function $\Delta$ is defined as usual, i.e., 
$\Delta:Q\times \Gamma^{\ell+2} \to Q\times \Gamma^{2}\times \{\leftarrow,\sqcup,\rightarrow\}^{\ell+2}$ such that $\Delta(q,(a_1,\ldots,a_{\ell},b,c))=\bigl(q',(b',c'),(\mathsf{d}_1,\ldots,\mathsf{d}_{\ell+2})\bigr)$ with $\mathsf{d}_i\in \{\leftarrow,\sqcup,\rightarrow\}$, means that when $T$ is in state $q$ and the $\ell+2$ heads on the tapes read symbols $a_1,\ldots,a_{\ell},b,c$, respectively, then $T$ transitions to state $q'$, writes $b',c'$ on the work and output tapes, respectively, at the position to which the work and output tapes' heads points at, and finally moves the heads on the tapes according $\mathsf{d}_1,\ldots,\mathsf{d}_{\ell+2}$. More specifically, $\leftarrow$  indicates a move to the left, 
$\rightarrow$ a move to the right, and finally, $\sqcup$ indicates that the head does not move.

We assume that $\Delta$ is defined such that it ensures that on none of the tapes, heads can move beyond the leftmost marker $\rhd$. Furthermore, the tapes $R_1,\ldots,R_\ell$ are treated as read-only the head cannot move beyond the end markers $\lhd$ on each of the input tapes. Similarly, $\Delta$ ensures that the output tape $O$ is write only, i.e., its head cannot move to the left.  We also assume that $\Delta$ does not change $\rhd$ or write $\lhd$ on the work and output tape.

A configuration of $T$ is defined in the usual way. That is, a configuration the input tapes is of the form
$\rhd w_1qw_2\lhd$ with $w_1,w_2\in\Sigma^*$ and represents that the current tape content is $\rhd w_1w_2\lhd$, $T$ is state $q$ and the head is position on the first symbol of $w_2$. Similarly, a configuration of the work and output tape are represented by $\rhd w_1qw_2$. A configuration of $T$ is consists of configurations for all tapes. Given two configurations $c_1$ and $c_2$, we say that $c_1$
transitions to $c_2$ if $c_2$ is the result of applying the transition function based on the information in $c_1$. Given $\ell$ input words $w_1,\ldots,w_\ell\in\Sigma^*$, the initial configuration is given by
 $\bigl(\rhd q_0 w_1\lhd,\rhd q_0 w_2\lhd,\ldots, \rhd q_0 w_\ell\lhd,\rhd q_0,\rhd q_0\bigr)$ and the accepting configuration is given by $\bigl(\rhd q_m w_1\lhd,\rhd q_m w_2\lhd,\ldots, \rhd q_m w_\ell\lhd,\rhd q_m,\rhd q_m w\bigr)$ for some $w\in\Sigma^*$. We say that $T$ computes the function $f:(\Sigma^*)^{\ell}\to\Sigma^*$ if for every $w_1,\ldots,w_\ell\in\Sigma^*$, $T$ transitions (transitively) from the initial configuration to the accepting configuration such that the configuration on the output tape is
 given by $\rhd q_m f(w_1,\ldots,w_\ell)$.

We assume that once $T$ reaches an accepting configuration it stays indefinitely in that configuration (i.e., it loops). We further assume that $T$ only reaches an accepting configuration when all its input
words have the same size. Furthermore, for every such input words, $T$ will reach an accepting configuration.


We say that $T$ is a \textit{linear space machine} when it reaches an accepting configuration 
on inputs of size $n$ by using $sn$ space on its work tape and additionally needs  $\mathcal{O}(n^k)$ steps to do so. A \textit{linear input-output function} is a function of the form $f=\bigcup_{n\geq 0} f_n:(\Sigma^n)^\ell\to\Sigma^n$. In other words, for every $\ell$ words of the same size $n$ it returns a word of size $n$. We say that a linear input-output function is a linear space input-output function if
there exists a linear space machine  $T$ that on input $w_1,\ldots,w_\ell\in\Sigma^n$ puts
$f_n(w_1,\ldots,w_\ell)$ on its the output tape when (necessarily) reaching an accepting configuration.

% We say that a function
% $f:\underbrace{\Sigma^n\times\cdots \times \Sigma^n}_{\text{$\ell$ times}}\to \Sigma^n$ is computable by a linear space machine $T$ if when $T$ is run on input $w_1,\ldots,w_\ell$ it halts and has $f(w_1,\ldots,w_\ell)$ on its output tape. We say that $f$ is a \textit{linear space poly function} if it is computable by a linear space TM $T$ which in addition runs in polynomial time, i.e., it hals in at most
% $\mathcal{O}(n^k)$ steps for a certain $k$ on any inputs $w_1,\ldots,w_\ell$ of size $n$.
\begin{proposition}
Let $f=\bigcup_{n\geq 0}f_n:(\Sigma^n)^\ell\to \Sigma^n$ be a linear space input-ouput function computed by a linear space  machine $T$ with $m$ states, $\ell$ input tapes, which consumes $sn$ space and runs in $\mathcal{O}(n^{k-1})$ time on inputs of size $n$. Then  exists (i)~a $\mathsf{MATLANG}$ 
schema $\mathcal{S}=(\mathcal{M},\textsf{size})$ where $\mathcal{M}$ consists matrix variables\footnote{We also need a finite number of auxiliary variables, these will be specified in the proof.} $Q_1,\ldots,Q_m,R_1,\ldots,R_\ell,H_1,\ldots,H_\ell,W_1,\ldots,W_s,H_{W_1},\ldots,H_{W_s},O,H_O, v_1,\ldots,v_{k}$  with
$\mathsf{size}(V)=\alpha\times 1$ for all $V\in\mathcal{M}$; and (ii)~a $\mathsf{MATLANG}$ expression $e_f$ over $\mathcal{S}$ such that for the instance $I=(\mathcal{D},\textsf{mat})$ over $\mathcal{S}$ with $\mathcal{D}(\alpha)=n$ and 
$$\mathsf{mat}(R_i)=\mathsf{vec}(w_i)\in \mathbb{R}^n\text{ and } \mathsf{mat}(Q_i)=\mathsf{mat}(H_i)=\mathsf{mat}(W_i)=\mathsf{mat}(H_{W_i})=\mathsf{mat}(O)=\mathsf{mat}(H_O)=[0]\in \mathbb{R}^n, 
$$
for words $w_1,\ldots,w_\ell\in\Sigma^n$ and such that $\mathsf{vec}(w_i)$ is the $n\times 1$-vector encoding the symbols in $w_i$, we have that $e_f(I)=\mathsf{vec}(f_n(w_1,\ldots,w_n))$.
\end{proposition}
\begin{proof}
We start by explaining the semantics of the matrix variables in $\mathcal{M}$. The variables $R_1,\ldots,R_\ell$ will hold the input vectors, $W_1,\ldots,W_s$ will hold the contents of the work
tape, and $O$ will hold the contents of the output tape. The vectors corresponding to the work and output tape are initially set to the zero vector.

 With each tape we associate a matrix variable encoding the position of the head. More specifically, $H_1,\ldots,H_\ell$ correspond to the input tape heads,
$H_{W_1},\ldots, H_{W_s}$ are the heads for the work tape (note that the size of work tape is $sn$ we encode this by $s$ tapes of size $n$), and $H_O$ is the head of the output tape. All these vectors are initialised with the zero vector. Later on, these vectors have will be zero except for the position to which the head points to, which will hold the value $1$.

%
% ,Y_1,\allowbreak\ldots,Y_\ell$ will be used inside for loops and will be updated using \textsf{MATLANG} expressions. Initially, all these matrix variables are instantiated with the zero column vector, as described by the instance $I$.
Furthermore, with each state $q_i\in Q$ we associate matrix variable $Q_i$.
Then, $T$ is in state $q_i$ when
 $Q_i=\left(\begin{smallmatrix}1\\
0\\\vdots\\0\end{smallmatrix}\right)$, otherwise $Q_i=\left(\begin{smallmatrix}0\\
0\\\vdots\\0\end{smallmatrix}\right)$.	

The variables $v_1,\ldots,v_{k}$ represent $k$ canonical vectors  which are use to iterate in for loops. By iterating over then, we can perform $(n)^{k}$ iterations, which suffices for simulating the $\mathcal{O}(n^{k-1})$ steps used by $T$ to reach an accepting configuration. (We recall that $T$ loops when reaching an accepting configuration.) In the following
we assume that $n$ is sufficiently large, say $n\geq N$, such that $T$ runs in at most $cn^{k-1}$ steps. We deal with
the finite number of cases $n<N$ later on.

The expression $e_f$ uses some subexpressions in $\mathsf{MATLANG}$ which use some auxiliary variables.
As a consequence, $e_f$ is an expression defined over an extended schema $\mathcal{S}'$. Hence, the instance $I$ in the statement of the Proposition is in fact an instance $I'$ of $\mathcal{S}'$ which
coincides with $I$ on $\mathcal{S}$ and in which the auxiliary matrix variables are all instantiated with zero vectors or matrices, depending on their size.

We list the used subexpressions next and explicitly denote the auxiliary matrix variables:
\begin{itemize}
	% \item $\mathsf{max}(z,Z)$, an expression over auxiliary variables $z$ and $Z$ with $\mathsf{size}(z)=\mathsf{size}(Z)=\alpha\times 1$. On input $I'$ with
	% $\mathsf{mat}(z)=\mathsf{mat}(Z)$ the zero column vector of dimension $n+2$,
	%  $\mathsf{max}(I)=\mathbf{e}_{n+2}$.
	\item $\mathsf{pred}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n$, and $\mathsf{mat}(Z')$ the zero $n\times n$ matrix,
	 $\mathsf{pred}(I')$ returns an $n\times n$ matrix such that 
	 
	 $$\mathsf{pred}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i-1} & \text{if $i>1$}\\
	 \mathbf{0} & \text{if $i=1$}.
	\end{cases}
	$$
	In other words, $\mathsf{pred}$ defines a predecessor relation among canonical vectors of dimension $n$.
	 \item $\mathsf{succ}(z,Z,z',Z')$, and expression over auxiliary variables $z$, $z'$, $Z$ and $Z'$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(Z)=\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	$\mathsf{mat}(z)=\mathsf{mat}(z')=\mathsf{mat}(Z)$ the zero column vector of dimension $n$, and $\mathsf{mat}(Z')$ the zero $n\times n$ matrix,
	 $\mathsf{succ}(I')$ returns an $n\times n$ matrix such that 
	 
	 $$\mathsf{succ}(I')\mathbf{e}_i:=\begin{cases} 
	 \mathbf{e}_{i+1} & \text{if $i<n$}\\
	 \mathbf{0} & \text{if $i=n$}.
	\end{cases}
	$$
	In other words, $\mathsf{succ}$ defines a successor relation among canonical vectors.
	\item $\textsf{ismin}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $n\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$	$$\mathsf{ismin}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_1$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\textsf{ismax}(v,z,Z,z',Z)$ with auxiliary variables $z$, $z'$, $Z$ and $Z'$ as before, and 
	and $v$ is one of the (vector) variables in $\mathcal{M}$. For an $n\times 1$ vector $\mathbf{v}$, on input $I'[v\gets \mathbf{v}]$
	
	$$\mathsf{ismax}(I'[v\gets\mathbf{v}]):=\begin{cases} 1 & \text{if $\mathbf{v}=\mathbf{e}_{n+2}$}\\
		0 & \text{otherwise}.
		\end{cases}$$
	\item $\mathsf{min}(z,Z,z',Z',z'',Z'')$, an expressions with
	auxiliary variables $z$, $z'$, $z''$, $Z$, $Z'$ and $Z''$ with $\mathsf{size}(z)=\mathsf{size}(z')=\mathsf{size}(z'')=\mathsf{size}(Z)=\mathsf{size}(Z'')\alpha\times 1$ and $\mathsf{size}(Z')=\alpha\times\alpha$. On input $I'$ with 
	matrix variables instantiated with zero vectors (or matrix for $Z'$),
 	 $\mathsf{min}(I')=\mathbf{e}_1$. 
	 		\item We additionally define, based on the previous expressions, 
			$$\mathsf{test}_b(v,z,Z,z',Z'):=\begin{cases} \mathsf{ismin}(v,z,Z,z,Z') & \text{if $b=\rhd$}\\
     \mathsf{ismax}(v,z,Z,z,Z') & \text{if $b=\lhd$}\\
	 (1-\mathsf{ismin}(v,z,Z,z',Z'))(1-\mathsf{ismax}(v,z,Z,z',Z')) & \text{if $b=0$}.
		\end{cases}.$$
	When evaluated on $I'[v\gets\mathbf{v}]$, $\mathsf{test}_b(I'[v\gets\mathbf{v}])$ will be $1$ when either
	$\mathbf{v}=\mathbf{e}_{1}$ and $b=\rhd$ (first position)
	$\mathbf{v}=\mathbf{e}_{n+2}$ and $b=\lhd$ (last position),
	$\mathbf{v}\neq \mathbf{e}_{1}$ and $\mathbf{v}\neq \mathbf{e}_{n+2}$  and $b=0$ (not first or last position). We use this expression below to check whether the heads are consistent with the symbols on the tape.
	
	
 \item Finally, we define
 $$
 \mathsf{move}_d(z,Z,z',Z'):=\begin{cases}
 e_{\mathsf{pred}}(z,Z,z',Z) & \text{if $d=\gets$}\\
  e_{\mathsf{succ}}(z,Z,z',Z) & \text{if $d=\to$}. 
 \end{cases},
 $$
 $\mathsf{move}_d(I')$ will simply return the predecessor matrix when $d=\gets$ and the successor matrix when $d=\to$. This expression will be used to move the heads.
\end{itemize}
We thus see that we only need $z,z',z'',Z,Z',Z''$ as auxiliary variables and these can be re-used for every occurrence of the subexpressions in $e_T$. From now one, we omit the auxiliary variables from the description of $e_f$.

We define the expression $e_T$, as follows\footnote{In the expression I uses $;w$ to indicate the output variable. That is, the for loops updates all instances for $X_1,\ldots,X_m,Y_1,\ldots,Y_\ell$ and $w$, but the result of the expression is only what is in the instance corresponding to $w$. We can simulate this again if we allow constant dimensional canonical basis vectors in $\mathsf{MATLANG}$.}:
$$
e_f:= \mathsf{for\,} v_1,\ldots,v_{k},Q_1,\ldots,Q_m,H_1,\ldots,H_\ell,H_{W_1},\ldots,H_{W_\ell},H_0, W_1,\ldots,W_\ell; O.(e_O,e_{Q_1},\ldots,e_{Q_m},e_{H_1},\ldots,e_{H_\ell},),
$$
with 
\begin{align*}\allowdisplaybreaks
	e_w&:=\mathsf{ismin}(X_m)\\
	e_{Q_1}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+ \sum_{\substack{(q_i,a_1,\ldots,a_\ell,b,c)\\
	\Delta(q_i,a_1,\ldots,a_\ell,b,c)=(q_1,\star)}} \!\!\!\!\!\!\!\!\! \textsf{ismin}(Q_i)\left(\prod_{j=1}^\ell \mathsf{test}_{a_j}(R_j^t\cdot H_i)\right)\mathsf{test}_{c}(O^t\cdot H_O)
	\left(\sum_{i=1}^s \mathsf{test}_{b}(W_i^t\cdot H_{W_i})\right)\mathsf{min}\\
	e_{Q_j}&:=\sum_{\substack{(q_i,a_1,\ldots,a_\ell,b,c)\\
	\Delta(q_i,a_1,\ldots,a_\ell,b,c)=(q_j,\star)}} \!\!\!\!\!\!\!\!\! \textsf{ismin}(Q_i)\left(\prod_{j=1}^\ell \mathsf{test}_{a_j}(R_j^t\cdot H_i)\right)\mathsf{test}_{c}(O^t\cdot H_O)
	\left(\sum_{i=1}^s \mathsf{test}_{b}(W_i^t\cdot H_{W_i})\right)\mathsf{min}
	 \quad \text{for $j\neq 1$}\\
	e_{H_i}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d_i},\star)}}\!\!\!\!\!\!\!\!\! \textsf{isconf}(q,a_1,\ldots,a_\ell,b,d)\cdot\mathsf{move}_d\cdot H_i\\
	e_{H_{W_i}}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d_i},\star)}}\!\!\!\!\!\!\!\!\! \textsf{isconf}(q,a_1,\ldots,a_\ell,b,d)\cdot\mathsf{move}_d\cdot H_i\\
	e_{H_O}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d_i},\star)}}\!\!\!\!\!\!\!\!\! \textsf{isconf}(q,a_1,\ldots,a_\ell,b,d)\cdot\mathsf{move}_d\cdot H_i\\
	e_{O}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d_i},\star)}}\!\!\!\!\!\!\!\!\! \textsf{isconf}(q,a_1,\ldots,a_\ell,b,d)\cdot\mathsf{move}_d\cdot H_i\\
	e_{W_i}&:=\left(\prod_{j=1}^{k+1} \textsf{ismin}(v_i)\right)\cdot\mathsf{min}
	+\sum_{\substack{(q,a_1,\ldots,a_\ell,b,d)\\
	\Delta(q,a_1,\ldots,a_\ell,b,c)=(\star,\mathsf{d_i},\star)}}\!\!\!\!\!\!\!\!\! \textsf{isconf}(q,a_1,\ldots,a_\ell,b,d)\cdot\mathsf{move}_d\cdot H_i
\end{align*}

The correctness of $e_T$ is now readily verified. We do this by induction on the number of iterations in the for loop. We note that initially, all variables are assigned zero vectors and values (for $w$). 

At the start of the run of $T$, we are in state $q_1$ and all heads point to the first position. We argue that after the first 
iterations, i.e., when $v_i=\mathbf{e}_1$ for $i\in[k+1]$, we indeed have that $X_1=\mathbf{e_1}$, $X_j=\mathbf{0}$ for $j\neq 1$, and $Y_j=\mathbf{e}_1$ for $j\in[\ell]$. Indeed, in the expression for $e_{X_1}$ the test $\prod_{j=1}^{k+1} \textsf{ismin}(v_i)$ will return $1$ and hence $X_1$ is replaced by $\mathsf{min}=\mathbf{e}_1$. Since all $X_i$ are initially zero, $\mathsf{ismin}(X_i)$ evaluate to zero for all $i\in[m]$ so the second term in $e_{X_1}$ adds the zero vector to $\mathbf{e}_1$ and thus $X_1$ remains $\mathbf{e_1}$.
Similarly, $e_{X_j}$ for $j\neq 1$ will leave $X_j$ unchanged, so these remain zero vectors. For the head positions, a similar arguments shows that after the first iterations, all $Y_i$ are set of $\mathbf{e}_1$.

We next assume that up to a certain iteration $\kappa-1$, the matrix variables correctly encode a configuration of $T$ on $w_n$ and furthermore, this configuration is reachable from the initial configuration. We next show that this remains to hold in the $\kappa$th iteration.

By induction, there will be a single $X_i$ which is instantiated with $\mathbf{e}_1$. Let use assume that this $X_q$. All other $X_i$ are instantiated with the zero vector. Furthermore, $Y_j=\mathbf{e}_{i_j}$ for some $i_j\in[n+2]$. 

Suppose that
$\Delta_Q(q,b_{i_1},\ldots,b_{i\ell})=p$ and
$\Delta_j(q,b_{i_1},\ldots,b_{i\ell})=d_j$. Then, inspecting the expressions $e_{X_i}$, all $X_{q_i}$ with $q_i\neq p$ will be replaced by the zero vector. The reason is that for $X_i$ to be replaced by $\mathsf{min}$ (i.e., $\mathbf{e}_1$) when $T$ is in state $q$,
there must be a transition $\Delta_Q(q,b_{i_1}',\ldots,b_{i_\ell}')=q_i$
and such that the $b_{i_j}'$ corresponds to the positions encoded by
the $Y_i$'s. In particular, $b_{i_1},\ldots,b_{i\ell}$ and 
$b_{i_1}',\ldots,b_{i_\ell}'$ must have $\rhd$ and $\lhd$ at the same positions, and since the only remaining symbol is $0$, $b_{i_1},\ldots,b_{i\ell}$ and $b_{i_1}',\ldots,b_{i\ell}'$ must agree.
This in turn would imply that there are two possible states $p$ and $q_i$ from $q$ while reading $b_{i_1},\ldots,b_{i\ell}$. This is impossible since $T$ is deterministic.

If we next consider the expressions $e_{Y_i}$ for $i\in[\ell]$, then a similar argument shows that at most one of the terms in the second part in $e_{Y_i}$ can replace $Y_i$ with $\mathsf{move}_d\cdot Y_i$. By defining of $\mathsf{move}_d$ in terms of the predecessor or successor matrix (depending on whether $d=\gets$ or $d=\to$, respectively), and given that $Y_i$ corresponds to a canonical vector, say $\mathbf{e}_{i_s}$, then $\mathsf{move}_d\cdot Y_i$ will replace $Y_i$
with either $\mathbf{e}_{i_s-1}$ or  $\mathbf{e}_{i_s+1}$. We note that when $Y_i$ is $\mathbf{e}_1$ or $\mathbf{e}_{n+2}$, $d$ must necessarily be $\to$ or $\gets$, respectively, since $T$ does not move beyond the end markers. 

Hence, all combined we see that after the $\kappa$the iteration, $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_\ell$ indeed correspond to the next configuration of $T$.

We now remark that $w$ will be zero unless in one of the iterations populates $X_m$ with $\mathbf{e}_{1}$, i.e., the $T$ is in the final state. By assumption, $T$ will continue to be in the final state from that point on, and thus after perform our $(n+2)^k$, $w$ will remain $1$. If no final state is encountered, $w$ remains $0$, as desired.
\end{proof}
