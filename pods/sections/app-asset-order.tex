\floris{This section should be placed in the appropriate place, i.e., this should be part of proofs for the last section. Let's see how the paper evolves? What is the precise result shown here?}
\thomas{Updated, I think now it's more clear what is shown here. It should still go in the appendix of the last section? To me it seems order related.}

% Assume we can only perform the restricted versions of $\ffor{v}{X}{e}$, this is, we can only do 
% $\ssum$ and $\sprod$. Note that these operators also iterate over canonical vectors
% in certain order. Since they are operations that \textit{aggregate} information, it doesn't seem
% possible to access this order explicitly and compare canonical vectors, like in the full version of $\langfor$.
% Let's see what we can do if this order is \textit{supplied}. 
We here show that if we have $e_{S_{<}}$, using $\ssum$ and $\sprod$ we
can do everything related to order (everything defined in section \ref{app:order}).
The goal of this is to have a restriction in the recursion property of $\ffor{v}{X}{e}$, this is, 
in the iteration we don't allow access to the current result.
We call this fragment \langsum extended with $\sprod$ and $e_{S_<}$, as in section \ref{subsec:langlinear}.

Supose we have $e_{S_{<}}$ (and thus $e_{S_{\leq}}$) such that the following holds:

$$
b_i^T\cdot S_{<} \cdot b_j=\begin{cases}
               1, \text{ if } i < j.\\
              \mathbf{0}, \text{ if not.}
            \end{cases}
$$
As a consequence we
can compute $\mathsf{succ}$ and $\mathsf{succ}^+$. We can define
\begin{align*}
  e_{\mathsf{min}}&:=\ssum v. \left[ \sprod w. \mathsf{succ}(w,v)\right] \times v. \\
  e_{\mathsf{max}}&:=\ssum v. \left[ \sprod w. \left( 1-\mathsf{succ}(w,v) \right) \right] \times v.
\end{align*}
Consider $S_<$ and the matrix $J$ that consist of all ones (assume they are $n\times n$). From linear algebra we know that
\[
S_<^2 \circ \left( S_{<}^2-2\times J\right) \circ \left( S_{<}^2-3\times J\right) = \begin{bmatrix}
    0 & n-1 & 0 & \cdots &  0 \\
    0 & \ddots & \ddots & \vdots & \vdots \\
    0 & \ddots & \ddots & \vdots & 0\\
    \hdotsfor{4} & n-1 \\
    0 & \cdots & \cdots & \cdots & 0 
\end{bmatrix}.
\]
Recall that $\circ$ is the Hadamard product. We can compute a matrix that has $n-1$ in
all of its entries as $\ssum v.(1-\mathsf{max}(v))\times e_{\ones}(v)\cdot e_{\ones}(v)^T$.
So, if we have $f_{/}$ we can define
$$
e_{\mathsf{Pred}}:= f_{/}\left( f_{\odot}\left(e_{S_{<}}^2, e_{S_{<}}^2-2\times (e_{\ones}(e_{S_{<}})\cdot e_{\ones}(e_{S_{<}})^T), e_{S_{<}}^2-3\times (e_{\ones}(e_{S_{<}})\cdot e_{\ones}(e_{S_{<}})^T) \right),\ssum v.(1-\mathsf{max}(v))\times e_{\ones}(v)\cdot e_{\ones}(v)^T\right) 
$$
Thus we have $e_{\mathsf{Next}}:=e_{\mathsf{Pred}}^T$.
We can now define $\mathsf{prev}(v)$ and $\mathsf{next}(v)$ as in the previous section. 
The same goes with $e_{\mathsf{getPrevMatrix}}(V)$, 
$e_{\mathsf{getNextMatrix}}(V)$, $e_{\mathsf{min}+i}$ and $e_{\mathsf{max}+i}$.


% Note that, if we have $f_{>0}$ we can define
% $$
% e_{\mathsf{Pred}}:= e_{S_{<}}- f_{>0}(e_{S_{<}}^2)
% $$
% Also $e_{\mathsf{Next}}:=e_{\mathsf{Pred}}^T$.
% We can now define $\mathsf{prev}(v)$ and $\mathsf{next}(v)$ as in the previous section. 
% The same goes with $e_{\mathsf{getPrevMatrix}}(V)$, 
% $e_{\mathsf{getNextMatrix}}(V)$, $e_{\mathsf{min}+i}$ and $e_{\mathsf{max}+i}$.

% On the other hand, supose we have $e_{\mathsf{Prev}}$
% (and thus $e_{\mathsf{Next}}$) such that the following holds:

% $$
% b_i^T\cdot \mathsf{Prev} \cdot b_j=\begin{cases}
%                1, \text{ if } i = j-1.\\
%               \mathbf{0}, \text{ if not.}
%             \end{cases},
% \hspace{2em}b_i^T\cdot \mathsf{Next} \cdot b_j=\begin{cases}
%                1, \text{ if } i=j+1.\\
%               \mathbf{0}, \text{ if not.}
%             \end{cases}.
% $$
% We can also define $\mathsf{prev}(v)$ and $\mathsf{next}(v)$.
% Note that we can compute
% $$
% e_{S_{\leq}}:=\ssum v. \left( v\cdot v^T + \ssum w. v\cdot\mathsf{next}(v)^T \right)
% $$
% Here, $(e_{S_{\leq}})_{ij}=1$ if and only if $b_i$ comes 
% before according to $\mathsf{Pred}$ and $\mathsf{Next}$, or is equal to $b_j$. As a consequence we
% can compute $\mathsf{succ}$ and $\mathsf{succ}^+$.

% We can now compute everything as we have $S_{\leq}$ and thus $S_{<}$.

% $$
% b_i^T\cdot \mathsf{Prev} \cdot b_j=\begin{cases}
%                1, \text{ if } b_i \text{ comes immediately before } b_j.\\
%               \mathbf{0}, \text{ if not.}
%             \end{cases},
% \hspace{2em}b_i^T\cdot \mathsf{Next} \cdot b_j=\begin{cases}
%                1, \text{ if } b_i \text{ comes immediately after } b_j.\\
%               \mathbf{0}, \text{ if not.}
%             \end{cases}.
% $$


% The expression in this section are intended to be used over canonical vectors.
% To have access to order, we need a matrix $S_{\leq}$ such that the following holds for 
% canonical vectors $b_i$ and $b_j$:
% $$
% b_i^T\cdot S_{\leq} \cdot b_j=\begin{cases}
%                1, \text{ if } b_i \text{ comes before } b_j\\
%               \mathbf{0}, \text{ if not.}
%             \end{cases}
% $$
% Note that $S_{\leq}$ encodes \textit{total} order. 

% Using this matrix 


% We would also like to have access to 
% \textit{local} information, this is, know if a canonical vector comes immediately before another. 
% Let's call this matrix $\mathsf{Prev}$. 
% The following must hold:
% $$
% b_i^T\cdot \mathsf{Prev} \cdot b_j=\begin{cases}
%                1, \text{ if } b_i \text{ comes immediately before } b_j\\
%               \mathbf{0}, \text{ if not.}
%             \end{cases}
% $$

% Using this matrix, we have that for a canonical vector $b_i$:
% \[
% \mathsf{Prev}\cdot b_i=\begin{cases}
%                b_{i-1}, \text{ if } i > 1 \\
%               \mathbf{0}, \text{ if } i = 1
%             \end{cases}
% \]

% Note that $\mathsf{Prev}$ can be defined using the following \langfor expression:
% $$
% e_{\mathsf{Prev}}:= \texttt{for }v,X.\quad X + \left[ (1 - \mathsf{max}(v))\times ve_{\mathsf{max}}^T - (Xe_{\mathsf{max}})\cdot e_{\mathsf{max}}^T + (Xe_{\mathsf{max}})\cdot v^T\right].
% $$

% Here, $X$ starts as 0 and thus in turn $X\cdot e_{\mathsf{max}}$, so we initiate storing $b_1$ in the last column. Next, we add a matrix that has the stored vector $Xe_{\mathsf{max}}$ (the previous canonical vector) in the column indicated by $v$ (the current canonical vector) and $v-Xe_{\mathsf{max}}$ in the last column, to replace the vector stored.
% The last iteration does nothing (sums the zero matrix).

% To get the \textit{next} relation we simply do $e_{\mathsf{Next}} = e_{\mathsf{Prev}}^T$. We have that for a canonical vector $b_i$:
% \[
% \mathsf{Next}\cdot b_i=\begin{cases}
%                b_{i+1}, \text{ if } i < n \\
%               \mathbf{0}, \text{ if } i = n
%             \end{cases}
% \]