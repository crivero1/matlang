When showing results based on induction of expressions in \langfor, it is often convenient to assume that function applications $f(e_1,\ldots,e_k)$ for $f\in\Fun_k$ are restricted to
the case when all expressions $e_1,\ldots,e_k$ have type $1\times 1$. This does not loose generality. Indeed,
for general function applications $f(e_1,\ldots,e_k)$, if we have $\ssum$, scalar product and function application on scalars (here denoted by $f_{1\times 1}$), we can simulate full function application, as follows:
 $$
f(e_1,\ldots, e_k) :=\Sigma v_i \Sigma v_j. f_{1\times 1}(v_i^T\cdot e_1\cdot v_j, \ldots ,v_i^T\cdot e_k\cdot v_j) \times v_i\cdot v_j^T.
$$

Furthermore, it also convenient at times to use the pointwise functions
$f_\odot^k:\RR^k\mapsto \RR:(x_1,\ldots,x_k)\mapsto x_1\times\cdots \cdot x_k$ and 
$f_\oplus^k:\RR^k\mapsto \RR:(x_1,\ldots,x_k)\mapsto x_1+\cdots + x_k$. In fact, it is readily observed that adding these functions does not extend the expressive power of \langfor:
\begin{lemma}
\label{lm-prod-sum}
We have that $\langforf{\emptyset} \equiv \langforf{\{f_\odot^k,f_\oplus^k \ | \ k\in \mathbf{N}\}}$.
\end{lemma}
In fact, this lemma also holds for the smaller fragments we consider.
%
We also observe that having $f_\odot^2:\RR^2\to\RR$ allows us to define scalar multiplication:
$$
e_1\times e_2 :=f_{\kprod}(\ones(e_2)^T\cdot e_1 \cdot \ones(e_2)^T, e_2).
$$
Conversely, $f_\odot^k$ can be expressed using scalar multiplication, as can be seen from our simulation of general function applications by pointwise function application on scalars.
%
% % assume \langfor to have functions that process only matrices of size $1\times 1$
% % and not scalar product.
%
% % \floris{I am confused by what follows below. I see that general function application can be
% % reduced to scalar function application. Similarly, matrix multiplication be reduced (do we use this?). BUT the expression for scalar multiplication used scalar multiplication?!?}
% \thomas{Resolved here, the following is used in the proofs of section 6. The reduction of matrix multiplication it's not used I believe}
% \begin{itemize}
% \item
%
%
% \item  This is useful when the function allowed is $\kprod$, since one can be computed using the other and viceversa.
% Note that, if $\ssum$ is present, multiplication function application on matrices of any size can be simulated as
% $$
% f_{\kprod}(e_1,\ldots, e_n) :=\Sigma x_i \Sigma x_j. (x_i^T\cdot e_1\cdot x_j)\times \ldots \times (x_i^T\cdot e_n\cdot x_j) \times x_i\cdot x_j^T,
% $$
% which also holds if $e_1,\ldots, e_n$ are scalars. If we have function application, we can simulate
% scalar product. Let $c$ a scalar:
% $$
% c\times A :=f_{\kprod}(e_{\ones}(A)^T\cdot c \cdot e_{\ones}(A)^T, A).
% $$
%
% Another interesting consequence of allowing for loops in \lang\ is that the pointwise product and sum can be defined without direct access to these operators. Namely, the following result tells us that we can always assume to have access to the product and the sum.
% This will be of crucial importance when comparing \langfor\ to arithmetic circuits in Section \ref{sec:circuits} and to annotated relations in Section \ref{sec:restrict}. Formally, we have:
%
%
%
%
Finally, a notational simplification is that when using scalars $a\in\RR$ in our expressions, we write sometimes
$a$ instead of $[a]$. For example,  $(1-e_{\ones}(v)^T\cdot v)$ stands for  $([1]-e_{\ones}(v)^T\cdot v)$.

%
% \end{itemize}
