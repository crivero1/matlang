%Probably the longest section.


%\begin{itemize}
%\item Define the syntax and the semantics of for loop language. 
%\item Examples:
%\begin{itemize}
%\item Simple queries (cover \lang\)
%\item Elementary operations
%\item How to define order (the $Z$ matrix, $v_{max}$,...)
%\item Standard Linear Algebra algorithms:
%\begin{itemize}
%\item Gaussian elimination
%\item Inverse
%\item Determinant
%\item $LU$
%\item $\cdots$
%\end{itemize}
%\end{itemize}
%\end{itemize}

While \lang\ serves as a solid basis for expressing linear algebra properties, it is still somewhat lacking when defining more advanced linear algebra operators such as Gaussian eliminations, or computing an inverse of a matrix. To alleviate these issues, we propose an extended version of \lang, called \langfor which allows us to express such properties.

%As before, we assume a countably infinite set of matrix variables $\Mvar = \{V_1, V_2, \ldots\}$, and a set $\Fun$  of functions $f:\mathbb{C}^n \mapsto \mathbb{C}$. Additionally, for each $n\in \mathbb{N}$, we denote by $e_1^n,\ldots ,e_n^n$ the complete list of canonical vectors of dimension $n$; that is, $e_1^n = [1\ 0 \cdots 0]^*$, $e_2^n = [0\ 1\ 0 \cdots 0]^*$, etc. When $n$ is clear from the context we will simply write $e_1,e_2$, etc. The syntax of \langfor is defined as follows:


%\medskip

%\begin{tabular}{lcll}
%$e$ & $::=$ & $V\in \Mvar$ & (matrix variable)\\
 %& $|$ & $e^*$ & (conjugate transpose)\\ 
 %& $|$ & $e_1 \cdot e_2$ & (matrix multiplication)\\   
 %& $|$ & $e_1 + e_2$ & (matrix addition)\\    
 %& $|$ & $f(e_1,\ldots ,e_n)$ & (application of $f\in \Fun$)\\
 %& $|$ & $\ffor{v}{X}{e}$ & (canonical for loop, with $v, X \in \Mvar$). 
%\end{tabular}

As before, we assume a countably infinite set of matrix variables $\Mvar = \{V_1, V_2, \ldots\}$, and a set $\Fun$  of functions $f:\mathbb{C}^n \mapsto \mathbb{C}$. Additionally, for each $n\in \mathbb{N}$, we denote by $e_1^n,\ldots ,e_n^n$ the complete list of canonical vectors of dimension $n$; that is, $e_1^n = [1\ 0 \cdots 0]^*$, $e_2^n = [0\ 1\ 0 \cdots 0]^*$, etc. When $n$ is clear from the context we will simply write $e_1,e_2$, etc. The syntax of \langfor is defined as \lang\, but with an extra expression.


\medskip

\begin{tabular}{lcll}
 $\ffor{v}{X}{e}$ & (canonical for loop, with $v, X \in \Mvar$). 
\end{tabular}

\medskip

%Similarly as when defining \lang\, here we start with a core set of matrix operations (i.e sum, product, and the transpose of a matrix). In addition to this, we allow applying functions, and  the $\ffor{v}{X}{e}$ construct, which allows looping over the canonical vectors of a specified dimension, and updating the context of the variable $X$ in each iteration. The latter operator is inspired by classical Linear Algebra algorithms \cite{num}, which commonly use loops whose termination conditions are determined by the matrix dimension, and will allow us to express many properties of interest.

%A \langfor {\em schema} $\Sch$ is a pair $\Sch=(\Mnam,\size)$, where $\Mnam\subset \Mvar$ is a finite set of matrix variables, and $\size: \Mvar \mapsto \DD\times \DD$ is a function that maps each matrix variable to a pair of {\em size symbols}. Given a schema $\Sch$, the type of a \langfor expression, denoted $\ttype(e)$, is defined inductively as follows:
%\begin{itemize}
%\item $\ttype(V) = \size(V)$, for a matrix variable $V$,
%\item $\ttype(e^*) = (\beta,\alpha)$, if $\ttype(e)=(\alpha,\beta)$; and undefined if $\ttype(e)$ is undefined,
%\item $\ttype(e_1 \cdot e_2) = (\alpha,\gamma)$, whenever $\ttype(e_1)=(\alpha,\beta)$, and $\ttype(e_2)=(\beta,\gamma)$; and is undefined otherwise,
%\item $\ttype(e_1 + e_2) = \ttype(e_1)$, if $\ttype(e_1) = \ttype(e_2)$; and is undefined otherwise,
%\item $\ttype(f(e_1,\ldots ,e_n)) = (1,1)$, whenever $\ttype(e_1)= \cdots =\ttype(e_n)=(1,1)$, and $f:\mathbb{C}^n\mapsto \mathbb{C}$; and is undefined otherwise,
%\item $\ttype(f(e_1,\ldots ,e_n)) = (\alpha,\beta)$, whenever $\ttype(e_1) = \ldots = \ttype(e_k) = (\alpha,\beta)$, and $f:\mathbb{C}^n \mapsto  \mathbb{C}$; and is undefined otherwise, and,
%\item $\ttype(\ffor{v}{X}{e}) = \ttype(e)$, if $\ttype(X) = \ttype(e)$, and $\ttype(v) = (\gamma,1)$; and is undefined otherwise.
%\end{itemize}
%\cristian{Is this the same than in the previous section? Is there any difference?}

To extend \lang\, we add $\ffor{v}{X}{e}$ construct, which allows looping over the canonical vectors of a specified dimension, and updating the context of the variable $X$ in each iteration. The latter operator is inspired by classical Linear Algebra algorithms \cite{num}, which commonly use loops whose termination conditions are determined by the matrix dimension, and will allow us to express many properties of interest.

A \langfor {\em schema} $\Sch$ is a pair $\Sch=(\Mnam,\size)$, where $\Mnam\subset \Mvar$ is a finite set of matrix variables, and $\size: \Mvar \mapsto \DD\times \DD$ is a function that maps each matrix variable to a pair of {\em size symbols}. Given a schema $\Sch$, the type of a \langfor expression, denoted $\ttype(e)$, is defined inductively as \lang\, and we add the following:
\begin{itemize}
\item $\ttype(\ffor{v}{X}{e}) = \ttype(e)$, if $\ttype(X) = \ttype(e)$, and $\ttype(v) = (\gamma,1)$; and is undefined otherwise.
\end{itemize}

We remark that in the expression $\ffor{v}{X}{e}$, we require that $\ttype(X) = \ttype(e)$ since, intuitively, this expression updates the content of the variable $X$ in each iteration using the result of $e$. As we will see below, evaluating $e$ will depend on a canonical vector stored in $v$, and the current content of $X$. %Another difference from \lang\ is that we allow function application only on scalars (more precisely, on $1\times 1$ matrices).

An expression $e$ is well-typed over a schema $\Sch$ if its type is defined. For well-typed expressions we can define the evaluation as follows.
%
A \langfor {\em instance} $\I$ over a schema $\Sch$, is a pair $\I = (\dom,\conc)$, where $\dom : \DD \mapsto \mathbb{N}$ assigns a value to each size symbol, and $\conc : \Mnam \mapsto \mtr{\mathbb{C}}$ assigns a concrete matrix to each matrix variable $M\in \Mnam$, such that $\dim(\conc(M)) = \dom(\alpha)\times \dom(\beta)$, where $\size(M) = (\alpha,\beta)$. As before, we assume that $\dom(1) = 1$, for every instance $\I$. 
 %(meaning that $e_1^n = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$, etc.).
If $\I$ is an instance, $V$ a matrix variable such that $\size(V)= (\alpha,\beta)$, and $M$ a matrix of dimension $\dom(\alpha)\times \dom(\beta)$, then $\I[V := M]$ denotes an instance that coincides with $\I$, apart from the fact that the value of the matrix variable $V$ is the matrix $M$. 
%If $e$ is a well-typed expression according to $\Sch$, then we denote by $\sem{e}{\I}$ the matrix obtained by evaluating $e$ over $\I$, and define it as follows:
%\begin{itemize}
%\item $\sem{M}{\I} = \conc(M)$, for $M\in \Mnam$;
%\item $\sem{e^*}{\I} = \sem{e}{\I}^*$, where $M^*$ is the conjugate transpose of a matrix $M$;
%\item $\sem{e_1\cdot e_2}{\I} = \sem{e_1}{\I} \cdot \sem{e_2}{\I}$;
%\item $\sem{e_1 + e_2}{\I} = \sem{e_1}{\I} + \sem{e_2}{\I}$;
%\item $\sem{f(e_1,\ldots ,e_n)}{\I}$ is a $1\times 1$ matrix whose only entry has the value $f(\sem{e_1}{\I},\ldots ,\sem{e_n}{\I})$. Here we abuse the notation and use $\sem{e}{\I}$ to denote both a $1\times 1$ matrix, and a scalar from $\mathbb{C}$.
%\item $\sem{f(e_1,\ldots ,e_n)}{\I}$ is a matrix $A$ of the same size as $\sem{e_1}{\I}$, and where $A_{ij}$ has the value $f(\sem{e_1}{\I}_{ij},\ldots ,\sem{e_n}{\I}_{ij})$.
%\end{itemize}
%\cristian{The same as with the type function, we can reuse the semantics of the previous section.}

If $e$ is a well-typed expression according to $\Sch$, then we denote by $\sem{e}{\I}$ the matrix obtained by evaluating $e$ over $\I$, and define it as in \lang\, adding the new operator. The semantics of $\ffor{v}{X}{e}$ over $\I$ is defined iteratively as follows:
\begin{itemize}
\item Let $\ttype(v)= (\gamma,1)$, and $\ttype(e) = (\alpha,\beta)$. Set $n := \dom(\gamma)$.
\item Let $A_0 = \mathbf{0}$, be the null matrix of size $\dom(\alpha)\times \dom(\beta)$.
\item For $i=1,\ldots n$, compute $A_i = \sem{e}{\I[v := e^{n}_i, X:= A_{i-1}]}$.
\item Finally, set $\sem{\ffor{v}{X}{e}}{\I} = A_{n}$.
\end{itemize}

\floris{Perhaps it is insightful to already give an easy example here, e.g., clique. Perhaps even better are the very simple expressions for diag and one vector. The text below is quite
cryptic.}

Notice that evaluating $\ffor{v}{X}{e}$ over an instance that already assigns values to $v$ and $X$, these values get overwritten immediately. Notice that if $e$ does not use the variable $v$, then it will have the same value in each iteration. Sometimes when we want to make it explicit that the expression $e$ uses matrix variables $X_1,\ldots ,X_n$, we write $e(X_1,\ldots ,X_n)$, where $X_1,\ldots ,X_n$ are all the matrix variables mentioned in $e$. Analogously with First Order Logic, we could define the notion of free and bound variables (i.e. the ones in the scope of a \texttt{for} operator), however, since we explicitly rewrite the value of these variables, this distinction will not play an important role.


\cristian{I don't understand this comment of free and bound variables. What do you mean? I believe than similar than in first order variables, one can define this concept, but here it is not used.}

%\subsection{Examples of \langfor expressions}




%%needs dimensions:
%Next we illustrate the versatility of the introduced language. To begin, we note that, unlike the original \lang\ proposal, we have at our disposals canonical vectors of arbitrary dimension. The most basic use of canonical vectors is for accessing a position $ij$ of some matrix $M$, a property which lies outside of the scope of \lang. For this, we can simply use the expression $(e_i^{n})^*\cdot M \cdot e_j^m$, where $M$ is a matrix of size $m\times n$, $e_i^n$ is the $i$th canonical vector of dimension $n$, and likewise, $e_j^m$ is the $j$th canonical vector of dimension $m$.

To illustrate how \langfor works, we next present a series of properties that the language can express, introduce some operators that will be commonly used throughout the paper, and show how the proposed language captures original \lang.

\medskip

\noindent{\bf For loops.} 
The biggest novelty of $\langfor$ is the for operator. To illustrate how this operator can be used, we first show how one can construct the identity matrix of needed dimension in \langfor. For this, it suffices to use the expression $$\ffor{v}{X}{X + v\cdot v^*}.$$ For this expression to be well-typed, $v$ has to be a vector variable of type $\alpha\times 1$, and $X$ a matrix variable such that $\size(X) = (\alpha,\alpha)$. When evaluated over some instance $\I$, this loop starts by initializing $X$ as the null matrix of dimension $n\times n$, where $n=\dom(\alpha)$, and then adds to $X$ the matrix $e_1^n\cdot (e_1^n)^*$ in the first iteration, the matrix $e_2^n\cdot (e_2^n)^*$ in the second iteration, and so on, until $X$ equals the identity matrix of dimension $n\times n$ in the end.

%%NOT DOABLE IF NO ACCESS TO DIMENSION
%\noindent{\bf Elementary matrix operations.} Here we show the true value of adding canonical vectors to the language, by illustrating how they allow us to express elementary matrix operations \cite{linalg}:
%\begin{enumerate}
%\item {\bf Row switching.} To switch the row $i$ and row $j$ of an $m\times n$ matrix $M$, we can simply use the expression $T^{ij} \cdot M$, where:
%$$T^{ij} = I^m - e_i^m\cdot (e_i^m)^* - e_j^m\cdot (e_j^m)^* + e_i^m\cdot (e_j^m)^* + e_j^m\cdot (e_i^m)^*.$$
%\item {\bf Row multiplication.} Multiplying a row by a scalar $c$ is performed by 
%\item {\bf Row addition.}
%\end{enumerate}
%Analogously, we can perform these transformations of columns, by using the transposed matrix $M^*$, and the canonical vectors of appropriate dimension.

\medskip

\noindent{\bf Core operators} 
Note that the  \texttt{for} operator is much more powerful that it seems. We can simulate some operations of the original \lang\ semantics. Assume the allowed expressions are

\begin{tabular}{lcll}
$e$ & $:=$ & $V\in \Mvar$ & (matrix variable)\\
 & $|$ & $e^T$ & (transpose)\\ 
 & $|$ & $e_1 \cdot e_2$ & (matrix multiplication)\\   
 & $|$ & $e_1 + e_2$ & (matrix addition)\\    
 & $|$ & $\text{apply}[f](e_1,\ldots ,e_n)$ & (application of $f\in \Fun$)\\
 & $|$ & $\ffor{v}{X}{e}$ & (canonical for loop, with $v, X \in \Mvar$). 
\end{tabular}

Where function application $\text{apply}[f]$ works only on scalars. This is 

\begin{itemize}
\item $\ttype(\text{apply}[f](e_1,\ldots ,e_n)) = (1,1)$, whenever $\ttype(e_1)= \cdots =\ttype(e_n)=(1,1)$, and $f:\mathbb{C}^n\mapsto \mathbb{C}$; and is undefined otherwise.
\item $\sem{\text{apply}[f](e_1,\ldots ,e_n)}{\I}$ is a $1\times 1$ matrix whose only entry has the value $f(\sem{e_1}{\I},\ldots ,\sem{e_n}{\I})$.
\end{itemize}

We have the following:

\begin{itemize}
\item {\em Pointwise function application.} Note that

\begin{align*}
f(e_1,\ldots, e_n)&=\\ 
&\texttt{for }v,X.\quad X + \\
&\quad \texttt{for }w,Y.\quad Y + \\ 
&\quad \quad v\left[ \text{apply}[f](v^Te_1w, \ldots, v^Te_nw) \right] w^T 
\end{align*}

\item {\em Conjugate transpose.} Let $\texttt{conj}(a) := \overline{a}$ be the conjugate function. Then we can simulate the conjugate transpose of expression $e$ as $\texttt{conj}(e^T)$.
\item {\em One vector.} Using the function $f(x) := 1$, we can define $$\ones(e) := f(\texttt{for } v,X. X + e\cdot v).$$
\item {\em Multiplying a matrix by a scalar.} Let $c$ be a scalar ($1\times 1$ matrix) and $f_{\times}(a, b) := a \times b$ . Then, for an expression $e$, $c\odot e := f_{\times}(\ones(e)\cdot c \cdot \ones(e^T)^T, e)$.
\item {\em Diagonal of a vector.} The operator $\diag(e)$ can be defined as:
$$\diag(e) := \texttt{for } v, X. X + (v^T\cdot e) \odot vv^T.$$
\end{itemize}

\medskip

\noindent{\bf Order.} The \texttt{for} operator assumes that canonical vectors come in some particular order, however, it does not give us an immediate access to this order. An interesting question is whether, using the \texttt{for} loops, we can define a \langfor expression which allow us to define the order of canonical vectors. Next we show that this is indeed possible. To begin with, we can easily obtain the last canonical vector using the expression $$v_{max} := \ffor{v}{X}{v}$$

The fundamental property of iteration we use here is that the result variable is always initiated with the null matrix. Therefore the loop above will simply keep on storing the current canonical vector before returning the final one. The ability to do this sort of manipulation was one of the reasons why we initiate $A_0$ in the semantics of \texttt{for} to null matrix.

To define an order relation for canonical vectors, notice that the following matrix:
\[
Z_{eq} = \begin{bmatrix}
    1 & 1 & \cdots &  1 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 1 
\end{bmatrix},
\] 
has the property that $e_i^*\cdot Z_{eq} \cdot e_j$, for two canonical vectors $e_i,e_j$ of the same dimension, is equal to one if and only $i\leq j$, and is zero otherwise. $Z_{eq}$ can easily de defined in \langfor as follows:
$$Z_{\text{eq}}=\ffor{v}{X}{X + \left[ (Xv_{max}) + v \right]v^* + vv^*_{max}},$$
where $v_{max}$ is as defined above. The intuition behind this expression is that using the last canonical vector $v_{max}$, we have access to the final column of $X$ (via the product $X\cdot v_{max}$), to which we add the current canonical vector $v$, thus constructing $Z_{eq}$ by filling it column by column. By defining $$\lleq{u}{v} := u^*\cdot Z_{eq} \cdot v,$$
we obtain an order relation that allows us to discern whether one canonical vector comes before the other in the order given by \texttt{for}. If we want a strict order, we can just use $Z_< := Z_{eq} - I$.

Interestingly, we can also define the predecessor relation between canonical vectors. For this, we require the following matrix:
%\[
%S := \begin{bmatrix}
%    0 & 1         & 0         & \cdots &  0 \\
%    0 & \ddots & 1         & \cdots & 0 \\
%    \vdots & \vdots & \ddots & \ddots & \vdots \\
%    \hdotsfor{3} & \ddots & 1 \\
%    0 & \cdots & \cdots & \cdots & 0 
%\end{bmatrix}.
%\]
\[
S = \begin{bmatrix}
    0 & 1 & \cdots &  0 \\
    0 & \ddots & \ddots & \vdots \\
    \hdotsfor{3} & 1 \\
    0 & \cdots & \cdots & 0
\end{bmatrix},
\] 
Using this matrix, we have that for a canonical vector $e_i$:
\[
  			S\cdot e_i=\begin{cases}
               e_{i-1}, \text{ if } i > 1 \\
              \mathbf{0}, \text{ if } i = 1
            \end{cases}
		\]
where $\mathbf{0}$ is a vector of zeros of the same type as $e_i$. Notice also that $\ones(v)^*\cdot S \cdot v$ is equal to zero, for a canonical vector $v$, if and only if $v = e_1$ is the first canonical vector, and zero otherwise. To define the first canonical vector in the order given by \texttt{for}, we can then write:
$$v_{min} := \ffor{v}{X}{\mmin{v}\cdot v},$$
where the expression $\mmin{v}$ is defined as $\mmin{v} := 1 - \ones(v)^*\cdot S \cdot v$, and, when evaluated over canonical vectors, will result in $1$ if and only if $v=e_1$ is the first canonical vector. Finally, we denote that $S$ can be defined using the following \langfor expression:
\begin{multline*}
S:= \texttt{for }v,X.\quad X + \\
\left[ (1 - v^*v_{max})vv_{max}^* - (Xv_{max}) v_{max}^* + (Xv_{max})v^*\right].
\end{multline*}

\noindent{\bf Initialization}.  As mentioned previously, all of the iterations in \texttt{for} loops start by initializing $A_0$ to the null matrix. We have already seen that this property is useful for defining the extremal points of our ordering, however, sometimes we would like to start the iteration using some concrete matrix $B$. Using the operator $\mmin$, we can actually achieve this as follows. First, let $\ffor{v}{X}{e(v,X)}$ be the \texttt{for} loop that begins iterating from the null matrix. Here we write $e(v,X)$ to denote the variables that $e$ potentially (but not necessarily) uses. To start the iteration from $B$ it now suffices to use the following loop:
$$\ffor{v}{X}{\mmin{v}\cdot e(v,X/B) + (1-\mmin{v})\cdot e(v,X)} \quad (\dag),$$
where $e(v,X/B)$ denotes the expression obtained by replacing every occurrence of $X$ in $e$ with $B$. The modified \texttt{for} expression will now use $B$ as the assignment of $X$ in the very first iteration (as determined by $\mmin{v}$), and will proceed as before whenever $v$ is not equal to the first canonical vector, thus defining the desired result. Since starting iteration with a non null matrix is often useful, we will denote  by $\initf{B}{v}{X}{e}$ the expression $(\dag)$.

\medskip

\noindent{\bf Canonical summation.} Notice that when defining the identity matrix, we actually only update $X$ by adding some matrix to it. This restricted form of the $\texttt{for}$ loop will prove useful throughout the paper, and we will therefore introduce it a special operator. That is, we define:
$$\Sigma v. e := \ffor{v}{X}{X + e}.$$

Apart from computing the identity matrix, the summation operator also allows for computing the trace of a matrix $A$ using the expression $tr(A) := \Sigma v. v^*\cdot A \cdot v$. Interestingly enough, this restricted version of \texttt{for} already allows us to capture the \lang\ operators that are not present in the syntax of \langfor. More precisely, we have:
\begin{itemize}
%\item {\bf Multiplying a matrix by a scalar.} One of the fundamental operations is the ability to multiply a matrix by a scalar. While this operation is not explicitly present in \langfor, we can simulate it as follows. Let $f_c(x) := c\cdot x$. For an expression $e$ 
%\item {\bf Function application.} Notice that in \lang, a function is applied pointwise to matrices of arbitrary size, while \langfor only allows functions that process matrices of size $1\times 1$. Using the summation operator we can lift this condition, and allow applying a function $f:\mathbb{C}^n \mapsto \mathbb{C}$ on expressions $e_1,\ldots ,e_n$ of arbitrary (but equal) type by writing 
%$$\Sigma x_i \Sigma x_j. f(x_i^*\cdot e_1\cdot x_j, \ldots ,x_i^*\cdot e_n\cdot x_j) \cdot x_i\cdot x_j^*,$$
%which simply reconstructs the matrix obtained by applying $f$ to every position of $e_1$ through $e_n$, by using the fact that for two canonical vectors $e_i^m$ and $e_j^n$, the product $e_i^m \cdot (e_j^n)^*$ defines a $m\times n$ matrix whose only non-zero entry is in the position $ij$. From now on we will abuse the notation and allow applying $f$ to an arbitrary matrix, and not only a $1\time 1$ matrix. PERHAPS USE THE SAME NOTATION AS MATLANG TO AVOID THIS THING ALTOGETHER?
\item {\em One vector.} Using the function $f(x) := 1$, we can define $$\ones(e) := f(\Sigma v. e\cdot v).$$
\item {\em Diagonal of a vector.} The operator $\ones(e)$ can be defined as:
$$\diag(e) := \Sigma v. (v^*\cdot e) \cdot vv^*.$$
\end{itemize}

Using the observations above we obtain the following:
\begin{corollary}
\lang\ is subsumed by \langfor.
\end{corollary}

To show that the inclusion here is strict, we illustrate how one can detect whether a undirected graph has a four clique, which it is not definable in \lang~\cite{BrijderGBW19}. For this, we define an expression $f(u,v) := 1 - u^*\cdot v$. Notice that when $u$ and $v$ are interpreted by two canonical vector of the same dimension, we have that:
\[
  			f(u,v)=1-u^*v=\begin{cases}
               0 \text{ if } u=v \\
               1 \text{ if } u\neq v
            \end{cases}
		\]

To distinguish four-cliques, we will need to determine whether we are dealing with four different nodes. For this, we will utilize the function $$g(u,v,w,r)=f(u,v)\cdot f(u,w)\cdot f(u,r)\cdot f(v,w)\cdot f(v,r)\cdot f(w,r),$$
which, when evaluated over four canonical vectors of the same dimension, will give us 1 if and only if the four vectors are distinct. With this at hand, we can now define:		
\begin{multline*}
\texttt{4-clique}(A) := \ssum v_1.\ssum v_2. \ssum v_3. \ssum v_4.\\ (v_1^*Av_2)(v_1^*Av_3)(v_1^*Av_4)(v_2^*Av_3)(v_2^*Av_4)(v_3^*Av_4) \cdot
\\g(v_1,v_2,v_3,v_4).
\end{multline*}

When $A$ is an adjacency matrix of an undirected graph $G$, then we have that $\texttt{4-clique}(A)$ is different from zero if and only if $G$ has a four-clique. Using this, and the fact that \lang\ is subsumed by First Order Logic with aggregates that uses only three variables \cite{matlang}, we immediately obtain the following:

\begin{corollary}
There is a  \langfor expression that is not expressible in \lang.
\end{corollary}

\medskip

\noindent{\bf Product.} Analogously to the summation with respect to canonical vectors, we can define the product. More precisely, 
if $e$ is an \langfor expression (that possibly uses the variable $v$), we would like to define the result of evaluating $\sem{e}{\I[v := e_1]} \cdot \sem{e}{\I[v := e_2]}\cdots \cdot \sem{e}{\I[v := e_n]}$, where the product is evaluated from left to right. For this purpose, we define the following operator:
$$\sprod v. e=\ffor {v}{X}{X\cdot e + min(v)\cdot e}.$$
Here we use the factor $min(v)\cdot e$ to handle the case of the first canonical vector which starts with $X$ being equal to the null matrix. 

Using the product operator we can express multiple interesting properties. To begin with, we can compute the product of diagonal elements of a matrix using the expression $$dp(A) := \sprod v. v^*\cdot A \cdot v.$$

Another property of interest is computing the transitive closure of a graph adjacency matrix $A$. It is well known the transitive closure of this matrix, denoted $tc(A)$ equals to the matrix consisting of non-zero entries of $(I + A)^n$, where $n$ is the dimension of $A$. Using the product operator we can define:
$$tc(A) := f_{>0}(\sprod v. (I + A)),$$
where $f_{>0}(x) := 1$ if $x>0$, and $f_{>0}(x) = 0$ otherwise, is used to make the result a zero-one matrix. Notice that the expression for $tc(A)$ ignores the canonical vectors, and simply multiplies the previous result with $(I + A)$, thus computing the desired value.

Using the combination of canonical sum and product, we can also define more general operators over matrices, such as the power sum operator, which, given a square matrix $A$, computes $I + A + A^2 + \cdots + A^n$. This operator, denoted by $ps(A)$ can be defined as follows:
$$ps(A) := \ssum v.\sprod w. \left( (w^*Zv)\cdot (A-I) + I\right),$$
where $Z$ is the (strict) order matrix defined above. The outer loop here defines which power we compute. That is, when $v$ is the $i$th canonical vector, we compute $A^i$. Computing $A^i$ is achieved via the inner product loop, which uses $w^*Zv$ to determine whether $w$ comes before $v$ in the ordering of canonical vectors. When this is the case we multiply the current result by $A$, and when $w$ is greater then or equal to $v$, we use $I$ not to affect the already computed result.

\medskip

\noindent{\bf Algorithms in Linear Algebra.} The real power of \langfor comes from the fact that \texttt{for} loops allow us to express many classical linear algebra algorithms. Here we illustrate this by showing how to compute the $LU$ factorization of a matrix using Gaussian eliminations, which is one of the most commonly used matrix algorithms. Recall that $A$ is said to be LU factorizable if there exists matrices $T_1,\ldots, T_{n}$ where $T_i=E_{n}^{(i)}\cdots E_{i+1}^{(i)}$ for some elementary matrices $E_{j}^{(i)}=I+\alpha_{ij}\cdot e_{i}e_{j}^{*}$ such that $T_{n}\cdots T_1A=U$ holds, where $U$ is an upper triangular matrix.

Now, assume that $A$ is a square matrix which allows $LU$ factorization without row pivoting (we deal with this case later on). This means that after reducing the column $i$ of $A$ (i.e. we make all of the entries below the diagonal zero in the column $i$), we will end up with a matrix which has the element in position $i+1,i+1$ different from zero, for each $i$ strictly less than the dimension of $A$. 

To reduce the first column of $A$ it suffices to multiply $A$ from the left with the matrix $T_1 := I + c_1\cdot e_1^*$, where the vector $I$ is the identity matrix, $e_1$ is the first canonical vector, and $c_1$ is the vector 
\[
c_1 :=
\begin{bmatrix}
    0 \\
    \alpha_{21} \\
    \vdots \\
    \alpha_{n1}
\end{bmatrix},
\]
with $\alpha_{j1} := -\frac{A_{j1}}{A_{11}}$ is the number with which we need to multiply the first row of $A$ in order to reduce the row $j$ of $A$. More generally, if $A$ is reduced up to column $i-1$, reducing the $i$th column is achieved by computing $T_i \cdot A$, where $T_i := I + c_i\cdot e_i^*$, with $c_i$ being the vector with zeros up to position $i$, and with the value $-\frac{A_{ji}}{A_{ii}}$ in position $j > i$. To express the matrices $T_i$ in \langfor we will use the following expression:
$$\ccol{A}{y} := \ffor{v}{X}{(y^*\cdot Z_{<} \cdot v)(v^*\cdot A \cdot y)v + X},$$
which, when $y$ is interpreted by the $j$th canonical vector, computes the vector which has zeroes in positions $1\ldots j$, and values $A_{ij}$ in positions $i>j$. Intuitively, the product $y^*\cdot Z_{<}\cdot v$ makes all positions up to $j$ equal zero, and $v^*\cdot A\cdot y$ extracts the element $A_{ij}$ of $A$ when $v$ is interpreted by the $i$th canonical vector, and $y$ by the $j$th one.



Using this expression, we can now compute $T_i$s by writing:
$$\red{A}{y} := I + f_/(\ccol{A}{y},-(y^*\cdot A\cdot y)\cdot \ones(y))y^*,$$
where $f_/$ is the division function. Notice that $f_/(\ccol{A}{y},-(y^*\cdot A\cdot y)\cdot \ones(y))$ in fact computes the vectors $c_i$ defined above. The Gaussian elimination can now be performed by the following expression:
$$
U(A) :=  \left( \initf{I}{v}{X}{\red{X\cdot A}{v}\cdot X} \right) \cdot A,
$$
which computes the $U$ from the $LU$ factorization of $A$, whenever $A$ can be $LU$ factorized without row interchange. Notice that the latter assumption is crucial, since it will ensure that the element of $A$ in position $ii$ is different from zero after each \texttt{reduce} step. As explained previously, the inner \texttt{for} loop in fact computes the product of matrices $T_{n}\cdots T_1$, which in fact amounts to the matrix $L^{-1}$ from the $LU$ factorization of $A$. Given that each $T_i$ is easily invertible, we can also recover $L$.

As stated previously, the construction above works under the assumption that no row pivoting is needed when performing the Gaussian eliminations, giving us:
\begin{proposition}\label{prop:gauss}
There is a \langfor expression $e(X)$ such that by interpreting $X$ with a matrix $A$ will result in the matrix $U$ from the $LU$ factorization of $A$ whenever $A$ is $LU$ factorizable.
\end{proposition}

If $A$ needs row interchange to be $LU$ factorizable we say that $A$ is $PALU$ factorizable, where the factorization is $PA=LU$. To accomplish this, we need something extra.
 \begin{proposition}\label{prop:palu}
There is a \langfor expression $e(X)$ such that by interpreting $X$ with a matrix $A$ will result in the matrix $U$ from the $PALU$ factorization of $A$ whenever $A$ is $PALU$ factorizable if and only if there exists a \langfor expression $e'(X)$ such that by interpreting $X$ with a vector $A$ outputs a canonical vector $e_k$ such that $A_k$ is the first non zero entry of $A$.
\end{proposition}

