\newtheorem*{INVERSE}{Proposition~\ref{prop:inverse}}

Here we only use operators $\ssum$ and $\sprod$ defined in section \ref{sec:restrict}. 
And also assume access to order, this is, we have an expression $e_{\mathsf{Prev}}$. This is enough to 
compute everything related to order, as it was shown in \ref{app:asset_order}.

Let's see why proposition \ref{prop:inverse} is true for upper and lower triangular matrices.
\begin{lemma}\label{prop:upperlowerinverse}
There are $\langforf{f_/}$ expressions $e_{\mathsf{upperDiagInv}}(V)$ and $e_{\mathsf{lowerDiagInv}}(V)$
such that $\sem{e_{\mathsf{upperDiagInv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
to an invertible upper triangular matrix $A$ and $\sem{e_{\mathsf{lowerDiagInv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
to an invertible lower triangular matrix $A$.
\end{lemma}

% If $A=LU$ (no pivoting), then we have that $\mathsf{det}(A)=\mathsf{det}(U)$. Define
% $$
% e_{\mathsf{diagProd}}(M):= \initf{e_{\mathsf{Id}}}{v}{X}v^T\cdot M \cdot v \cdot X.
% $$
% Then
% $$
% e_{\mathsf{ALUdet}}(M):=  e_{\mathsf{diagProd}}\left( e_{\mathsf{U}}(M) \right)
% $$
% defines an expression that, when $M$ is interpreted by $A$ and $A$ is $LU$ factorizable, outputs the determinant of $A$. Note that $X$ has to be $1\times 1$, and starts as $[1]$.
\textit{Proof}. Define
$$
e_{\mathsf{ps}}(M):= e_{\mathsf{Id}} + \ssum v.\sprod w. \left[ \mathsf{succ}(w,v)\times M + (1 - \mathsf{succ}(w,v))\times e_{\mathsf{Id}} \right].
$$
Here, $e_{\mathsf{ps}}(A)$ results in $I+A+A^2+\cdots + A^n$ for any matrix $A$. In the expression, the outer loop defines which power we compute. 
That is, when $v$ is the $i$th canonical vector, we compute $A^i$.
Computing $A^i$ is achieved via the inner product loop, which uses $\mathsf{succ}^+(w,v)$ to 
determine whether $w$ comes before or is $v$ in the ordering of canonical vectors.
When this is the case, we multiply the current result by $A$, and when $w$ is greater 
than $v$, we use the identity not to affect the already computed result. We add the identity at the end.

Now, let $A$ be a $n\times n$ matrix that is upper triangular and
\[
D_A = \begin{bmatrix}
    a_{11} & \cdots & \cdots &  0 \\
    0 & a_{22} & \cdots &  0 \\
    0 & \ddots & \vdots & \vdots \\
    \vdots & \cdots& \cdots & a_{nn}
\end{bmatrix}.
\]
This is, $D_A$ is the diagonal matrix of $A$. We compute this as
$$
e_{\mathsf{getDiag}}(M) := \ssum v. (v^TMv) \times vv^T.
$$
so $e_{\mathsf{getDiag}}(A)=D_A$.
Let $T=A-D_A$, then
$$
A^{-1}=\left[ D_A+T \right]^{-1}= \left[ D_A\left( I+D_A^{-1}T\right) \right]^{-1} = \left( I+D_A^{-1}T\right)^{-1}D_A^{-1}.
$$
Note that
$$
D_A^{-1}=\ssum v. f_{/}(1,v^TD_Av)\times vv^T=\ssum v. f_{/}(1,v^TAv)\times vv^T.
$$
Where $f_{/}$ is the division function. In the last equality we take advantage of the fact that the diagonals of $A$ and $D_A$ are the same.
We now focus on calculating $\left( I+D_A^{-1}T\right)^{-1}$. First, by construction, $D_A^{-1}T$ is strictly upper triangular and thus nilpotent, 
such that $\left( D_A^{-1}T\right)^n=0$, where $n$ is the dimension of $A$.
Recall the following algebraic identity and apply it to $x=D_A^{-1}T$. $$(1+x)\left( \sum_{i=0}^{m}(-x)^i \right)=1-(-x)^{m+1}$$
Choosing $m=n-1$ we have
$$
\left(I+D_A^{-1}T \right)\left( \sum_{i=0}^{n-1}(-D_A^{-1}T)^i \right)=I- \left( -D_A^{-1}T\right)^n =I.
$$
So
$$
\left(I+D_A^{-1}T \right)^{-1}=\sum_{i=0}^{n-1}(-D_A^{-1}T)^i=\sum_{i=0}^{n}(-D_A^{-1}T)^i.
$$
In our language
$$
e_{\mathsf{ps}}([-1]\times D_A^{-1}T)=\sum_{i=0}^{n}(-D_A^{-1}T)^i=\left(I+D_A^{-1}T \right)^{-1}.
$$
We define
$$
e_{\mathsf{diagInverse}}(M):=\ssum v. f_{/}(1,v^TMv)\times vv^T.
$$
So
$$
A^{-1}= e_{\mathsf{ps}}\left([-1]\times \left[e_{\mathsf{diagInverse}}(A)(A-e_{\mathsf{getDiag}}(A))\right] \right)e_{\mathsf{diagInverse}}(A).
$$
Define
$$
e_{\mathsf{upperDiagInv}}(M):= e_{\mathsf{ps}}\left([-1]\times \left[e_{\mathsf{diagInverse}}(M)(M-e_{\mathsf{getDiag}}(M))\right] \right)e_{\mathsf{diagInverse}}(M).
$$
an expression that, when interpreting $M$ as $A$ and $A$ is upper triangular and invertible, outputs $A^{-1}$.
If $A$ is an invertible lower triangular matrix, since $\left(A^{-1}\right)^T=\left(A^T\right)^{-1}$ we define
$$
e_{\mathsf{lowerDiagInv}}(M):= e_{\mathsf{upperDiagInv}}(M^T)^T.
$$
\qed

Next, we prove proposition \ref{prop:inverse}.

\begin{INVERSE}
  There are $\langforf{f_/}$ expressions $e_{\mathsf{det}}(V)$ and $e_{\mathsf{inv}}(V)$ such that
  $\sem{e_{\mathsf{det}}}{\I}=\mathsf{det}(A)$, and  
  $\sem{e_{\mathsf{inv}}}{\I}=A^{-1}$ when $\I$ assigns $V$
  to $A$ and $A$ is invertible.
\end{INVERSE}

\textit{Proof}. Let $A$ be any $n \times n$ matrix.
Let $p_A(x):=\mathsf{det}(xI-A)$ denote its characteristic polynomial.
We write $p_A(x)=1 + \sum_{i=1}^n c_ix^i$. Let $S_i:=\frac{1}{i+1}\mathsf{tr}(A^i)$. 
Then, the coefficients $c_1,\ldots,c_n$ are known to satisfy 
$$
\underbrace{\left(\begin{matrix}
1 & 0 & 0 & \cdots & 0 & 0\\
S_1 & 1 & 0 & \cdots  &0 & 0\\
S_2 & S_1 & 1 & \cdots  &0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & 0\\
S_{n-1} & S_{n-2} & S_{n-3} & \cdots & S_1 & 1\\
\end{matrix}\right)}_{S}\cdot
\underbrace{\left(\begin{matrix}
c_1\\
c_2\\
c_3\\
\vdots\\
c_n\\
\end{matrix}\right)}_{\bar c}=\underbrace{\left(\begin{matrix}
S_1\\
S_2\\
S_3\\
\vdots\\
S_n\\
\end{matrix}\right)}_{\bar b}
$$
and furthermore, $c_n=(-1)^n\mathsf{det}(A)$ and if $c_{n}\neq 0$, then
$$
A^{-1}=\frac{1}{c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i},
$$
with $c_0=1$. It is now easy to see that we can compute the $S_i's$ in $\langfor$. Indeed, for
$i=1,\ldots,n$ we can consider
$$
e_{\mathsf{powTr}}(M,v):=\ssum w. w^T\cdot\left(e_{\mathsf{pow}}(M,v)\cdot M\right)\cdot w
$$
with 
$$
e_{\mathsf{pow}}(M,v):= \sprod w. (\mathsf{succ}^+(w,v)\times M+(1-\mathsf{succ}^+(w,v))\times e_{\mathsf{Id}}).
$$
Here we have that $e_{\mathsf{pow}}(A,b_j)=A^{j-1}$ and thus $e_{\mathsf{powTr}}(A,b_j)=\mathsf{tr}(A^j)$. Define 
$$
e_{S}(M,v):=f_{/}(1, 1 + \ssum w. \mathsf{succ}(w,v))\times e_{\mathsf{powTr}}(M,v).
$$
Here $e_{S}(A,b_i)=S_i$. Note that $i+1$ is computed summing up to the dimension indicated by $v$, and adding 1.
We can now easily construct the vector $\bar b$ by means of the expression
$$
e_{\bar b}(M):=\ssum w.e_{S}(M,w)\times w.
$$
We next construct the matrix $S$. We need to be able to \textit{shift} a vector $a$ in $k$ positions, i.e.,
such that $(a_1,\ldots,a_n)\mapsto (0,\ldots,a_1,\ldots,a_{n-k})$. We use $e_{\mathsf{getNextMatrix}}$ 
defined in section \ref{app:order}.
$$
e_{\mathsf{shift}}(U,V):=\ssum w.(w^T\cdot U)\times(e_{\mathsf{getNextMatrix}}(V)\cdot w)
$$
performs the desired shift when $U$ is assigned a vector $a$ and $V$ is $b_k$. 
The matrix $S$ is now obtained as follows:
$$
S(M):= e_{\mathsf{Id}} + \ssum v. e_{\mathsf{shift}}(e_{\bar b}(M), v)\cdot v^T
$$
We now observe that $S$ is lower triangular with nonzero diagonal entries. So, $S$ is LU factorizable, invertible
and $e_{\mathsf{lowerDiagInv}}(S)=S^{-1}$, thus
$$
e_{\bar c}(M):=e_{\mathsf{lowerDiagInv}}(S(M))\cdot e_{\bar b}(M).
$$
ouputs $\bar c$ when $M$ is interpreted as matrix $A$. Observe that we only use the division operator.
We can now define
$$
e_{\mathsf{det}}(M):=\left( \left(\left( \sprod w. (-1)\times e_{\ones}(M)\right)^T\cdot e_{\mathsf{max}}\right) \times e_{\bar c}(M) \right)^T\cdot e_{\mathsf{max}},
$$
an expression that, when $M$ is interpreted as any matrix $A$, outputs $\mathsf{det}(A)$.
Here, $(\sprod w. (-1)\times e_{\ones}(M))$ is the $n$ dimensional vector with $(-1)^n$ in all of its entries.
Since $c_n=(-1)^n\mathsf{det}(A)$, we extract $(-1)^n(-1)^n\mathsf{det}(A)=\mathsf{det}(A)$ with $e_{\mathsf{max}}$.

For the inverse, we have that
$$
A^{-1}=\frac{1}{c_n}\sum_{i=0}^{n-1}c_i A^{n-1-i} = \frac{1}{c_n}A^{n-1} + \sum_{i=1}^{n-1}\frac{c_i}{c_n}A^{n-1-i}.
$$
We compute $\frac{1}{c_n}A^{n-1}$ as
$$
f_{/}(1, e_{\bar c}(A)^T\cdot e_{\mathsf{max}})\times e_{\mathsf{pow}}(A, e_{\mathsf{max}})
$$
and $\sum_{i=1}^{n-1}\frac{c_i}{c_n}A^{n-1-i}$ as
$$
\ssum v. f_{/}\left( e_{\bar c}(A)^T\cdot v, e_{\bar c}(A)^T\cdot e_{\mathsf{max}} \right)\times e_{\mathsf{invPow}}(A, v),
$$
where
$$
e_{\mathsf{invPow}}(M, v):= \sprod w. (1-\mathsf{max}(w)) \times \left[ (1 - \mathsf{succ}(w,v))\times M + \mathsf{succ}(w,v)\times e_{\mathsf{Id}} \right] + \mathsf{max}(w)\times e_{\mathsf{Id}}.
$$
Here, $e_{\mathsf{invPow}}(A, b_i)=A^{n-1-i}$ and $e_{\mathsf{invPow}}(A, b_n)=I$.
Note that we always multiply by $e_{\mathsf{Id}}$ in the last step.
Define
$$
e_{\mathsf{inv}}(M):= f_{/}(1, e_{\bar c}(M)^T\cdot e_{\mathsf{max}})\times e_{\mathsf{pow}}(M, e_{\mathsf{max}}) + \left[ \ssum v. f_{/}\left( e_{\bar c}(A)^T\cdot v, e_{\bar c}(A)^T\cdot e_{\mathsf{max}} \right)\times e_{\mathsf{invPow}}(A, v) \right],
$$
an expresion that, when $M$ is interpreted as any invertible matrix $A$, computes $A^{-1}$.
\qed

As stated at the beginning of this section, note that we only use $\ssum$, $\sprod$ and $e_{\mathsf{Pred}}$.

% $$
% \text{compute }L^{-1}\rightarrow\text{compute }U=L^{-1}A\rightarrow\text{compute }U^{-1}\rightarrow\text{compute }A^{-1}=U^{-1}L^{-1}
% $$

% Now, if $PA=LU$, we have to change $e_{\mathsf{diagInverse}}(U)$ a little to account for the case when $A$ is singular ($\mathsf{det}(U)=0$):
% $$e_{\mathsf{diagInverse}}(U)=\ssum v. f_{/}(f_{>0}(\mathsf{det}(U)),v^T\left( f_{>0}(\mathsf{det}(U))(U-I) + I \right)v)\times vv^T.$$
% Let $e_{\mathsf{diagprod}}(U)=\sprod v. v^TUv$, note that if $U$ is from $PA=LU$ then $e_{\mathsf{diagprod}}(U)=\mathsf{det}(U)$. We define
% $$e_{\mathsf{diagInverse}}(U):=\ssum v. f_{/}\left(f_{>0}(e_{\mathsf{diagprod}}(U)),v^T\left( f_{>0}(e_{\mathsf{diagprod}}(U))(V-I) + I \right)v\right)\times vv^T.$$
% Here, $e_{\mathsf{diagInverse}}(U)$ computes $D_A^{-1}$ if $f_{>0}(e_{\mathsf{diagprod}}(U))=1$, this is $\mathsf{det}(U)\neq 0$, and outputs the zero matrix otherwise thus $e_{U^{-1}}$ does not change and outputs the zero matriz if $A$ is singular.
% Finally $$e_{\mathsf{inv}}(V):=e_{U^{-1}}(V)\cdot e_{L^{-1}P}(V).$$

% Next, we show why propositions \ref{prop:determinant}  and \ref{prop:inverse} are true for any matrix $A$.

% To get the gaussian elimination of $A$ we use $e_{\mathsf{gauss}}(A) = e_{L^{-1}P}(A)\cdot A = U$. From linear algebra we know that if $PA=LU$
% then
% $$
% \mathsf{det}(P)\mathsf{det}(A)=\mathsf{det}(PA)=\mathsf{det}(LU)=\mathsf{det}(U)
% $$
% since $\mathsf{det}(L)=\mathsf{det}(L^{-1})=1$. Note that $e_{\mathsf{diagProd}(e_{\mathsf{gauss}}(A))}(A)$ computes $\mathsf{det}(U)$ when $PA=LU$ holds.
% Similarly, we take advantage that $L^{-1}$ has ones in its diagonal, and define
% $$
% e_{\mathsf{det}(P)}(V):=\initf{e_{\mathsf{Id}}}{v}{X}\left( 2\cdot v^T \cdot e_{L^{-1}P}(V) \cdot v - 1 \right)\cdot \mathsf{succ}^+\left( v, \mathsf{minnz}(\mathsf{fullcol}(V,v)) \right) \cdot X.
% $$
% Where
% $$
% \mathsf{fullcol}(V,y):= \ffor{v}{X}{(v^T\cdot V \cdot y)\times v + X}.
% $$
% Basically $\mathsf{fullcol}$ extracts the column indicated by canonical vector $y$. In $e_{\mathsf{det}(P)}(V)$, we multiply the current result by $\left( 2\cdot v^T \cdot e_{L^{-1}P}(V) \cdot v - 1 \right)$, which is $1$ if the current diagonal entry is not zero, and $-1$ if it's zero.
% And we do this only if $\mathsf{succ}^+\left( v, \mathsf{minnz}(\mathsf{fullcol}(V,v)) \right)$ outputs $1$, which happens when the first nonzero entry of the column indicated by $v$ comes after the diagonal (indicated by $v$).
% So, when we interpret $V$ as the matrix $A$, we count as one interchange a zero entry in the diagonal of $e_{L^{-1}P}(A)$ with only zero values upwards, we multiply by $-1$ everytime this happens. We can do this because $L^{-1}$ has ones in its diagonal.
% Therefore we can define
% $$
% e_{\mathsf{det}}(V)=f_/\left( e_{\mathsf{det}(U)}(V), e_{\mathsf{det}(P)}(V) \right)
% $$
% Here, when interpreting $V$ as the matrix $A$, we basically do
% $$
% \mathsf{det}(A)=\dfrac{\mathsf{det}(U)}{\mathsf{det}(P)}.
% $$
% Note that if $A$ is singular then $\mathsf{det}(U)=0$ and we know that $\mathsf{det}(P)$ is $1$ or $-1$. If we don't allow $f_{>0}$ and if $A$ is $LU$ factorizable, since $\mathsf{det}(A)=\mathsf{det}(U)$ we define
% $$
% e_{\mathsf{ALU-det}}(V):=\initf{e_{\mathsf{Id}}}{v}{X}v^T\cdot e_{\mathsf{ALU}}(V) \cdot v \cdot X.
% $$