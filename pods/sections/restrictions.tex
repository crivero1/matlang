% !TeX spellcheck = en_US

We give here a nice overview of the section.

\newcommand{\hprod}{\circ}

\subsection{Sumation matlang and relational algebra}

When defining the identity matrix and several other expressions, we actually only update $X$ by adding some matrix to it. This restricted form of the $\texttt{for}$ loop proved to be useful throughout the paper, and we will therefore introduce it a special operator. That is, we define:
$$\Sigma v. e := \ffor{v}{X}{X + e}.$$
We define the subfragment of \langfor, called \langsum, to consist of the $\Sigma$ operator plus the ``core'' operators in \lang, namely, transposition, matrix multiplication and addition, scalar multiplication, and pointwise function applications.

Apart from defining the identity matrix with \langsum, the sum quantifier also allows for computing the trace of a matrix $A$ using the expression $tr(A) := \Sigma v. v^*\cdot A \cdot v$. Interestingly enough, this restricted version of \texttt{for} already allows us to capture the \lang\ operators that are not present in the syntax of \langsum. More precisely, we have:
\begin{itemize}
\item {\em Function application.} Notice that in \lang, a function is applied pointwise to matrices of arbitrary size, while \langsum only allows functions that process matrices of size $1\times 1$. Using the summation operator we can lift this condition, and allow applying a function $f:\mathbb{\RR}^n \mapsto \mathbb{\RR}$ on expressions $e_1,\ldots ,e_n$ of arbitrary (but equal) type by writing 
$$\Sigma x_i \Sigma x_j. f(x_i^T\cdot e_1\cdot x_j, \ldots ,x_i^T\cdot e_n\cdot x_j) \cdot x_i\cdot x_j^T,$$
which simply reconstructs the matrix obtained by applying $f$ to every position of $e_1$ through $e_n$, by using the fact that for two canonical vectors $b_i^m$ and $b_j^n$, the product $b_i^m \cdot (b_j^n)^*$ defines a $m\times n$ matrix whose only non-zero entry is in the position $ij$.
\item {\em One vector.} We can define $$\ones(e) := \Sigma v.\, v.$$ where $\ttype(v) = (\alpha, 1)$ and $\ttype(e) = (\alpha, \beta)$ for some $\beta$. 
\item {\em Diagonal of a vector.} The operator $\diag(e)$ can be defined as:
$$\diag(e) := \Sigma v. (v^T\cdot e) \cdot vv^T.$$
\end{itemize}
Furthermore, one can easily check that the 4-clique expression of Example~\ref{ex:fourcliques} can be defined in \langsum. Therefore, we conclude the following result. 
\begin{corollary}
\lang\ is strictly subsumed by \langsum.
\end{corollary}

%To show that the inclusion here is strict, we illustrate how one can detect whether a undirected graph has a four clique, which it is not definable in \lang~\cite{BrijderGBW19}. For this, we define an expression $f(u,v) := 1 - u^*\cdot v$. Notice that when $u$ and $v$ are interpreted by two canonical vector of the same dimension, we have that:
%\[
%  			f(u,v)=1-u^*v=\begin{cases}
%               0 \text{ if } u=v \\
%               1 \text{ if } u\neq v
%            \end{cases}
%		\]
%
%To distinguish four-cliques, we will need to determine whether we are dealing with four different nodes. For this, we will utilize the function $$g(u,v,w,r)=f(u,v)\cdot f(u,w)\cdot f(u,r)\cdot f(v,w)\cdot f(v,r)\cdot f(w,r),$$
%which, when evaluated over four canonical vectors of the same dimension, will give us 1 if and only if the four vectors are distinct. With this at hand, we can now define:		
%\begin{multline*}
%\texttt{4-clique}(A) := \ssum v_1.\ssum v_2. \ssum v_3. \ssum v_4.\\ (v_1^*Av_2)(v_1^*Av_3)(v_1^*Av_4)(v_2^*Av_3)(v_2^*Av_4)(v_3^*Av_4) \cdot
%\\g(v_1,v_2,v_3,v_4).
%\end{multline*}
%
%When $A$ is an adjacency matrix of an undirected graph $G$, then we have that $\texttt{4-clique}(A)$ is different from zero if and only if $G$ has a four-clique. Using this, and the fact that \lang\ is subsumed by First Order Logic with aggregates that uses only three variables \cite{matlang}, we immediately obtain the following:
%
%\begin{corollary}
%There is a  \langfor expression that is not expressible in \lang.
%\end{corollary}

What operations over matrices can be defined with \langsum that is beyond \lang? In~\cite{brijder2019matrices}, it was shown that \lang\ was strictly included in the relational algebra of $K$-relations~\cite{GreenKT07}, called here Annotated Relational Algebra (ARA).
Then a natural idea is to compare the expressive power of \langsum with ARA. For making this comparison clear, in the following we give the formal definition of ARA~\cite{GreenKT07} to then see how to connect both formalism.

\newcommand{\ddom}{\mathbb{D}}
\newcommand{\fdom}{\operatorname{dom}}
\newcommand{\att}{\mathbb{A}}
\newcommand{\tuples}{\mathbf{tuples}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\adom}{\mathbf{adom}}

\newcommand{\ksum}{\oplus}
\newcommand{\kprod}{\odot}
\newcommand{\bigksum}{\bigoplus}
\newcommand{\bigkprod}{\bigodot}
\newcommand{\kzero}{\mymathbb{0}}
\newcommand{\kone}{\mymathbb{1}}

\newcommand{\row}{\mathsf{row}}
\newcommand{\rows}{\mathsf{rows}}
\newcommand{\col}{\mathsf{col}}
\newcommand{\cols}{\mathsf{cols}}


Let $\ddom$ be a data domain and $\att$ a set of attributes. A relational signature is a finite subset of $\att$. A relational schema is a function $\cR$ on finite set of symbols $\fdom(\cR)$ such that $\cR(R)$ is a relation signature for each $R \in \fdom(\cR)$. To simplify the notation, from now on we write $R$ to denote both the symbol $R$ and the relational signature $\cR(R)$.
Furthermore, we write $R \in \cR$ to say that $R$ is a symbol of $\cR$. 
For $R \in \cR$, an $R$-tuple is a function $t: R \rightarrow \ddom$. We denote by $\tuples(R)$ the set of all $R$-tuples. Given $X \subseteq R$, we denote by $t[X]$ the restriction of $t$ to the set $X$.

A semiring $(K, \ksum, \kprod, \kzero, \kone)$ is an algebraic structure where $K$ is a non-empty set, $\ksum$ and $\kprod$ are binary operations over $K$, and $\kzero, \kone \in K$. Furthermore,  $\ksum$ and $\kprod$ are associative operations, $\kzero$ and $\kone$ are the identities of $\ksum$ and $\kprod$ respectively, $\ksum$ is a commutative operation, $\kprod$ distributes over $\ksum$, and $\kzero$ annihilates $K$ (i.e. $\kzero \kprod k = k \kprod \kzero = \kzero$). As usual, we assume that all semirings in this paper are commutative, namely, $\kprod$ is also commutative. We use $\bigksum_X$ or $\bigkprod_X$ for the $\ksum$- or $\kprod$-operation over all elements in $X$, respectively. Typical examples of semirings are the reals $(\RR, +, \times, 0,1)$, the natural numbers $(\RR, +, \times, 0,1)$, and the boolean semiring $(\{0,1\}, \vee, \wedge, 0, 1)$. 

Fix a semiring $(K, \ksum, \kprod, \kzero, \kone)$ and a relational schema $\cR$. A $K$-relation of $R \in \cR$ is a function $r: \tuples(R) \rightarrow K$ such that the support  $\supp(r) = \{t \in \tuples(R) \mid r(t) \neq \kzero\}$ is finite. 
A $K$-instance $\cJ$ of $\cR$ is a function that assigns relational signatures of $\cR$ to $K$-relations. Given $R \in \cR$, we denote by $R^\cJ$ the $K$-relation associated to $R$. Recall that $R^\cJ$ is a function and then $R^\cJ(t)$ is the value in $K$ assign to $t$. 
Given a $K$-relation $r$ we denote by $\adom(r)$ the active domain of $r$ defined as $\adom(r) = \{t(a) \mid t \in \supp(r) \wedge a \in R\}$.
Then the active domain of an $K$-instance $\cJ$ of $\cR$ is defined as $\adom(\cJ) = \bigcup_{R \in \cR} \adom(R^\cJ)$. 

An ARA expression $\alpha$ over $\cR$ is given by the following syntax:
$$
\begin{array}{rcl}
\alpha & := & R \ \mid \ \alpha \cup \alpha \ \mid \  \pi_X(\alpha) \ \mid \  \sigma_X(\alpha) \ \mid \ \rho_f(\alpha) \ \mid \ \alpha \bowtie \alpha
\end{array}
$$
where $R \in \cR$, $X \subseteq \att$ is finite, and $f: X \rightarrow Y$ is a one to one mapping with $Y \subseteq \att$. One can extend the relational schema $\cR$ to any ARA expressions over $\cR$ recursively as follows: $\cR(R) = R$, $\cR(\alpha \cup \alpha') = \cR(\alpha)$, $\cR(\pi_X(\alpha)) = X$, $\cR(\sigma_X(\alpha)) = \cR(\alpha)$, $\cR(\rho_f(\alpha)) = X$ where $f:X \rightarrow Y$, and $\cR(\alpha \bowtie \alpha') = \cR(\alpha) \cup \cR(\alpha')$ for every ARA expressions $\alpha$ and $\alpha'$.
We further assume that any ARA expression $\alpha$ satisfies the following syntactic restrictions: $\cR(\alpha') = \cR(\alpha'')$ whenever $\alpha = \alpha' \cup \alpha''$, $X \subseteq \cR(\alpha')$ whenever $\alpha = \pi_X(\alpha')$ or $\alpha = \sigma_X(\alpha')$, and $Y = \cR(\alpha')$ whenever $\alpha = \rho_f(\alpha')$ with $f: X \rightarrow Y$.

Given an ARA expression $\alpha$ and a $K$-instance $\cJ$ of $\cR$, we define the semantics $\ssem{\alpha}{\cJ}$ as a $K$-relation of $\cR(\alpha)$ as follows. For $X \subseteq \att$, let $\operatorname{Eq}_X(t) = \kone$ when $t(a) = t(b)$ for every $a, b \in X$, and $\operatorname{Eq}_X(t) = \kzero$ otherwise. For every tuple $t \in \cR(\alpha)$:
$$
\begin{array}{ll}
\text{if $\alpha = R$, then} & \ssem{\alpha}{\cJ}(t) = R^\cJ(t) \\
\text{if $\alpha = \alpha' \cup \alpha''$, then} & \ssem{\alpha}{\cJ}(t) = \ssem{\alpha'}{\cJ}(t) \ksum \ssem{\alpha''}{\cJ}(t)  \\
\text{if $\alpha = \pi_X(\alpha')$, then} & \ssem{\alpha}{\cJ}(t) = \bigodot_{t': t'[X] = t} \ssem{\alpha'}{\cJ}(t') \\
\text{if $\alpha = \sigma_X(\alpha')$, then} & \ssem{\alpha}{\cJ}(t) = 
\ssem{\alpha'}{\cJ}(t) \kprod \operatorname{Eq}_X(t)  \\
\text{if $\alpha = \rho_f(\alpha')$, then} & \ssem{\alpha}{\cJ}(t) = 
\ssem{\alpha'}{\cJ}(t \circ f)  
\\
\text{if $\alpha = \alpha' \bowtie \alpha''$, then} & \ssem{\alpha}{\cJ}(t) =  \ssem{\alpha'}{\cJ}(t[Y]) \kprod  \ssem{\alpha''}{\cJ}(t[Z])
\end{array}
$$
where $Y = \cR(\alpha')$ and $Z = \cR(\alpha'')$. It is important to note that the $\bigksum$-operation on the semantics of $\pi_X(\alpha')$ is well defined given that the support of $\ssem{\alpha'}{\cJ}$ is always finite. 

We are ready for comparing the expressive power of \langsum with ARA. First of all, we need to extend \langsum from $\RR$ to any semiring $(K, \ksum, \kprod, \kzero, \kone)$. Indeed, one can easily verify that the semantics of \lang, \langfor and \langsum can be translated from $\RR$ to $K$ by switching from matrices over $(\RR, +, \times, 0, 1)$ to matrices over $(K, \ksum, \kprod, \kzero, \kone)$.
From now on we denote by  $\mtr{K}$ the set of all $K$-matrices. Similar than for \lang\ over $\RR$, given a \lang\ schema $\Sch$ a $K$-instance $\I$ over $\Sch$ is a pair $\I = (\dom,\conc)$, where $\dom : \DD \mapsto \NN$ assigns a value to each size symbol, and $\conc : \Mnam \mapsto \mtr{K}$ assigns a concrete $K$-matrix to each matrix variable. It is straightforward to extend the semantics of \lang, \langfor, and \langsum from $(\RR, +, \times, 0, 1)$ to $(K, \ksum, \kprod, \kzero, \kone)$ by switching $+$ with $\ksum$ and $\times$ with $\kprod$. 

The next step for comparing \langsum with ARA is to represent $K$-matrices as $K$-relations.
Recall that $\Sch=(\Mnam,\size)$ is a \lang\ schema, where $\Mnam\subset \Mvar$ is a finite set of matrix variables, and $\size: \Mvar \mapsto \DD\times \DD$ is a function that maps each matrix variable to a pair of size symbols. On the relational side
we have for each size symbol $\alpha\in\DD\setminus\{1\}$, attributes $\alpha$, $\row_\alpha$, and $\col_\alpha$ in $\att$. For each $V\in\Mnam$ and $\alpha \in \DD$ we denote
by $R_V$ and $R_\alpha$ its corresponding relation name, respectively. Then, given $\Sch$ we define the relational schema $\text{Rel}(\Sch)$ such that $\fdom(\text{Rel}(\Sch)) =  \{R_\alpha \mid \alpha\in\DD\} \cup \{R_V \mid V \in \Mnam\}$ where $\text{Rel}(\Sch)(R_\alpha) = \{\alpha\}$ and:
\[
\text{Rel}(\Sch)(R_V) = \begin{cases}
\lbrace\row_\alpha,\col_\beta \rbrace & \text{ if $ \size(V)=(\alpha,\beta)$} \\
\lbrace\row_\alpha \rbrace & \text{ if $ \size(V)=(\alpha,1)$} \\
\lbrace\col_\beta \rbrace  &
\text{ if $ \size(V)=(1,\beta)$} \\
\lbrace\rbrace & \text{ if $\size(V)=(1,1)$}.
\end{cases}
\]
Consider now a matrix instance $\I = (\dom,\conc)$ over $\Sch$.
Let $V\in\Mnam$ with $\size(V)=(\alpha,\beta)$ and let $\conc(V)$ be its corresponding $K$-matrix of dimension $\dom(\alpha)\times \dom(\beta)$.
To encode $\I$ as a $K$-instance in ARA, we use as data domain $\ddom = \mathbb{N} \setminus \{0\}$. Then we construct the $K$-instance $\text{Rel}(\I)$ such that for each $V\in\Mnam$ we define 
$R_V^{\text{Rel}(\I)}(t):=\conc(V)_{ij}$ whenever $t(\row_\alpha) = i \leq \dom(\alpha)$ and $t(\col_\beta) = j \leq \dom(\beta)$, and $\kzero$ otherwise. Furthermore, for each $\alpha \in \DD$ we define $R_\alpha^{\text{Rel}(\I)}(t):=\kone$ whenever $t(\alpha) \leq \dom(\alpha)$, and $\kzero$ otherwise. In other words, $R_\alpha$ and $R_\beta$ encodes the active domain of a matrix variable $V$ with $\size(V)=(\alpha,\beta)$. Given that the ARA framework of \cite{GreenKT07} represents the ``absence'' of a tuple in the relation with $\kzero$, we need to find a way to encode all entries of a matrix in ARA. For instance, we need to be able to encode a $\kzero$-matrix of dimension $(\alpha,\beta)$ in~ARA.

We are ready to state the first connection between \langsum and ARA by using the previous encoding.
\begin{proposition}
	For each \langsum expression $e$ over \lang \ schema $\Sch$ such that $\Sch(e)=(\alpha,\beta)$ with $\alpha\neq 1\neq\beta$, there exists an ARA expression $\Phi(e)$ over relational schema $\text{Rel}(\Sch)$ such that $\text{Rel}(\Sch)(\Phi(e))=\{\row_\alpha,\row_\beta\}$ and 
	such that for any instance $\I$ over~$\Sch$,
	$$
	\sem{e}{\I}_{i,j}=\ssem{\Phi(e)}{\text{Rel}(\I)}(t)
	$$
	for tuple $t(\mathrm{row}_\alpha)=i$ and $t(\mathrm{col}_\beta)=j$. Similarly for when $e$ has schema $\Sch(e)=(\alpha,1)$, $\Sch(e)=(1,\beta)$ or $\Sch(e)=(1,1)$, then $\Phi(e)$ has schema $\text{Rel}(\Sch)(\Phi(e))=\{\mathrm{row}_\alpha\}$,
	$\text{Rel}(\Sch)(\Phi(e))=\{\mathrm{col}_\alpha\}$, or
	$\text{Rel}(\Sch)(\Phi(e))=\{\}$, respectively.
\end{proposition}
To translate ARA into \langsum, we must restrict our comparison to ARA over binary $K$-relations. Given that linear algebra works over vector and matrices, it is reasonable to restrict to unary or binary relations as input. Note that this is only a restriction to the input relations and not to intermediate relations, namely, expressions can create relation signatures of arbitrary size from the binary input relations. Thus, from now we say that a relational schema $\cR$ is binary if $|R| \leq 2$ for every $R \in \cR$. We also make the assumption that there is an (arbitrary) order, denoted by $<$, on the attributes in $\att$. 
This is to identify which attributes correspond to rows and columns when moving the matrix setting. 
Then, given that relations will be  either unary or binary and there is an order in the attributes, we write $t = (v)$ or $t = (u,v)$ to denote a tuple over a unary or binary relation $R$, respectively, where $u$ and $v$ is the value of the first and second attribute with respect to $<$.

Consider a binary relational schema $\cR$. With each $R\in \cR$ we associate a matrix variable $V_R$ such that, if $R$ is a binary relational signature, then $V_R$ represents a (square) matrix, and, if not (i.e. $R$ is unary), then $V_R$ represents a vector. Formally, fix a symbol $\alpha \in \DD \setminus \{1\}$. Let $\text{Mat}(\cR)$ denote the \lang \ schema
$(\Mnam,\size)$ such that $\Mnam = \{ V_R \mid R \in \cR\}$ and $\size(V_R) = (\alpha, \alpha)$ whenever $|R| = 2$, and $\size(V_R) = (\alpha, 1)$ whenever $|R|=1$. 
Take now a $K$-instance $\cJ$ of $\cR$ and suppose that $\adom(\cJ) = \{d_1, \ldots, d_n\}$ is the active domain of $\cJ$ (i.e. the order over $\adom(\cJ)$ is arbitrary). Then we define the matrix instance $\text{Mat}(\cJ) = (\dom,\conc)$ such that $\dom(\alpha) = n$, $\conc(V_R)_{i,j} = R^{\cJ}((d_i, d_j))$ whenever $|R|=2$, and $\conc(V_R)_{i} = R^{\cJ}((d_i))$ whenever $|R|=1$. 
Note that, although each binary $K$-relation can have different active domain, we encode them as square matrices by considering the active domain of the whole $K$-instance.

\begin{proposition}
	Let $\cR$ be a binary relational schema. For each ARA expression $\alpha$ over $\cR$  such that $|\cR(\alpha)| = 2$, there exists an ARA expression $\Psi(\alpha)$ over \lang \ schema $\text{Mat}(\cR)$ such that for any $K$-instance $\cJ$ with $\adom(\cJ) = \{d_1, \ldots, d_n\}$ over $\cR$,
	$$
	\ssem{\alpha}{\cJ}((d_i, d_j))=\sem{\Psi(\alpha)}{\text{Mat}(\cJ)}_{i,j}.
	$$
	Similarly for when $|\cR(\alpha)| = 1$, or $|\cR(\alpha)| = 0$ respectively.
\end{proposition} 

It is important to remark that the $\alpha$ of the previous result can have intermediate expressions that are not necessary binary given that the proposition only restricts that the input relation and the schema of $\alpha$ must have arity at most two. 

Given the previous two propositions we derive the following conclusion which is the first characterization of relational algebra with a (sub)-fragment of linear algebra.
\begin{corollary}
	\langsum and ARA over binary relational schemas are equally expressive. 
\end{corollary}


\subsection{Comparison with weighted logics}

\input{./sections/wl}


\subsection{Matrix multiplication as a quantifier}

Analogously to the summation with respect to canonical vectors, we can define the product. More precisely, 
if $e$ is an \langfor expression (that possibly uses the variable $v$), we would like to define the result of evaluating $\sem{e}{\I[v := e_1]} \cdot \sem{e}{\I[v := e_2]}\cdots \cdot \sem{e}{\I[v := e_n]}$, where the product is evaluated from left to right. For this purpose, we define the following operator:
$$\sprod v. e=\ffor {v}{X}{X\cdot e + min(v)\cdot e}.$$
Here we use the factor $min(v)\cdot e$ to handle the case of the first canonical vector which starts with $X$ being equal to the null matrix. 

Using the product operator we can express multiple interesting properties. To begin with, we can compute the product of diagonal elements of a matrix using the expression $$dp(A) := \sprod v. v^*\cdot A \cdot v.$$

Another property of interest is computing the transitive closure of a graph adjacency matrix $A$. It is well known the transitive closure of this matrix, denoted $tc(A)$ equals to the matrix consisting of non-zero entries of $(I + A)^n$, where $n$ is the dimension of $A$. Using the product operator we can define:
$$tc(A) := f_{>0}(\sprod v. (I + A)),$$
where $f_{>0}(x) := 1$ if $x>0$, and $f_{>0}(x) = 0$ otherwise, is used to make the result a zero-one matrix. Notice that the expression for $tc(A)$ ignores the canonical vectors, and simply multiplies the previous result with $(I + A)$, thus computing the desired value.

Using the combination of canonical sum and product, we can also define more general operators over matrices, such as the power sum operator, which, given a square matrix $A$, computes $I + A + A^2 + \cdots + A^n$. This operator, denoted by $ps(A)$ can be defined as follows:
$$ps(A) := \ssum v.\sprod w. \left( (w^*Zv)\cdot (A-I) + I\right),$$
where $Z$ is the (strict) order matrix defined above. The outer loop here defines which power we compute. That is, when $v$ is the $i$th canonical vector, we compute $A^i$. Computing $A^i$ is achieved via the inner product loop, which uses $w^*Zv$ to determine whether $w$ comes before $v$ in the ordering of canonical vectors. When this is the case we multiply the current result by $A$, and when $w$ is greater then or equal to $v$, we use $I$ not to affect the already computed result.
